{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The degree of aggressivity & Negativity\n",
    "\n",
    "### 2. Personnalization\n",
    "    - Depency tree, establish the subject, object, complement, aux\n",
    "    - Also, tell the (person 1st, 2nd, 3rd) of the subject and the object. \n",
    "    - Identify what are they accuse from \n",
    "    - Identify who’s being targeted (Lexicon or wordnet)\n",
    "    - Established if the aux is directed to the object  (Establishing intention)\n",
    "### 3. Harms inflicted and to who\n",
    "    - Now, using the complement (output of dependency tree), we can map these to threats\n",
    "    - We need wordnet to identify threat. (Actually better lexicon than word net)\n",
    "    - To object refers to what category? race, nationality, religion, color, gender, \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your environment\n",
    "- source spacy/bin/activate (Bash)\n",
    "- get back to your base: deactivate (Bash)\n",
    "\n",
    "You have to be in anaconda3 interpreter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from spacy import displacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbook = ('/Users/philippebeliveau/Desktop/Notebook/Winter_2024/Text_mining/Git_MiningRepository/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "mac_mini = ('/Users/philippebeliveau/Desktop/Notebook_Jupyter_R/Winter_2024/Text_mining/Project/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "df = pd.read_csv(mac_mini)\n",
    "df.head()\n",
    "categories = df['cyberbullying_type'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the tweets by category\n",
    "tweets_by_category = {}\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['tweet_text'].tolist()\n",
    "    \n",
    "    # Add the list of tweets to the dictionary\n",
    "    tweets_by_category[category] = category_tweets\n",
    "\n",
    "category = tweets_by_category['ethnicity'][100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter 2000 tweets from each category\n",
    "df = df.groupby('cyberbullying_type').apply(lambda x: x.sample(min(len(x), 100))).reset_index(drop=True)\n",
    "\n",
    "# Now you can work with 'sampled_df' which contains 2000 samples from each category\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    #removing hastags, links and specific symbol\n",
    "    pattern=re.compile(r\"(#[a-zA-Z0-9]+|@[a-zA-Z0-9]+|https?://\\S+|www\\.\\S+|\\S+\\.[a-z]+|RT @|Ã°ÂÂÂ)\")\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning of data\n",
    "df['clean_data']=df['tweet_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the tweets by category\n",
    "tweets_by_category = {}\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['clean_data'].tolist()\n",
    "    \n",
    "    # Add the list of tweets to the dictionary\n",
    "    tweets_by_category[category] = category_tweets\n",
    "\n",
    "category = tweets_by_category['ethnicity'][100:120]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Critique vs Insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple approach\n",
    "- Extract the adjectives, nouns, the Subject Person, and the complement/conjunction. \n",
    "- Extract the name entities\n",
    "- Extract all the synonyms of the adjective, nouns and complement into a long list.\n",
    "- Create a polarity score using Vadersentiment on the tweet itself and on the list of synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new elements\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df_clean['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    ccomp_conj_children = []\n",
    "    ccomp_conj_sentiments = []\n",
    "    AUX_child = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"acomp\", \"conj\", 'ROOT', 'ccomp'):\n",
    "            children = [child.text for child in token.children]\n",
    "            ccomp_conj_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "        if token.pos_ in (\"AUX\", 'PROPN'): \n",
    "            child = [child.text for child in token.children]\n",
    "            AUX_child.append(child)\n",
    "\n",
    "        # if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "        #     child = token.text\n",
    "        #     AUX_child.append(child)\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_ccomp_conj_sentiment = sum(ccomp_conj_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Dependency Children\": ccomp_conj_children,\n",
    "        \"Dependency Sentiment\": sum_ccomp_conj_sentiment,\n",
    "        \"Aux/pronouns dependence\": AUX_child,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sentiwordnet\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df_clean['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    dep_children = []\n",
    "    senti_sentiments = []\n",
    "    textToken = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"conj\", 'ROOT', 'ccomp'):\n",
    "            children = [child.text for child in token.children]\n",
    "            dep_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            for child in children:\n",
    "                synsets = list(swn.senti_synsets(child))\n",
    "                if synsets:\n",
    "                    senti_sentiments.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "        if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "            text = token.text\n",
    "            textToken.append(text)\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_senti_sentiments = sum(senti_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Dependency Children\": dep_children,\n",
    "        \"Dependency Sentiment\": sum_senti_sentiments,\n",
    "        \"Aux/pronouns dependence\": textToken,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With VADER\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df_clean['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    dep_children = []\n",
    "    senti_sentiments = []\n",
    "    textToken = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"conj\", 'ROOT', 'ccomp'):\n",
    "            children = [child.text for child in token.children]\n",
    "            dep_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            senti_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "        if token.pos_ in (\"AUX\", 'PROPN'): \n",
    "            child = [child.text for child in token.children]\n",
    "            textToken.append(child)\n",
    "        # if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "        #     text = token.text\n",
    "        #     textToken.append(text)\n",
    "    # Compute sum of sentiment scores for the children\n",
    "    sum_senti_sentiments = sum(senti_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Dependency Children\": dep_children,\n",
    "        \"Dependency Sentiment\": sum_senti_sentiments,\n",
    "        \"Aux/pronouns dependence\": textToken,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_person(subjects, person):\n",
    "    person_list = [p for _, p_list in subjects for p in p_list]\n",
    "    return str(person) in person_list\n",
    "\n",
    "df1['Has 1st Person'] = df1['Subjects'].apply(lambda x: contains_person(x, 1))\n",
    "df1['Has 2nd Person'] = df1['Subjects'].apply(lambda x: contains_person(x, 2))\n",
    "df1['Has 3rd Person'] = df1['Subjects'].apply(lambda x: contains_person(x, 3))\n",
    "\n",
    "df1['Word Count'] = df1['Tweet'].apply(lambda x: len(x.split()))\n",
    "df1['Adjusted SentiWordScore'] = df1.apply(lambda row: row['Dependency Sentiment'] / len(row['Dependency Children']) if len(row['Dependency Children']) != 0 else 0, axis=1)\n",
    "#df1['Adjusted SentiWordScore'] = df1.apply(lambda row: row['Dependency Sentiment'] / len(row['Tweet']) if len(row['Tweet']) != 0 else 0, axis=1)\n",
    "df1['Has Capital Word'] = df1['Tweet'].apply(lambda x: any(word.isupper() for word in x.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Has Question Mark</th>\n",
       "      <th>Negated Tweet</th>\n",
       "      <th>Has Discourse Marker</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>Dependency Children</th>\n",
       "      <th>Dependency Sentiment</th>\n",
       "      <th>Aux/pronouns dependence</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Synonyms Sentiment Score</th>\n",
       "      <th>Has Third Person Verb</th>\n",
       "      <th>Average SentiWordNet Score</th>\n",
       "      <th>Has 1st Person</th>\n",
       "      <th>Has 2nd Person</th>\n",
       "      <th>Has 3rd Person</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Adjusted SentiWordScore</th>\n",
       "      <th>Has Capital Word</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thinking about how some girl gets paid six figures to act like a dog on onlyfans and all these girls that acted like dogs and cats in high school got bullied out of their bag</td>\n",
       "      <td>False</td>\n",
       "      <td>thinking about how some girl gets paid six figures to act like a dog on onlyfans and all these girls that acted like dogs and cats in high school got bullied out of their bag</td>\n",
       "      <td>False</td>\n",
       "      <td>[high]</td>\n",
       "      <td>[girl, figures, dog, onlyfans, girls, dogs, cats, school, bag]</td>\n",
       "      <td>[(that, [])]</td>\n",
       "      <td>[thinking, girls, cats, bullied]</td>\n",
       "      <td>[[about], [all, these, acted], [], [dogs, got, out]]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>[(six, CARDINAL)]</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>-0.6908</td>\n",
       "      <td>False</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I got bullied a lot in grade school, jump forward to high school when my art was good enough to do photo realistic portraits and bullies suddenly wanted art of their girl friends... yeah told a lot of people \"NO\", good times.</td>\n",
       "      <td>False</td>\n",
       "      <td>I got bullied a lot in grade school, jump forward to high school when my art was good enough to do photo realistic portraits and bullies suddenly wanted art of their girl friends... yeah told a lot of people \"NO\", good times.</td>\n",
       "      <td>False</td>\n",
       "      <td>[high, good, realistic, good]</td>\n",
       "      <td>[lot, grade, school, school, art, photo, portraits, bullies, art, girl, friends, lot, people, times]</td>\n",
       "      <td>[(art, [])]</td>\n",
       "      <td>[bullied, jump, good, bullies, wanted, told]</td>\n",
       "      <td>[[I, got, lot, in, ,, jump, wanted, ...], [forward, to, was], [], [suddenly, art], [yeah, lot, \", NO, .]]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[], [when, art, good]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.7184</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>False</td>\n",
       "      <td>0.033088</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’ve always wanted to transfer school sa US because most of the movies that I’ve seen made me believe that the new kid always gets bullied and I just wanna have that “Try me bitch” moment in case someone actually tries to bully me.</td>\n",
       "      <td>False</td>\n",
       "      <td>I’ve always wanted to transfer school sa US because most of the movies that I’ve seen made me believe that the new kid always gets bullied and I just wanna have that “Try me bitch” moment in case someone actually tries to bully me.</td>\n",
       "      <td>False</td>\n",
       "      <td>[most, new]</td>\n",
       "      <td>[school, movies, kid, moment, case]</td>\n",
       "      <td>[(I, [1]), (most, []), (I, [1]), (me, [1]), (I, [1]), (wanna, []), (me, [1]), (someone, [])]</td>\n",
       "      <td>[wanted, have]</td>\n",
       "      <td>[[I, ’ve, always, transfer, .], [me, bullied], [that, kid, always, gets, and, have], [wanna, Try, tries], [that, “, bitch, moment, in], [me, ”], [someone, actually, bully]]</td>\n",
       "      <td>-1.7047</td>\n",
       "      <td>[[], [], [sa], [], [], [I, just]]</td>\n",
       "      <td>[(school sa US, ORG)]</td>\n",
       "      <td>-0.8074</td>\n",
       "      <td>0.8074</td>\n",
       "      <td>False</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>44</td>\n",
       "      <td>-0.243529</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Rob Liefeld was in middle school, some bully accused him of having a foot fetish, and he resolved on the spot to never do anything to warrant such rumours in his life.</td>\n",
       "      <td>False</td>\n",
       "      <td>When Rob Liefeld was in middle school, some bully accused him of having a foot fetish, and he resolved on the spot to never do_NEG anything_NEG to_NEG warrant_NEG such_NEG rumours_NEG in_NEG his_NEG life._NEG</td>\n",
       "      <td>False</td>\n",
       "      <td>[middle, such]</td>\n",
       "      <td>[school, foot, fetish, spot, rumours, life]</td>\n",
       "      <td>[(Liefeld, []), (some, []), (he, [3])]</td>\n",
       "      <td>[accused, resolved]</td>\n",
       "      <td>[[was, ,, some, bully, him, of, ,, and, resolved], [he, on, do, .]]</td>\n",
       "      <td>-0.3160</td>\n",
       "      <td>[[], [Rob], [When, Liefeld, in]]</td>\n",
       "      <td>[(Rob Liefeld, PERSON)]</td>\n",
       "      <td>-0.8074</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.065217</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>33</td>\n",
       "      <td>-0.158000</td>\n",
       "      <td>False</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOONIES + ARU SHAH To locate a magical cure that will save his dying father, 13-yr-old Sam goes to the land of Decapolis with his 2 bf’s. They’re hailed as heroes, returned from the past - which has its benefits. But then a 4th \"hero” shows up - the school bully.</td>\n",
       "      <td>False</td>\n",
       "      <td>GOONIES + ARU SHAH To locate a magical cure that will save his dying father, 13-yr-old Sam goes to the land of Decapolis with his 2 bf’s. They’re hailed as heroes, returned from the past - which has its benefits. But then a 4th \"hero” shows up - the school bully.</td>\n",
       "      <td>False</td>\n",
       "      <td>[magical, old, 4th]</td>\n",
       "      <td>[cure, father, yr, land, bf, heroes, past, benefits, hero, school, bully]</td>\n",
       "      <td>[(GOONIES, []), (that, []), (Sam, []), (which, []), (hero, [])]</td>\n",
       "      <td>[SHAH, goes, hailed, shows]</td>\n",
       "      <td>[[ARU], [locate, ,, Sam, to, with, .], [They, ’re, as, ,, returned, .], [But, then, hero, ”, up, -, bully, .]]</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>[[+, SHAH], [], [ARU], [], [13, old], [], []]</td>\n",
       "      <td>[(13, CARDINAL), (Sam, PERSON), (Decapolis, GPE), (2, CARDINAL), (4th, ORDINAL)]</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.003676</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>51</td>\n",
       "      <td>0.015875</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                     Tweet  \\\n",
       "0                                                                                           thinking about how some girl gets paid six figures to act like a dog on onlyfans and all these girls that acted like dogs and cats in high school got bullied out of their bag   \n",
       "1                                        I got bullied a lot in grade school, jump forward to high school when my art was good enough to do photo realistic portraits and bullies suddenly wanted art of their girl friends... yeah told a lot of people \"NO\", good times.   \n",
       "2                                  I’ve always wanted to transfer school sa US because most of the movies that I’ve seen made me believe that the new kid always gets bullied and I just wanna have that “Try me bitch” moment in case someone actually tries to bully me.   \n",
       "3                                                                                             When Rob Liefeld was in middle school, some bully accused him of having a foot fetish, and he resolved on the spot to never do anything to warrant such rumours in his life.   \n",
       "4  GOONIES + ARU SHAH To locate a magical cure that will save his dying father, 13-yr-old Sam goes to the land of Decapolis with his 2 bf’s. They’re hailed as heroes, returned from the past - which has its benefits. But then a 4th \"hero” shows up - the school bully.   \n",
       "\n",
       "   Has Question Mark  \\\n",
       "0              False   \n",
       "1              False   \n",
       "2              False   \n",
       "3              False   \n",
       "4              False   \n",
       "\n",
       "                                                                                                                                                                                                                                                             Negated Tweet  \\\n",
       "0                                                                                           thinking about how some girl gets paid six figures to act like a dog on onlyfans and all these girls that acted like dogs and cats in high school got bullied out of their bag   \n",
       "1                                        I got bullied a lot in grade school, jump forward to high school when my art was good enough to do photo realistic portraits and bullies suddenly wanted art of their girl friends... yeah told a lot of people \"NO\", good times.   \n",
       "2                                  I’ve always wanted to transfer school sa US because most of the movies that I’ve seen made me believe that the new kid always gets bullied and I just wanna have that “Try me bitch” moment in case someone actually tries to bully me.   \n",
       "3                                                         When Rob Liefeld was in middle school, some bully accused him of having a foot fetish, and he resolved on the spot to never do_NEG anything_NEG to_NEG warrant_NEG such_NEG rumours_NEG in_NEG his_NEG life._NEG   \n",
       "4  GOONIES + ARU SHAH To locate a magical cure that will save his dying father, 13-yr-old Sam goes to the land of Decapolis with his 2 bf’s. They’re hailed as heroes, returned from the past - which has its benefits. But then a 4th \"hero” shows up - the school bully.   \n",
       "\n",
       "   Has Discourse Marker                     Adjectives  \\\n",
       "0                 False                         [high]   \n",
       "1                 False  [high, good, realistic, good]   \n",
       "2                 False                    [most, new]   \n",
       "3                 False                 [middle, such]   \n",
       "4                 False            [magical, old, 4th]   \n",
       "\n",
       "                                                                                                  Nouns  \\\n",
       "0                                        [girl, figures, dog, onlyfans, girls, dogs, cats, school, bag]   \n",
       "1  [lot, grade, school, school, art, photo, portraits, bullies, art, girl, friends, lot, people, times]   \n",
       "2                                                                   [school, movies, kid, moment, case]   \n",
       "3                                                           [school, foot, fetish, spot, rumours, life]   \n",
       "4                             [cure, father, yr, land, bf, heroes, past, benefits, hero, school, bully]   \n",
       "\n",
       "                                                                                       Subjects  \\\n",
       "0                                                                                  [(that, [])]   \n",
       "1                                                                                   [(art, [])]   \n",
       "2  [(I, [1]), (most, []), (I, [1]), (me, [1]), (I, [1]), (wanna, []), (me, [1]), (someone, [])]   \n",
       "3                                                        [(Liefeld, []), (some, []), (he, [3])]   \n",
       "4                               [(GOONIES, []), (that, []), (Sam, []), (which, []), (hero, [])]   \n",
       "\n",
       "                                    Complements  \\\n",
       "0              [thinking, girls, cats, bullied]   \n",
       "1  [bullied, jump, good, bullies, wanted, told]   \n",
       "2                                [wanted, have]   \n",
       "3                           [accused, resolved]   \n",
       "4                   [SHAH, goes, hailed, shows]   \n",
       "\n",
       "                                                                                                                                                            Dependency Children  \\\n",
       "0                                                                                                                          [[about], [all, these, acted], [], [dogs, got, out]]   \n",
       "1                                                                     [[I, got, lot, in, ,, jump, wanted, ...], [forward, to, was], [], [suddenly, art], [yeah, lot, \", NO, .]]   \n",
       "2  [[I, ’ve, always, transfer, .], [me, bullied], [that, kid, always, gets, and, have], [wanna, Try, tries], [that, “, bitch, moment, in], [me, ”], [someone, actually, bully]]   \n",
       "3                                                                                                           [[was, ,, some, bully, him, of, ,, and, resolved], [he, on, do, .]]   \n",
       "4                                                                [[ARU], [locate, ,, Sam, to, with, .], [They, ’re, as, ,, returned, .], [But, then, hero, ”, up, -, bully, .]]   \n",
       "\n",
       "   Dependency Sentiment                        Aux/pronouns dependence  \\\n",
       "0                0.0000                                       [[], []]   \n",
       "1                0.0000                        [[], [when, art, good]]   \n",
       "2               -1.7047              [[], [], [sa], [], [], [I, just]]   \n",
       "3               -0.3160               [[], [Rob], [When, Liefeld, in]]   \n",
       "4                0.0635  [[+, SHAH], [], [ARU], [], [13, old], [], []]   \n",
       "\n",
       "                                                                     Named Entities  \\\n",
       "0                                                                 [(six, CARDINAL)]   \n",
       "1                                                                                []   \n",
       "2                                                             [(school sa US, ORG)]   \n",
       "3                                                           [(Rob Liefeld, PERSON)]   \n",
       "4  [(13, CARDINAL), (Sam, PERSON), (Decapolis, GPE), (2, CARDINAL), (4th, ORDINAL)]   \n",
       "\n",
       "   Sentiment Score  Synonyms Sentiment Score  Has Third Person Verb  \\\n",
       "0          -0.0258                   -0.6908                  False   \n",
       "1           0.7184                    0.9997                  False   \n",
       "2          -0.8074                    0.8074                  False   \n",
       "3          -0.8074                    0.5423                  False   \n",
       "4           0.0516                    0.9976                  False   \n",
       "\n",
       "   Average SentiWordNet Score  Has 1st Person  Has 2nd Person  Has 3rd Person  \\\n",
       "0                    0.028846           False           False           False   \n",
       "1                    0.033088           False           False           False   \n",
       "2                    0.007576            True           False           False   \n",
       "3                   -0.065217           False           False            True   \n",
       "4                   -0.003676           False           False           False   \n",
       "\n",
       "   Word Count  Adjusted SentiWordScore  Has Capital Word  Classification  \n",
       "0          35                 0.000000             False  Critique/Other  \n",
       "1          42                 0.000000              True  Critique/Other  \n",
       "2          44                -0.243529              True  Critique/Other  \n",
       "3          33                -0.158000             False  Critique/Other  \n",
       "4          51                 0.015875              True  Critique/Other  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv\n",
    "#df.to_csv('/Users/philippebeliveau/Library/Mobile Documents/com~apple~CloudDocs/_Bureau_/Master/Winter_2024/Text_mining/Project/Dataframe/smaller_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv \n",
    "#df1 = pd.read_csv('/Users/philippebeliveau/Library/Mobile Documents/com~apple~CloudDocs/_Bureau_/Master/Winter_2024/Text_mining/Project/Dataframe/smaller_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Rules\n",
    "1. \"Target identification\" -> Identify if there is a target, Does it have a 2nd or 3rd person subject, and or Is the name entities is a PERSON or NORP. If that is true, then increase the threshold required to classify as an insult. \n",
    "2. \"Absence of target\" -> If the Entities are not speaking about a Person or NORP, and have no 2nd and 3rd person subject, then increase the required sentiment score. \n",
    "3. \"Discourse or Insults?\" -> Make a rule regarding Discourse marker. If a tweet shows more of a discourse,the threshold for the sentiment should be very high. Does the tweet as a question mark or a discourse marker such as [\"but\", \"however\", \"on the other hand\", \"yet\", \"nevertheless\", \"although\",\n",
    "                     \"while\", \"even though\", \"despite\", \"regardless\", \"rather\", \"instead\",\n",
    "                     \"meanwhile\", \"in contrast\", \"conversely\", \"compared to\", \"on the contrary\",\n",
    "                     \"besides\", \"furthermore\", \"moreover\", \"in addition\", \"additionally\",\n",
    "                     \"further\", \"also\", \"next\", \"then\", \"afterward\", \"finally\",\n",
    "                     \"therefore\", \"thus\", \"hence\", \"consequently\", \"as a result\", \"so\",\n",
    "                     \"because\", \"since\", \"due to\", \"as long as\", \"provided that\", \"given that\",\n",
    "                     \"for example\", \"for instance\", \"specifically\", \"in particular\", \"to illustrate\",\n",
    "                     \"to clarify\", \"to explain\", \"in summary\", \"to sum up\", \"in conclusion\",\n",
    "                     \"indeed\", \"certainly\", \"obviously\", \"clearly\", \"undoubtedly\", \"surely\"]. \n",
    "4. \"SentiWordScore\" -> If the Adjusted SentiWordScore is smaller then -0.6, then classify it automatically as an Insults\n",
    "5. \"Expressiveness\" -> If the tweets have row['Has Capital Word'] == True, then make the threshold of the sentiment lower. \n",
    "6. \"Degree of Positiveness\" -> If the general tweet is positive, such as row['Sentiment Score'] > 0.3 and row['Synonyms Sentiment Score'] > 0.2, make it more different to be classify as insult and increase the threshold of Adjusted SentiWordScore\n",
    "7. \"Age tweets?\" -> If education_terms = ['school', 'schools', 'college', 'graduation']\n",
    "    if any(term in str(word) for term in education_terms for word in row[['Nouns', 'Tweet']]):\n",
    "        return 'Critique/Other'\n",
    "8. \"Description tweets\" -> Tweets with name entities such as Percent, Quanity, Ordinal etc should have more difficult threshold to be classify as insults. \n",
    "Note, the score here refers to this one: Adjusted SentiWordScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 rules classification \n",
    "def classify(row):\n",
    "    # Rule 1: Threshold for insult\n",
    "    insult_threshold = -0.2  # Define your threshold here\n",
    "\n",
    "    # Rule 2: Absence of target\n",
    "    if not any(entity in ['PERSON', 'NORP'] for entity in row['Named Entities']) and not row['Has 2nd Person'] and not row['Has 3rd Person']:\n",
    "        insult_threshold -= 0.4  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 3: Discourse or Insults?\n",
    "    discourse_markers = [\"but\", \"however\", \"on the other hand\", \"yet\", \"nevertheless\", \"although\",\n",
    "                         \"while\", \"even though\", \"despite\", \"regardless\", \"rather\", \"instead\",\n",
    "                         \"meanwhile\", \"in contrast\", \"conversely\", \"compared to\", \"on the contrary\",\n",
    "                         \"besides\", \"furthermore\", \"moreover\", \"in addition\", \"additionally\",\n",
    "                         \"further\", \"also\", \"next\", \"then\", \"afterward\", \"finally\",\n",
    "                         \"therefore\", \"thus\", \"hence\", \"consequently\", \"as a result\", \"so\",\n",
    "                         \"because\", \"since\", \"due to\", \"as long as\", \"provided that\", \"given that\",\n",
    "                         \"for example\", \"for instance\", \"specifically\", \"in particular\", \"to illustrate\",\n",
    "                         \"to clarify\", \"to explain\", \"in summary\", \"to sum up\", \"in conclusion\",\n",
    "                         \"indeed\", \"certainly\", \"obviously\", \"clearly\", \"undoubtedly\", \"surely\"]\n",
    "    if any(marker in row['Tweet'] for marker in discourse_markers):\n",
    "        insult_threshold -= 0.2  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 4: SentiWordScore\n",
    "    if row['Adjusted SentiWordScore'] < -0.6:\n",
    "        return 'Insults'\n",
    "\n",
    "    # Rule 5: Expressiveness\n",
    "    if row['Has Capital Word']:\n",
    "        insult_threshold += 0.1  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 6: Degree of Positiveness\n",
    "    if row['Sentiment Score'] > 0.3 and row['Synonyms Sentiment Score'] > 0.2:\n",
    "        insult_threshold -= 0.2  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 7: Age tweets?\n",
    "    education_terms = ['school', 'schools', 'college', 'graduation']\n",
    "    if any(term in str(word) for term in education_terms for word in row[['Nouns', 'Tweet']]):\n",
    "        return 'Critique/Other'\n",
    "\n",
    "    # Rule 8: Description tweets\n",
    "    if any(entity in ['PERCENT', 'QUANTITY', 'ORDINAL', 'TIME'] for entity in row['Named Entities']):\n",
    "        insult_threshold -= 0.8  # Adjust the threshold as needed\n",
    "        \n",
    "    # Final classification\n",
    "    if row['Adjusted SentiWordScore'] < insult_threshold:\n",
    "        return 'Insults'\n",
    "    \n",
    "    else:\n",
    "        return 'Critique/Other'\n",
    "\n",
    "df1['Classification'] = df1.apply(classify, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4174386755952381"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[df1['Classification'] == 'Insults', 'Adjusted SentiWordScore'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critique/Other    552\n",
      "Insults            48\n",
      "Name: Classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get counts\n",
    "classification_counts = df1['Classification'].value_counts()\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(classification_counts.index, classification_counts.values, color=['blue', 'orange'])\n",
    "plt.xlabel('Classification')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Histogram of Tweet Classifications')\n",
    "plt.show()\n",
    "print(classification_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Adjusted SentiWordScore</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>Dependency Children</th>\n",
       "      <th>Aux/pronouns dependence</th>\n",
       "      <th>Has 1st Person</th>\n",
       "      <th>Has 2nd Person</th>\n",
       "      <th>Has 3rd Person</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Has Discourse Marker</th>\n",
       "      <th>Has Question Mark</th>\n",
       "      <th>Has Third Person Verb</th>\n",
       "      <th>Adjectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Frankly, you're no better than my idiot cousin (not one of the cops) who tries to tell me my muslim coworkers and friends are secretly evil terrorists only here to kill us all and force sharia law on us all. The prejudice in this world is disgusting. All of you.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.318767</td>\n",
       "      <td>[(you, [2]), (who, []), (prejudice, [])]</td>\n",
       "      <td>['re, better, friends, are, force, is, disgusting, All]</td>\n",
       "      <td>[[Frankly, ,, you, better, are, .], [], [secretly, terrorists, here, kill], [law, on], [prejudice, disgusting, .], [of, .]]</td>\n",
       "      <td>[[Frankly, ,, you, better, are, .], [secretly, terrorists, here, kill], [prejudice, disgusting, .]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(muslim, NORP)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[better, muslim, evil, sharia, disgusting]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>_caldera you are an idiot. Your piece on face coveries by Muslim countries is unreal. It's like you know everything your saying is bullshit made up just to respond to people who think face shields are dumb. Any face covering should lower the numbers. Journalism is dead.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.236813</td>\n",
       "      <td>[(you, [2]), (piece, []), (It, [3]), (you, [2]), (everything, []), (saying, []), (who, []), (shields, []), (face, []), (covering, []), (Journalism, [])]</td>\n",
       "      <td>[are, is, unreal, 's, dumb, lower, is, dead]</td>\n",
       "      <td>[[_, you, idiot, .], [piece, unreal, .], [It, like, know, made, .], [everything, saying, bullshit], [up, respond], [shields, dumb], [covering, should, numbers, .], [Journalism, dead, .]]</td>\n",
       "      <td>[[_, you, idiot, .], [piece, unreal, .], [It, like, know, made, .], [everything, saying, bullshit], [shields, dumb], [], [Journalism, dead, .]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[(Muslim, NORP)]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Muslim, unreal, dumb, dead]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Bloody hell I haven't been a size 4 since I was 15, uni what have you done to me</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.340400</td>\n",
       "      <td>[(I, [1]), (I, [1]), (you, [2])]</td>\n",
       "      <td>[been]</td>\n",
       "      <td>[[hell, I, have, n't, size, was, ,, uni, done], [what, have, you, to]]</td>\n",
       "      <td>[[], [hell, I, have, n't, size, was, ,, uni, done], [since, I, 15], []]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(4, CARDINAL), (15, DATE)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Bloody]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Cyber bullying: Out you go! | Malay Mail Online</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.299700</td>\n",
       "      <td>[(you, [2])]</td>\n",
       "      <td>[go, Online]</td>\n",
       "      <td>[[bullying, Out, you, !], [|, Mail]]</td>\n",
       "      <td>[[], [], [Malay], [|, Mail]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(Malay Mail Online, PERSON)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>you're all a bunch of dumb motherfuckers. Put a nigger dick in your mouth and shut the fuck up.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.350967</td>\n",
       "      <td>[(you, [2])]</td>\n",
       "      <td>['re, Put, shut]</td>\n",
       "      <td>[[you, bunch, .], [dick, in, and, shut, .], [fuck, up]]</td>\n",
       "      <td>[[you, bunch, .]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dumb, nigger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>: HAHAHAHA FUCK YOU DUMB ASS NIGGER. YOUR THE REASON US WHITES HATE BLACK PEOPLE</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.881400</td>\n",
       "      <td>[(YOU, [2])]</td>\n",
       "      <td>[DUMB, REASON]</td>\n",
       "      <td>[[FUCK, YOU, NIGGER, .], [YOUR, THE, HATE]]</td>\n",
       "      <td>[[HAHAHAHA], [], [], [WHITES, PEOPLE]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(US, GPE)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>all my bitches bad, you'll never see me around ugly females Ã°ÂÂÂ</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.542300</td>\n",
       "      <td>[(you, [2])]</td>\n",
       "      <td>[see]</td>\n",
       "      <td>[[bad, ,, you, 'll, never, me, around, Ã, °, ÂÂÂ]]</td>\n",
       "      <td>[[], []]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(ÂÂÂ, CARDINAL)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[bad, ugly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>___ you dumb fuck. I'm saying that's what I am. That's why people call me sand nigger</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.165600</td>\n",
       "      <td>[(you, [2]), (I, [1]), (that, []), (I, [1]), (That, []), (people, [])]</td>\n",
       "      <td>[_, fuck, saying, 's]</td>\n",
       "      <td>[[_, _], [dumb, .], [I, 'm, 's, .], [that, am], [what, I], [That, call], [why, people, me, sand, nigger]]</td>\n",
       "      <td>[[], [that, am], [what, I], [That, call]]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Fuck you uncle drew you don't follow anyone who retweets you dumb ugly fucken nigger</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.324300</td>\n",
       "      <td>[(uncle, []), (you, [2]), (who, [])]</td>\n",
       "      <td>[drew]</td>\n",
       "      <td>[[uncle, follow, nigger], [you, do, n't, anyone]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[fucken]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Why do nearly half of all Muslims want to kill people for who they have sex with?</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.690800</td>\n",
       "      <td>[(half, []), (they, [3])]</td>\n",
       "      <td>[want]</td>\n",
       "      <td>[[Why, do, half, kill, ?]]</td>\n",
       "      <td>[[], [all], [they, sex, with]]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[(nearly half, CARDINAL), (Muslims, NORP)]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Nigger means ignorant, unknowing.. IGNORANCE COMES IN ANY COLOR! You dumb fuck. &amp;amp; you look like that ignorant fuck, now.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.267800</td>\n",
       "      <td>[(Nigger, []), (you, [2])]</td>\n",
       "      <td>[means, ignorant, fuck, amp, look]</td>\n",
       "      <td>[[Nigger, ignorant, ,, unknowing, .., COMES, !], [You, dumb, .], [], [&amp;, ;, you, like, now, .]]</td>\n",
       "      <td>[[], [IGNORANCE, IN], [ANY], []]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[ignorant, dumb, ignorant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>I’ve been doing homecoming hair on girls that would have definitely bullied me in high school all day so yes same kill me</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.690800</td>\n",
       "      <td>[(I, [1]), (that, []), (same, [])]</td>\n",
       "      <td>[doing]</td>\n",
       "      <td>[[I, ’ve, been, hair, on, kill]]</td>\n",
       "      <td>[[], [], [], []]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[(all day, DATE)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[high, same]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Son you know you fucked up when you said you found a \"nigger\" to fuck right __neucere you dumb for that fuck</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.425375</td>\n",
       "      <td>[(you, [2]), (you, [2]), (you, [2]), (you, [2]), (you, [2])]</td>\n",
       "      <td>[fucked, neucere]</td>\n",
       "      <td>[[Son, know, you, up, said], [you, nigger, fuck], [dumb], [you, for]]</td>\n",
       "      <td>[[]]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>FUCKING ASSHOLES GO THE FUCK YOU SLEEP YOU DUMB NIGGER BITCH COCKSUCKER FUCK I SWEAR TO FUCKING GOD IF YOU HAVE ONE MORE LOUD ASS CONVERSATION THAT ECHOES THROUGH THE WHOLE ASS HOUSE IM GONNA MOP THE FUCKING FLOOR WITH YOUR DEAD BODY</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.573850</td>\n",
       "      <td>[(ASSHOLES, []), (YOU, [2]), (YOU, [2]), (I, [1]), (YOU, [2]), (THAT, [])]</td>\n",
       "      <td>[DUMB, SWEAR, HAVE]</td>\n",
       "      <td>[[ASSHOLES, FUCK], [GO, YOU, COCKSUCKER], [FUCK, I, TO], [IF, YOU, CONVERSATION]]</td>\n",
       "      <td>[[], [FUCKING], [THE, SLEEP], [], [NIGGER], [BITCH], [], [FUCKING], [], [], [], [WHOLE, ASS], [THE, HOUSE], [], [IM, GONNA, FLOOR], []]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(WHOLE, ORG), (GONNA, PERSON)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[LOUD, DEAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>doctorow: They have told me that writing this down is 'intimidation'</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.210750</td>\n",
       "      <td>[(They, [3])]</td>\n",
       "      <td>[told]</td>\n",
       "      <td>[[doctorow, :, They, have, me, is, '], [that, writing, ', intimidation]]</td>\n",
       "      <td>[[], [that, writing, ', intimidation]]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>_EA LOL. A staged, escorted video shot at gun point. How stupid can you get?</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.105340</td>\n",
       "      <td>[(A, []), (you, [2])]</td>\n",
       "      <td>[_, LOL, staged, escorted, get]</td>\n",
       "      <td>[[], [EA, .], [A, ,, escorted, .], [shot], [stupid, can, you, ?]]</td>\n",
       "      <td>[[EA, .], []]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(EA LOL, PERSON)]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[stupid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>is a racist endeavour. She believes in a master race of white Europeans who should be overlords of Christian and Muslim Palestinians, steal their land and treat them like vermin. She is the vile racist. Say NO to Israeli apartheid.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.106533</td>\n",
       "      <td>[(She, [3]), (who, []), (She, [3])]</td>\n",
       "      <td>[is, believes, steal, treat, is, Say]</td>\n",
       "      <td>[[endeavour, .], [She, in, ,, steal, .], [land, and, treat], [them, like], [She, racist, .], [NO, to, .]]</td>\n",
       "      <td>[[endeavour, .], [white], [], [who, should, overlords], [], [Christian, Muslim], [She, racist, .]]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[(Europeans, NORP), (Christian, NORP), (Muslim, NORP), (Palestinians, NORP), (Israeli, NORP)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[racist, white, Christian, vile, Israeli]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>The fuck is this nigger shit!? -grant HAHAHAHAHAHAHA ITS MARTIN YOU DUMB ASS!!! /oel8V0wOHP</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.410500</td>\n",
       "      <td>[(fuck, []), (YOU, [2])]</td>\n",
       "      <td>[is, -grant, DUMB, /oel8V0wOHP]</td>\n",
       "      <td>[[fuck, shit, !, ?], [], [HAHAHAHAHAHAHA, YOU, ASS, !, !, !], []]</td>\n",
       "      <td>[[fuck, shit, !, ?], [MARTIN], [ITS], []]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(HAHAHAHAHAHAHA, ORG)]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[nigger, -grant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>she's a nigger. If you're a black girl, u should just shut up. ur black. ur a nigger. nobody likes u. I think ur disgusting. trash.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.227987</td>\n",
       "      <td>[(she, [3]), (you, [2]), (u, []), (nobody, []), (I, [1]), (ur, [])]</td>\n",
       "      <td>['s, shut, black, ur, likes, think, trash]</td>\n",
       "      <td>[[she, nigger, .], ['re, ,, u, should, just, up, .], [ur, .], [nigger, .], [nobody, u.], [I, disgusting, .], [ur], [.]]</td>\n",
       "      <td>[[she, nigger, .], [If, you, girl], [], [], [], [ur]]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[black, black]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>It's about harassment… and street fights?</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.200950</td>\n",
       "      <td>[(It, [3])]</td>\n",
       "      <td>['s, fights]</td>\n",
       "      <td>[[It, about, …, and, fights, ?], [street]]</td>\n",
       "      <td>[[It, about, …, and, fights, ?]]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                              Tweet  \\\n",
       "597          Frankly, you're no better than my idiot cousin (not one of the cops) who tries to tell me my muslim coworkers and friends are secretly evil terrorists only here to kill us all and force sharia law on us all. The prejudice in this world is disgusting. All of you.   \n",
       "504  _caldera you are an idiot. Your piece on face coveries by Muslim countries is unreal. It's like you know everything your saying is bullshit made up just to respond to people who think face shields are dumb. Any face covering should lower the numbers. Journalism is dead.   \n",
       "490                                                                                                                                                                                                Bloody hell I haven't been a size 4 since I was 15, uni what have you done to me   \n",
       "319                                                                                                                                                                                                                                 Cyber bullying: Out you go! | Malay Mail Online   \n",
       "102                                                                                                                                                                                 you're all a bunch of dumb motherfuckers. Put a nigger dick in your mouth and shut the fuck up.   \n",
       "126                                                                                                                                                                                                : HAHAHAHA FUCK YOU DUMB ASS NIGGER. YOUR THE REASON US WHITES HATE BLACK PEOPLE   \n",
       "298                                                                                                                                                                                                            all my bitches bad, you'll never see me around ugly females Ã°ÂÂÂ   \n",
       "154                                                                                                                                                                                           ___ you dumb fuck. I'm saying that's what I am. That's why people call me sand nigger   \n",
       "128                                                                                                                                                                                            Fuck you uncle drew you don't follow anyone who retweets you dumb ugly fucken nigger   \n",
       "563                                                                                                                                                                                               Why do nearly half of all Muslims want to kill people for who they have sex with?   \n",
       "195                                                                                                                                                    Nigger means ignorant, unknowing.. IGNORANCE COMES IN ANY COLOR! You dumb fuck. &amp; you look like that ignorant fuck, now.   \n",
       "65                                                                                                                                                        I’ve been doing homecoming hair on girls that would have definitely bullied me in high school all day so yes same kill me   \n",
       "198                                                                                                                                                                    Son you know you fucked up when you said you found a \"nigger\" to fuck right __neucere you dumb for that fuck   \n",
       "119                                       FUCKING ASSHOLES GO THE FUCK YOU SLEEP YOU DUMB NIGGER BITCH COCKSUCKER FUCK I SWEAR TO FUCKING GOD IF YOU HAVE ONE MORE LOUD ASS CONVERSATION THAT ECHOES THROUGH THE WHOLE ASS HOUSE IM GONNA MOP THE FUCKING FLOOR WITH YOUR DEAD BODY   \n",
       "449                                                                                                                                                                                                            doctorow: They have told me that writing this down is 'intimidation'   \n",
       "309                                                                                                                                                                                                    _EA LOL. A staged, escorted video shot at gun point. How stupid can you get?   \n",
       "559                                         is a racist endeavour. She believes in a master race of white Europeans who should be overlords of Christian and Muslim Palestinians, steal their land and treat them like vermin. She is the vile racist. Say NO to Israeli apartheid.   \n",
       "191                                                                                                                                                                                     The fuck is this nigger shit!? -grant HAHAHAHAHAHAHA ITS MARTIN YOU DUMB ASS!!! /oel8V0wOHP   \n",
       "184                                                                                                                                             she's a nigger. If you're a black girl, u should just shut up. ur black. ur a nigger. nobody likes u. I think ur disgusting. trash.   \n",
       "415                                                                                                                                                                                                                                       It's about harassment… and street fights?   \n",
       "\n",
       "    Classification  Adjusted SentiWordScore  \\\n",
       "597        Insults                -0.318767   \n",
       "504        Insults                -0.236813   \n",
       "490        Insults                -0.340400   \n",
       "319        Insults                -0.299700   \n",
       "102        Insults                -0.350967   \n",
       "126        Insults                -0.881400   \n",
       "298        Insults                -0.542300   \n",
       "154        Insults                -0.165600   \n",
       "128        Insults                -0.324300   \n",
       "563        Insults                -0.690800   \n",
       "195        Insults                -0.267800   \n",
       "65         Insults                -0.690800   \n",
       "198        Insults                -0.425375   \n",
       "119        Insults                -0.573850   \n",
       "449        Insults                -0.210750   \n",
       "309        Insults                -0.105340   \n",
       "559        Insults                -0.106533   \n",
       "191        Insults                -0.410500   \n",
       "184        Insults                -0.227987   \n",
       "415        Insults                -0.200950   \n",
       "\n",
       "                                                                                                                                                     Subjects  \\\n",
       "597                                                                                                                  [(you, [2]), (who, []), (prejudice, [])]   \n",
       "504  [(you, [2]), (piece, []), (It, [3]), (you, [2]), (everything, []), (saying, []), (who, []), (shields, []), (face, []), (covering, []), (Journalism, [])]   \n",
       "490                                                                                                                          [(I, [1]), (I, [1]), (you, [2])]   \n",
       "319                                                                                                                                              [(you, [2])]   \n",
       "102                                                                                                                                              [(you, [2])]   \n",
       "126                                                                                                                                              [(YOU, [2])]   \n",
       "298                                                                                                                                              [(you, [2])]   \n",
       "154                                                                                    [(you, [2]), (I, [1]), (that, []), (I, [1]), (That, []), (people, [])]   \n",
       "128                                                                                                                      [(uncle, []), (you, [2]), (who, [])]   \n",
       "563                                                                                                                                 [(half, []), (they, [3])]   \n",
       "195                                                                                                                                [(Nigger, []), (you, [2])]   \n",
       "65                                                                                                                         [(I, [1]), (that, []), (same, [])]   \n",
       "198                                                                                              [(you, [2]), (you, [2]), (you, [2]), (you, [2]), (you, [2])]   \n",
       "119                                                                                [(ASSHOLES, []), (YOU, [2]), (YOU, [2]), (I, [1]), (YOU, [2]), (THAT, [])]   \n",
       "449                                                                                                                                             [(They, [3])]   \n",
       "309                                                                                                                                     [(A, []), (you, [2])]   \n",
       "559                                                                                                                       [(She, [3]), (who, []), (She, [3])]   \n",
       "191                                                                                                                                  [(fuck, []), (YOU, [2])]   \n",
       "184                                                                                       [(she, [3]), (you, [2]), (u, []), (nobody, []), (I, [1]), (ur, [])]   \n",
       "415                                                                                                                                               [(It, [3])]   \n",
       "\n",
       "                                                 Complements  \\\n",
       "597  ['re, better, friends, are, force, is, disgusting, All]   \n",
       "504             [are, is, unreal, 's, dumb, lower, is, dead]   \n",
       "490                                                   [been]   \n",
       "319                                             [go, Online]   \n",
       "102                                         ['re, Put, shut]   \n",
       "126                                           [DUMB, REASON]   \n",
       "298                                                    [see]   \n",
       "154                                    [_, fuck, saying, 's]   \n",
       "128                                                   [drew]   \n",
       "563                                                   [want]   \n",
       "195                       [means, ignorant, fuck, amp, look]   \n",
       "65                                                   [doing]   \n",
       "198                                        [fucked, neucere]   \n",
       "119                                      [DUMB, SWEAR, HAVE]   \n",
       "449                                                   [told]   \n",
       "309                          [_, LOL, staged, escorted, get]   \n",
       "559                    [is, believes, steal, treat, is, Say]   \n",
       "191                          [is, -grant, DUMB, /oel8V0wOHP]   \n",
       "184               ['s, shut, black, ur, likes, think, trash]   \n",
       "415                                             ['s, fights]   \n",
       "\n",
       "                                                                                                                                                                            Dependency Children  \\\n",
       "597                                                                 [[Frankly, ,, you, better, are, .], [], [secretly, terrorists, here, kill], [law, on], [prejudice, disgusting, .], [of, .]]   \n",
       "504  [[_, you, idiot, .], [piece, unreal, .], [It, like, know, made, .], [everything, saying, bullshit], [up, respond], [shields, dumb], [covering, should, numbers, .], [Journalism, dead, .]]   \n",
       "490                                                                                                                      [[hell, I, have, n't, size, was, ,, uni, done], [what, have, you, to]]   \n",
       "319                                                                                                                                                        [[bullying, Out, you, !], [|, Mail]]   \n",
       "102                                                                                                                                     [[you, bunch, .], [dick, in, and, shut, .], [fuck, up]]   \n",
       "126                                                                                                                                                 [[FUCK, YOU, NIGGER, .], [YOUR, THE, HATE]]   \n",
       "298                                                                                                                                       [[bad, ,, you, 'll, never, me, around, Ã, °, ÂÂÂ]]   \n",
       "154                                                                                   [[_, _], [dumb, .], [I, 'm, 's, .], [that, am], [what, I], [That, call], [why, people, me, sand, nigger]]   \n",
       "128                                                                                                                                           [[uncle, follow, nigger], [you, do, n't, anyone]]   \n",
       "563                                                                                                                                                                  [[Why, do, half, kill, ?]]   \n",
       "195                                                                                             [[Nigger, ignorant, ,, unknowing, .., COMES, !], [You, dumb, .], [], [&, ;, you, like, now, .]]   \n",
       "65                                                                                                                                                             [[I, ’ve, been, hair, on, kill]]   \n",
       "198                                                                                                                       [[Son, know, you, up, said], [you, nigger, fuck], [dumb], [you, for]]   \n",
       "119                                                                                                           [[ASSHOLES, FUCK], [GO, YOU, COCKSUCKER], [FUCK, I, TO], [IF, YOU, CONVERSATION]]   \n",
       "449                                                                                                                    [[doctorow, :, They, have, me, is, '], [that, writing, ', intimidation]]   \n",
       "309                                                                                                                           [[], [EA, .], [A, ,, escorted, .], [shot], [stupid, can, you, ?]]   \n",
       "559                                                                                   [[endeavour, .], [She, in, ,, steal, .], [land, and, treat], [them, like], [She, racist, .], [NO, to, .]]   \n",
       "191                                                                                                                           [[fuck, shit, !, ?], [], [HAHAHAHAHAHAHA, YOU, ASS, !, !, !], []]   \n",
       "184                                                                     [[she, nigger, .], ['re, ,, u, should, just, up, .], [ur, .], [nigger, .], [nobody, u.], [I, disgusting, .], [ur], [.]]   \n",
       "415                                                                                                                                                  [[It, about, …, and, fights, ?], [street]]   \n",
       "\n",
       "                                                                                                                             Aux/pronouns dependence  \\\n",
       "597                                              [[Frankly, ,, you, better, are, .], [secretly, terrorists, here, kill], [prejudice, disgusting, .]]   \n",
       "504  [[_, you, idiot, .], [piece, unreal, .], [It, like, know, made, .], [everything, saying, bullshit], [shields, dumb], [], [Journalism, dead, .]]   \n",
       "490                                                                          [[], [hell, I, have, n't, size, was, ,, uni, done], [since, I, 15], []]   \n",
       "319                                                                                                                     [[], [], [Malay], [|, Mail]]   \n",
       "102                                                                                                                                [[you, bunch, .]]   \n",
       "126                                                                                                           [[HAHAHAHA], [], [], [WHITES, PEOPLE]]   \n",
       "298                                                                                                                                         [[], []]   \n",
       "154                                                                                                        [[], [that, am], [what, I], [That, call]]   \n",
       "128                                                                                                                                             [[]]   \n",
       "563                                                                                                                   [[], [all], [they, sex, with]]   \n",
       "195                                                                                                                 [[], [IGNORANCE, IN], [ANY], []]   \n",
       "65                                                                                                                                  [[], [], [], []]   \n",
       "198                                                                                                                                             [[]]   \n",
       "119          [[], [FUCKING], [THE, SLEEP], [], [NIGGER], [BITCH], [], [FUCKING], [], [], [], [WHOLE, ASS], [THE, HOUSE], [], [IM, GONNA, FLOOR], []]   \n",
       "449                                                                                                           [[], [that, writing, ', intimidation]]   \n",
       "309                                                                                                                                    [[EA, .], []]   \n",
       "559                                               [[endeavour, .], [white], [], [who, should, overlords], [], [Christian, Muslim], [She, racist, .]]   \n",
       "191                                                                                                        [[fuck, shit, !, ?], [MARTIN], [ITS], []]   \n",
       "184                                                                                            [[she, nigger, .], [If, you, girl], [], [], [], [ur]]   \n",
       "415                                                                                                                 [[It, about, …, and, fights, ?]]   \n",
       "\n",
       "     Has 1st Person  Has 2nd Person  Has 3rd Person  \\\n",
       "597           False            True           False   \n",
       "504           False            True            True   \n",
       "490            True            True           False   \n",
       "319           False            True           False   \n",
       "102           False            True           False   \n",
       "126           False            True           False   \n",
       "298           False            True           False   \n",
       "154            True            True           False   \n",
       "128           False            True           False   \n",
       "563           False           False            True   \n",
       "195           False            True           False   \n",
       "65             True           False           False   \n",
       "198           False            True           False   \n",
       "119            True            True           False   \n",
       "449           False           False            True   \n",
       "309           False            True           False   \n",
       "559           False           False            True   \n",
       "191           False            True           False   \n",
       "184            True            True            True   \n",
       "415           False           False            True   \n",
       "\n",
       "                                                                                    Named Entities  \\\n",
       "597                                                                               [(muslim, NORP)]   \n",
       "504                                                                               [(Muslim, NORP)]   \n",
       "490                                                                    [(4, CARDINAL), (15, DATE)]   \n",
       "319                                                                  [(Malay Mail Online, PERSON)]   \n",
       "102                                                                                             []   \n",
       "126                                                                                    [(US, GPE)]   \n",
       "298                                                                           [(ÂÂÂ, CARDINAL)]   \n",
       "154                                                                                             []   \n",
       "128                                                                                             []   \n",
       "563                                                     [(nearly half, CARDINAL), (Muslims, NORP)]   \n",
       "195                                                                                             []   \n",
       "65                                                                               [(all day, DATE)]   \n",
       "198                                                                                             []   \n",
       "119                                                                [(WHOLE, ORG), (GONNA, PERSON)]   \n",
       "449                                                                                             []   \n",
       "309                                                                             [(EA LOL, PERSON)]   \n",
       "559  [(Europeans, NORP), (Christian, NORP), (Muslim, NORP), (Palestinians, NORP), (Israeli, NORP)]   \n",
       "191                                                                        [(HAHAHAHAHAHAHA, ORG)]   \n",
       "184                                                                                             []   \n",
       "415                                                                                             []   \n",
       "\n",
       "     Has Discourse Marker  Has Question Mark  Has Third Person Verb  \\\n",
       "597                 False              False                  False   \n",
       "504                  True              False                  False   \n",
       "490                 False              False                  False   \n",
       "319                 False              False                  False   \n",
       "102                 False              False                  False   \n",
       "126                 False              False                  False   \n",
       "298                 False              False                  False   \n",
       "154                  True              False                  False   \n",
       "128                 False              False                  False   \n",
       "563                 False               True                  False   \n",
       "195                 False              False                  False   \n",
       "65                  False              False                  False   \n",
       "198                 False              False                  False   \n",
       "119                 False              False                  False   \n",
       "449                 False              False                  False   \n",
       "309                 False               True                  False   \n",
       "559                 False              False                  False   \n",
       "191                 False               True                  False   \n",
       "184                 False              False                  False   \n",
       "415                 False               True                  False   \n",
       "\n",
       "                                     Adjectives  \n",
       "597  [better, muslim, evil, sharia, disgusting]  \n",
       "504                [Muslim, unreal, dumb, dead]  \n",
       "490                                    [Bloody]  \n",
       "319                                          []  \n",
       "102                              [dumb, nigger]  \n",
       "126                                          []  \n",
       "298                                 [bad, ugly]  \n",
       "154                                          []  \n",
       "128                                    [fucken]  \n",
       "563                                          []  \n",
       "195                  [ignorant, dumb, ignorant]  \n",
       "65                                 [high, same]  \n",
       "198                                          []  \n",
       "119                                [LOUD, DEAD]  \n",
       "449                                          []  \n",
       "309                                    [stupid]  \n",
       "559   [racist, white, Christian, vile, Israeli]  \n",
       "191                            [nigger, -grant]  \n",
       "184                              [black, black]  \n",
       "415                                          []  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set max rows and columns\n",
    "# Set max column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_insult = df1[df1['Classification'] == 'Insults']\n",
    "df_critique = df1[df1['Classification'] == 'Critique/Other']\n",
    "df_insult[['Tweet', 'Classification', 'Adjusted SentiWordScore', 'Subjects','Complements',\"Dependency Children\",\n",
    "           'Aux/pronouns dependence','Has 1st Person', 'Has 2nd Person', 'Has 3rd Person', 'Named Entities', 'Has Discourse Marker', \n",
    "           'Has Question Mark', 'Has Third Person Verb', 'Adjectives']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "\n",
    "# Create a VADER sentiment analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    ccomp_conj_children = []\n",
    "    ccomp_conj_sentiments = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"ccomp\", \"conj\", 'ROOT'):\n",
    "            children = [child.text for child in token.children]\n",
    "            ccomp_conj_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_ccomp_conj_sentiment = sum(ccomp_conj_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"CCOMP or CONJ Children\": ccomp_conj_children,\n",
    "        \"CCOMP or CONJ Children Sum Sentiment\": sum_ccomp_conj_sentiment,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She nsubj criticized VERB []\n",
      "criticized ROOT criticized VERB [She, work, believed, .]\n",
      "his poss work NOUN []\n",
      "work dobj criticized VERB [his]\n",
      "because mark believed VERB []\n",
      "she nsubj believed VERB []\n",
      "believed advcl criticized VERB [because, she, lacked]\n",
      "it nsubj lacked VERB []\n",
      "lacked ccomp believed VERB [it, attention, to]\n",
      "attention dobj lacked VERB []\n",
      "to prep lacked VERB [detail]\n",
      "detail pobj to ADP []\n",
      ". punct criticized VERB []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator at 0x7fd97544f9a0>"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"HOE you mad? damn right our PRESI§DENT IS BLACK ,tf you gone do bouh it? he smartn than romney ass : FUCK OBAMA, dumb ass nigger\"\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'The God of Abraham works this way: dirt done is visited upon perpetrators. Your face is in headlines news across Europe for lowlife abusive language. Nothing to do representing Christian decency of Floridians, just making America look like an immature school yard of bullies.'\n",
    "test =  \"She criticized his work because she believed it lacked attention to detail.\"\n",
    "instance1 = nlp(test)\n",
    "    # for token in doc_text:\n",
    "    #     print(token.text,\" : \",token.dep_, '|', token.head)\n",
    "for token in instance1:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        [child for child in token.children])\n",
    "\n",
    "token.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a18fe30408c64730bb0bc3dcc033f3be-0\" class=\"displacy\" width=\"2500\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">believe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Russians</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">beautiful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Justin</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Trudeau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">angel,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">him</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 2150.0,2.0 2150.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-8\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-10\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-12\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,354.0 L2318.0,342.0 2302.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'The God of Abraham works this way: dirt done is visited upon perpetrators. Your face is in headlines news across Europe for lowlife abusive language. Nothing to do representing Christian decency of Floridians, just making America look like an immature school yard of bullies.'\n",
    "test = 'You idiot, Western slavery was started by the Muslims in Africa! They were sold to the Arabs by their “own” tribal chieftains!'\n",
    "test = \"HOE you mad? damn right our PRESI§DENT IS BLACK ,tf you gone do bouh it? he smartn than romney ass : FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test =  \"She criticized his work because she believed it lacked attention to detail.\"\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "instance = nlp(test)\n",
    "#This part of the library is imported for graphic visualization only.\n",
    "from spacy import displacy\n",
    "\n",
    "#For our doc object, we display the dependency graph, which allows us to better understand/analyze these relationships manually\n",
    "displacy.render(instance, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'are'], ['Russians', 'beautiful', 'and', 'is'], ['Trudeau', 'angel'], ['believe', ',', 'I', 'him']]\n",
      "[[], ['Russians', 'beautiful', 'and', 'is'], [], ['Justin'], ['Trudeau', 'angel']]\n",
      "beautiful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['beautiful']"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the CCOMP function \n",
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test = \"She does not look very beautiful\"\n",
    "test = 'You idiot, Western slavery was started by the Muslims in Africa! They were sold to the Arabs by their “own” tribal chieftains,\tInsults'\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "\n",
    "test \n",
    "doc = nlp(test)\n",
    "# Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "complements = [token.text for token in doc if token.dep_ in (\"ccomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "# Extract children of ccomp or conj\n",
    "ccomp_conj_children = []\n",
    "ccomp_conj_sentiments = []\n",
    "AUX_child = []\n",
    "text = []\n",
    "textToken = []\n",
    "for token in doc:\n",
    "    if token.dep_ in ('ROOT', 'ccomp', 'conj'): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "        children = [child.text for child in token.children]\n",
    "        #children = token.text\n",
    "        ccomp_conj_children.append(children)\n",
    "        # Compute sentiment scores for each child and append to list\n",
    "        #ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "    if token.pos_ in (\"AUX\", 'PROPN'): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "            child = [child.text for child in token.children]\n",
    "            AUX_child.append(child)\n",
    "    if token.pos_ in (\"ADJ\"): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "        text_1 = token.text\n",
    "        #children = token.text\n",
    "        text.append(text_1)\n",
    "    if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "        text = token.text\n",
    "        textToken.append(text)\n",
    "print(ccomp_conj_children)\n",
    "print(AUX_child)\n",
    "print(text)\n",
    "complements\n",
    "textToken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
