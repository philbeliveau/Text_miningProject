{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The degree of aggressivity & Negativity\n",
    "\n",
    "### 2. Personnalization\n",
    "    - Depency tree, establish the subject, object, complement, aux\n",
    "    - Also, tell the (person 1st, 2nd, 3rd) of the subject and the object. \n",
    "    - Identify what are they accuse from \n",
    "    - Identify who’s being targeted (Lexicon or wordnet)\n",
    "    - Established if the aux is directed to the object  (Establishing intention)\n",
    "### 3. Harms inflicted and to who\n",
    "    - Now, using the complement (output of dependency tree), we can map these to threats\n",
    "    - We need wordnet to identify threat. (Actually better lexicon than word net)\n",
    "    - To object refers to what category? race, nationality, religion, color, gender, \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your environment\n",
    "- source spacy/bin/activate (Bash)\n",
    "- get back to your base: deactivate (Bash)\n",
    "\n",
    "You have to be in anaconda3 interpreter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from spacy import displacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbook = ('/Users/philippebeliveau/Desktop/Notebook/Winter_2024/Text_mining/Git_MiningRepository/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "mac_mini = ('/Users/philippebeliveau/Desktop/Notebook_Jupyter_R/Winter_2024/Text_mining/Project/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "df = pd.read_csv(mac_mini)\n",
    "df.head()\n",
    "categories = df['cyberbullying_type'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the tweets by category\n",
    "tweets_by_category = {}\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['tweet_text'].tolist()\n",
    "    \n",
    "    # Add the list of tweets to the dictionary\n",
    "    tweets_by_category[category] = category_tweets\n",
    "\n",
    "category = tweets_by_category['ethnicity'][100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter 2000 tweets from each category\n",
    "df = df.groupby('cyberbullying_type').apply(lambda x: x.sample(min(len(x), 100))).reset_index(drop=True)\n",
    "\n",
    "# Now you can work with 'sampled_df' which contains 2000 samples from each category\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    #removing hastags and links\n",
    "    pattern=re.compile(r\"(#[a-zA-Z0-9]+|@[a-zA-Z0-9]+|https?://\\S+|www\\.\\S+|\\S+\\.[a-z]+|RT @)\")\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning of data\n",
    "df['clean_data']=df['tweet_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the tweets by category\n",
    "tweets_by_category = {}\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['clean_data'].tolist()\n",
    "    \n",
    "    # Add the list of tweets to the dictionary\n",
    "    tweets_by_category[category] = category_tweets\n",
    "\n",
    "category = tweets_by_category['ethnicity'][100:120]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Critique vs Insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple approach\n",
    "- Extract the adjectives, nouns, the Subject Person, and the complement/conjunction. \n",
    "- Extract the name entities\n",
    "- Extract all the synonyms of the adjective, nouns and complement into a long list.\n",
    "- Create a polarity score using Vadersentiment on the tweet itself and on the list of synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = [\"rape is real..zvasiyana nema jokes about being drunk or being gay or being lesbian...rape is not ones choice or wish..thtz where the sensitivity is coming from\",\n",
    "         \"You never saw any celebrity say anything like this for Obama: B Maher Incest Rape 'Joke' S Colbert Gay 'joke' K Griffin beheading 'joke'\",\n",
    "         \"@ManhattaKnight I mean he's gay, but he uses gendered slurs and makes rape jokes\",\n",
    "         \"RT @Raul_Novoa16: @AliciaBernardez @Alex_Aim @_mecaesmal feminazi\",\n",
    "         \"Rape is rape. And the fact that I read one post about a guy getting raped and the comments are calling him gay and he should be happy...? stfu and I really hope no one takes this as a joke tf you own no ones body. You have no rights to do whatever you want to someone else.\",\n",
    "         \"@coiny Also, it's hard to take a company seriously re: harassment when the game has rape jokes & gay jokes and treats women the way it does.\",\n",
    "         \"Idgaf if you are gay, lesbian, bisexual, or whatever the fuck you are, you don't fuck make a rape joke and think it's fucken ok\",\n",
    "         \"#GermanProfessor gives meaning to term FemiNazi. Doesn't hire 'Indian Men' bcz of 'Indias's Rape Problem'\",\n",
    "         \"RT @mcclure111: #DontDateSJWs #ThatWouldBeAViolationOfTheRestrainingOrder\",\n",
    "         \"So I call you female I’m basically calling you a bitch ?\"]\n",
    "\n",
    "Ethnicity = [\n",
    "\"Hey dumb fuck celebs stop doing something for people for publicity on Facebook... Wtf happen to life u niggers are cowards.\",\n",
    "\"Fuck u bitch RT @tayyoung_: FUCK OBAMA, dumb ass nigger\",\n",
    "\"@JoeBiden No Joe, YOU are the RACIST. They have so many video and audio clips of the RACIST shit you have spewed in your past, How can anyone see you as anything but a RACIST. You hate Black people just admit it. And you loves the women to touch. Just look at your own stuff!\",\n",
    "\"when your truck looks dumb as fuck out trying to make it look worse. you ain't country. quit tryin nigger.\",\n",
    "\"That nigger food in the cafe today was disgusting\",\n",
    "\"Muthafuckas still EGO tripping!Fool fuck your damn ego and live life to the fullest!!! LOL at these dumb ass niggers with the bullshit!\",\n",
    "\"@iMADE_YouMOAN don't play dumb, I should fuck yo little white ass up you nigger. 3chainz.? Aw.\",\n",
    "\"I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR HAVE THE SUSPICION OF BEING CALLED A NIGGER! NIGGA! OR NEGRO AT ANY POINT IN YOUR LIFE YOU CANNOT USE THE N WORD. PERIOD. I am talking about non black people in specific!!!! White passing mixed kids are on THIN ICE.\",\n",
    "\"@CJ_Jacobs oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\",\n",
    "\"@Otoliver @Yo_PaTnA_dEm20 this bitch is dumb talk bout 'do not dew 'said not sayed' nigger not nigga lol bitch fuck off\"\n",
    "]\n",
    "\n",
    "noncyber = [\n",
    "\"In other words #katandandre, your food was crapilicious! #mkr\",\n",
    "\"Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\",\n",
    "\"@XochitlSuckkks a classy whore? Or more red velvet cupcakes?\",\n",
    "\"@Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.\",\n",
    "\"@RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.\",\n",
    "\"@Raja5aab @Quickieleaks Yes, the test of god is that good or bad or indifferent or weird or whatever, it all proves gods existence.\",\n",
    "\"Itu sekolah ya bukan tempat bully! Ga jauh kaya neraka\",\n",
    "\"Karma. I hope it bites Kat on the butt. She is just nasty. #mkr\",\n",
    "\"@stockputout everything but mostly my priest\",\n",
    "\"Rebecca Black Drops Out of School Due to Bullying:\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new elements\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df_clean['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    ccomp_conj_children = []\n",
    "    ccomp_conj_sentiments = []\n",
    "    AUX_child = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"acomp\", \"conj\", 'ROOT', 'ccomp'):\n",
    "            children = [child.text for child in token.children]\n",
    "            ccomp_conj_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "        # if token.pos_ in (\"AUX\", 'PROPN'): \n",
    "        #     child = [child.text for child in token.children]\n",
    "        #     AUX_child.append(child)\n",
    "\n",
    "        if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "            child = token.text\n",
    "            AUX_child.append(child)\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_ccomp_conj_sentiment = sum(ccomp_conj_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Dependency Children\": ccomp_conj_children,\n",
    "        \"Dependency Sentiment\": sum_ccomp_conj_sentiment,\n",
    "        \"Aux/pronouns dependence\": AUX_child,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sentiwordnet\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df_clean['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    dep_children = []\n",
    "    senti_sentiments = []\n",
    "    textToken = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"conj\", 'ROOT', 'ccomp'):\n",
    "            children = [child.text for child in token.children]\n",
    "            dep_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            for child in children:\n",
    "                synsets = list(swn.senti_synsets(child))\n",
    "                if synsets:\n",
    "                    senti_sentiments.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "        if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "            text = token.text\n",
    "            textToken.append(text)\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_senti_sentiments = sum(senti_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Dependency Children\": dep_children,\n",
    "        \"Dependency Sentiment\": sum_senti_sentiments,\n",
    "        \"Aux/pronouns dependence\": textToken,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With VADER\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df_clean['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # # Extract children of ccomp or conj\n",
    "    # dep_children = []\n",
    "    # senti_sentiments = []\n",
    "    # textToken = []\n",
    "    # for token in doc:\n",
    "    #     if token.dep_ in (\"acomp\", \"conj\", 'ROOT', 'ccomp'):\n",
    "    #         children = [child.text for child in token.children]\n",
    "    #         dep_children.append(children)\n",
    "    #         # Compute sentiment scores for each child and append to list\n",
    "    #         senti_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "    #     if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "    #         text = token.text\n",
    "    #         textToken.append(text)\n",
    "    # # Compute sum of sentiment scores for ccomp or conj children\n",
    "    # sum_senti_sentiments = sum(senti_sentiments)\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    dep_children = []\n",
    "    senti_sentiments = []\n",
    "    textToken = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"conj\", 'ROOT', 'ccomp'):\n",
    "            children = [child.text for child in token.children]\n",
    "            dep_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            senti_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "        # if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "        #     text = token.text\n",
    "        #     textToken.append(text)\n",
    "    # Compute sum of sentiment scores for the children\n",
    "    sum_senti_sentiments = sum(senti_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Dependency Children\": dep_children,\n",
    "        \"Dependency Sentiment\": sum_senti_sentiments,\n",
    "        \"Aux/pronouns dependence\": textToken,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df1 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_person(subjects, person):\n",
    "    person_list = [p for _, p_list in subjects for p in p_list]\n",
    "    return str(person) in person_list\n",
    "\n",
    "df1['Has 1st Person'] = df1['Subjects'].apply(lambda x: contains_person(x, 1))\n",
    "df1['Has 2nd Person'] = df1['Subjects'].apply(lambda x: contains_person(x, 2))\n",
    "df1['Has 3rd Person'] = df1['Subjects'].apply(lambda x: contains_person(x, 3))\n",
    "\n",
    "df1['Word Count'] = df1['Tweet'].apply(lambda x: len(x.split()))\n",
    "df1['Adjusted SentiWordScore'] = df1.apply(lambda row: row['Dependency Sentiment'] / len(row['Dependency Children']) if len(row['Dependency Children']) != 0 else 0, axis=1)\n",
    "#df1['Adjusted SentiWordScore'] = df1.apply(lambda row: row['Dependency Sentiment'] / len(row['Tweet']) if len(row['Tweet']) != 0 else 0, axis=1)\n",
    "df1['Has Capital Word'] = df1['Tweet'].apply(lambda x: any(word.isupper() for word in x.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv\n",
    "#df.to_csv('/Users/philippebeliveau/Library/Mobile Documents/com~apple~CloudDocs/_Bureau_/Master/Winter_2024/Text_mining/Project/Dataframe/smaller_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv \n",
    "#df1 = pd.read_csv('/Users/philippebeliveau/Library/Mobile Documents/com~apple~CloudDocs/_Bureau_/Master/Winter_2024/Text_mining/Project/Dataframe/smaller_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Rules\n",
    "1. \"Target identification\" -> Identify if there is a target, Does it have a 2nd or 3rd person subject, and or Is the name entities is a PERSON or NORP. If that is true, then increase the threshold required to classify as an insult. \n",
    "2. \"Absence of target\" -> If the Entities are not speaking about a Person or NORP, and have no 2nd and 3rd person subject, then increase the required sentiment score. \n",
    "3. \"Discourse or Insults?\" -> Make a rule regarding Discourse marker. If a tweet shows more of a discourse,the threshold for the sentiment should be very high. Does the tweet as a question mark or a discourse marker such as [\"but\", \"however\", \"on the other hand\", \"yet\", \"nevertheless\", \"although\",\n",
    "                     \"while\", \"even though\", \"despite\", \"regardless\", \"rather\", \"instead\",\n",
    "                     \"meanwhile\", \"in contrast\", \"conversely\", \"compared to\", \"on the contrary\",\n",
    "                     \"besides\", \"furthermore\", \"moreover\", \"in addition\", \"additionally\",\n",
    "                     \"further\", \"also\", \"next\", \"then\", \"afterward\", \"finally\",\n",
    "                     \"therefore\", \"thus\", \"hence\", \"consequently\", \"as a result\", \"so\",\n",
    "                     \"because\", \"since\", \"due to\", \"as long as\", \"provided that\", \"given that\",\n",
    "                     \"for example\", \"for instance\", \"specifically\", \"in particular\", \"to illustrate\",\n",
    "                     \"to clarify\", \"to explain\", \"in summary\", \"to sum up\", \"in conclusion\",\n",
    "                     \"indeed\", \"certainly\", \"obviously\", \"clearly\", \"undoubtedly\", \"surely\"]. \n",
    "4. \"SentiWordScore\" -> If the Adjusted SentiWordScore is smaller then -0.6, then classify it automatically as an Insults\n",
    "5. \"Expressiveness\" -> If the tweets have row['Has Capital Word'] == True, then make the threshold of the sentiment lower. \n",
    "6. \"Degree of Positiveness\" -> If the general tweet is positive, such as row['Sentiment Score'] > 0.3 and row['Synonyms Sentiment Score'] > 0.2, make it more different to be classify as insult and increase the threshold of Adjusted SentiWordScore\n",
    "7. \"Age tweets?\" -> If education_terms = ['school', 'schools', 'college', 'graduation']\n",
    "    if any(term in str(word) for term in education_terms for word in row[['Nouns', 'Tweet']]):\n",
    "        return 'Critique/Other'\n",
    "8. \"Description tweets\" -> Tweets with name entities such as Percent, Quanity, Ordinal etc should have more difficult threshold to be classify as insults. \n",
    "Note, the score here refers to this one: Adjusted SentiWordScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Has Question Mark</th>\n",
       "      <th>Negated Tweet</th>\n",
       "      <th>Has Discourse Marker</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>Dependency Children</th>\n",
       "      <th>Dependency Sentiment</th>\n",
       "      <th>Aux/pronouns dependence</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Synonyms Sentiment Score</th>\n",
       "      <th>Has Third Person Verb</th>\n",
       "      <th>Average SentiWordNet Score</th>\n",
       "      <th>Has 1st Person</th>\n",
       "      <th>Has 2nd Person</th>\n",
       "      <th>Has 3rd Person</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Adjusted SentiWordScore</th>\n",
       "      <th>Has Capital Word</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>AustinDrawbond I swear I'm not sexist but there are way too many female drivers in this town.</td>\n",
       "      <td>False</td>\n",
       "      <td>AustinDrawbond I swear I'm not sexist_NEG but_NEG there_NEG are_NEG way_NEG too_NEG many_NEG female_NEG drivers_NEG in_NEG this_NEG town._NEG</td>\n",
       "      <td>False</td>\n",
       "      <td>[sexist, many, female]</td>\n",
       "      <td>[drivers, town]</td>\n",
       "      <td>[(I, [1]), (I, [1])]</td>\n",
       "      <td>[swear, sexist, are]</td>\n",
       "      <td>[[AustinDrawbond, I, 'm, but, are, .], [I, not, sexist], [there, drivers]]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(AustinDrawbond, ORG)]</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.05000</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>IMrTrippy: I'm all for equal rights, just annoying that they complain about things that don't matter is …</td>\n",
       "      <td>False</td>\n",
       "      <td>IMrTrippy: I'm all for equal rights, just annoying that they complain about things that don't matter_NEG is_NEG …_NEG</td>\n",
       "      <td>False</td>\n",
       "      <td>[equal, annoying]</td>\n",
       "      <td>[rights, things]</td>\n",
       "      <td>[(I, [1]), (they, [3]), (that, [])]</td>\n",
       "      <td>['m]</td>\n",
       "      <td>[[IMrTrippy, :, I, all, for, ,, annoying, is, …], [that, they, about], []]</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.6456</td>\n",
       "      <td>-0.9869</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.03125</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.133967</td>\n",
       "      <td>False</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>na na na na na na na na na na na na na na na na BANTHEMMM I'm not sexist but female comedians are all shite!!!</td>\n",
       "      <td>False</td>\n",
       "      <td>na na na na na na na na na na na na na na na na BANTHEMMM I'm not sexist_NEG but_NEG female_NEG comedians_NEG are_NEG all_NEG shite!!!_NEG</td>\n",
       "      <td>False</td>\n",
       "      <td>[sexist, female, shite]</td>\n",
       "      <td>[comedians]</td>\n",
       "      <td>[(I, [1]), (comedians, [])]</td>\n",
       "      <td>[na, 'm, sexist, are, shite]</td>\n",
       "      <td>[[na, na, na, BANTHEMMM], [I, not, sexist, but, are], [comedians, all, shite, !, !, !]]</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.5684</td>\n",
       "      <td>-0.5267</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.03500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>0.098667</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              Tweet  \\\n",
       "270                   AustinDrawbond I swear I'm not sexist but there are way too many female drivers in this town.   \n",
       "222       IMrTrippy: I'm all for equal rights, just annoying that they complain about things that don't matter is …   \n",
       "248  na na na na na na na na na na na na na na na na BANTHEMMM I'm not sexist but female comedians are all shite!!!   \n",
       "\n",
       "     Has Question Mark  \\\n",
       "270              False   \n",
       "222              False   \n",
       "248              False   \n",
       "\n",
       "                                                                                                                                     Negated Tweet  \\\n",
       "270  AustinDrawbond I swear I'm not sexist_NEG but_NEG there_NEG are_NEG way_NEG too_NEG many_NEG female_NEG drivers_NEG in_NEG this_NEG town._NEG   \n",
       "222                          IMrTrippy: I'm all for equal rights, just annoying that they complain about things that don't matter_NEG is_NEG …_NEG   \n",
       "248     na na na na na na na na na na na na na na na na BANTHEMMM I'm not sexist_NEG but_NEG female_NEG comedians_NEG are_NEG all_NEG shite!!!_NEG   \n",
       "\n",
       "     Has Discourse Marker               Adjectives             Nouns  \\\n",
       "270                 False   [sexist, many, female]   [drivers, town]   \n",
       "222                 False        [equal, annoying]  [rights, things]   \n",
       "248                 False  [sexist, female, shite]       [comedians]   \n",
       "\n",
       "                                Subjects                   Complements  \\\n",
       "270                 [(I, [1]), (I, [1])]          [swear, sexist, are]   \n",
       "222  [(I, [1]), (they, [3]), (that, [])]                          ['m]   \n",
       "248          [(I, [1]), (comedians, [])]  [na, 'm, sexist, are, shite]   \n",
       "\n",
       "                                                                         Dependency Children  \\\n",
       "270               [[AustinDrawbond, I, 'm, but, are, .], [I, not, sexist], [there, drivers]]   \n",
       "222               [[IMrTrippy, :, I, all, for, ,, annoying, is, …], [that, they, about], []]   \n",
       "248  [[na, na, na, BANTHEMMM], [I, not, sexist, but, are], [comedians, all, shite, !, !, !]]   \n",
       "\n",
       "     Dependency Sentiment Aux/pronouns dependence           Named Entities  \\\n",
       "270                0.0000                      []  [(AustinDrawbond, ORG)]   \n",
       "222               -0.4019                      []                       []   \n",
       "248                0.2960                      []                       []   \n",
       "\n",
       "     Sentiment Score  Synonyms Sentiment Score  Has Third Person Verb  \\\n",
       "270          -0.0258                    0.0000                  False   \n",
       "222          -0.6456                   -0.9869                  False   \n",
       "248           0.5684                   -0.5267                  False   \n",
       "\n",
       "     Average SentiWordNet Score  Has 1st Person  Has 2nd Person  \\\n",
       "270                    -0.05000            True           False   \n",
       "222                    -0.03125            True           False   \n",
       "248                    -0.03500            True           False   \n",
       "\n",
       "     Has 3rd Person  Word Count  Adjusted SentiWordScore  Has Capital Word  \\\n",
       "270           False          17                 0.000000              True   \n",
       "222            True          18                -0.133967             False   \n",
       "248           False          26                 0.098667              True   \n",
       "\n",
       "     Classification  \n",
       "270  Critique/Other  \n",
       "222  Critique/Other  \n",
       "248  Critique/Other  "
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First rule based classification\n",
    "def classify(row):\n",
    "    education_terms = ['school', 'schools', 'college', 'graduation']\n",
    "    if any(term in str(word) for term in education_terms for word in row[['Nouns', 'Tweet']]):\n",
    "        return 'Critique/Other'\n",
    "    elif row['Adjusted SentiWordScore'] > -0.20:\n",
    "        return 'Critique/Other'\n",
    "    elif row['Adjusted SentiWordScore'] < -0.15 and row['Sentiment Score'] > 0.3 and row['Synonyms Sentiment Score'] > 0.2:\n",
    "        return 'Critique/Other'\n",
    "    elif row['Has Capital Word'] == True and row['Adjusted SentiWordScore'] < -0.10 and 'you' in row['Subjects'] or 'your' in row['Subjects']:\n",
    "        return 'Insults'\n",
    "    else:\n",
    "        return 'Insults'\n",
    "\n",
    "df1['Classification'] = df1.apply(classify, axis=1)\n",
    "df1.shape[0]\n",
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42790423185941057"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.loc[df1['Classification'] == 'Insults', 'Adjusted SentiWordScore'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 rules classification \n",
    "def classify(row):\n",
    "    # Rule 1: Threshold for insult\n",
    "    insult_threshold = -0.2  # Define your threshold here\n",
    "\n",
    "    # Rule 2: Absence of target\n",
    "    if not any(entity in ['PERSON', 'NORP'] for entity in row['Named Entities']) and not row['Has 2nd Person'] and not row['Has 3rd Person']:\n",
    "        insult_threshold -= 0.4  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 3: Discourse or Insults?\n",
    "    discourse_markers = [\"but\", \"however\", \"on the other hand\", \"yet\", \"nevertheless\", \"although\",\n",
    "                         \"while\", \"even though\", \"despite\", \"regardless\", \"rather\", \"instead\",\n",
    "                         \"meanwhile\", \"in contrast\", \"conversely\", \"compared to\", \"on the contrary\",\n",
    "                         \"besides\", \"furthermore\", \"moreover\", \"in addition\", \"additionally\",\n",
    "                         \"further\", \"also\", \"next\", \"then\", \"afterward\", \"finally\",\n",
    "                         \"therefore\", \"thus\", \"hence\", \"consequently\", \"as a result\", \"so\",\n",
    "                         \"because\", \"since\", \"due to\", \"as long as\", \"provided that\", \"given that\",\n",
    "                         \"for example\", \"for instance\", \"specifically\", \"in particular\", \"to illustrate\",\n",
    "                         \"to clarify\", \"to explain\", \"in summary\", \"to sum up\", \"in conclusion\",\n",
    "                         \"indeed\", \"certainly\", \"obviously\", \"clearly\", \"undoubtedly\", \"surely\"]\n",
    "    if any(marker in row['Tweet'] for marker in discourse_markers):\n",
    "        insult_threshold -= 0.2  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 4: SentiWordScore\n",
    "    if row['Adjusted SentiWordScore'] < -0.6:\n",
    "        return 'Insults'\n",
    "\n",
    "    # Rule 5: Expressiveness\n",
    "    if row['Has Capital Word']:\n",
    "        insult_threshold += 0.1  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 6: Degree of Positiveness\n",
    "    if row['Sentiment Score'] > 0.3 and row['Synonyms Sentiment Score'] > 0.2:\n",
    "        insult_threshold -= 0.2  # Adjust the threshold as needed\n",
    "\n",
    "    # Rule 7: Age tweets?\n",
    "    education_terms = ['school', 'schools', 'college', 'graduation']\n",
    "    if any(term in str(word) for term in education_terms for word in row[['Nouns', 'Tweet']]):\n",
    "        return 'Critique/Other'\n",
    "\n",
    "    # Rule 8: Description tweets\n",
    "    if any(entity in ['PERCENT', 'QUANTITY', 'ORDINAL', 'TIME'] for entity in row['Named Entities']):\n",
    "        insult_threshold -= 0.8  # Adjust the threshold as needed\n",
    "        \n",
    "    # Final classification\n",
    "    if row['Adjusted SentiWordScore'] < insult_threshold:\n",
    "        return 'Insults'\n",
    "    \n",
    "    else:\n",
    "        return 'Critique/Other'\n",
    "\n",
    "df1['Classification'] = df1.apply(classify, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critique/Other    550\n",
      "Insults            50\n",
      "Name: Classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get counts\n",
    "classification_counts = df1['Classification'].value_counts()\n",
    "\n",
    "# Create histogram\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(classification_counts.index, classification_counts.values, color=['blue', 'orange'])\n",
    "plt.xlabel('Classification')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.title('Histogram of Tweet Classifications')\n",
    "plt.show()\n",
    "print(classification_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Adjusted SentiWordScore</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>Dependency Children</th>\n",
       "      <th>Aux/pronouns dependence</th>\n",
       "      <th>Has 1st Person</th>\n",
       "      <th>Has 2nd Person</th>\n",
       "      <th>Has 3rd Person</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Has Discourse Marker</th>\n",
       "      <th>Has Question Mark</th>\n",
       "      <th>Has Third Person Verb</th>\n",
       "      <th>Adjectives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>It is PC gone mad! You cannot even punch anyone any more! Bloody leftard feminazi fascists at the BBC. Etc *froth* /joking</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.123475</td>\n",
       "      <td>[(It, [3]), (You, [2]), (*, [])]</td>\n",
       "      <td>[gone, mad, punch, fascists, froth]</td>\n",
       "      <td>[[is, PC, mad, !], [You, can, not, even, anyone, more, !], [feminazi, at, .], [*, *, /joking]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[(BBC, ORG)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[mad, Bloody]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>By victims you mean Daesh terrorists and dictators?</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.312450</td>\n",
       "      <td>[(you, [2])]</td>\n",
       "      <td>[mean, dictators]</td>\n",
       "      <td>[[By, you, terrorists, ?], []]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>He's fucking dreadful.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.440400</td>\n",
       "      <td>[(He, [3])]</td>\n",
       "      <td>['s, dreadful]</td>\n",
       "      <td>[[He, dreadful, .]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[fucking, dreadful]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Independent investigation into bullying policy at Broadoak Sports College (Hazel Wood High School) Bury, Lancashire and GMP full investigation - Sign the Petition! via This girl was bullied by her white peers &amp;amp; they got to walk away, PLS SIGN!!</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.104150</td>\n",
       "      <td>[(investigation, []), (they, [3])]</td>\n",
       "      <td>[Bury, Lancashire, GMP, amp, got]</td>\n",
       "      <td>[[investigation, ,, Lancashire, investigation, -, Sign, !], [and, GMP], [], [via, girl, was, by], [], [bullied, ;, they, walk, ,, SIGN, !, !]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[(Broadoak Sports College, ORG), (Hazel Wood High School, ORG), (Lancashire, GPE), (GMP, ORG), (white peers &amp;amp, ORG)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Independent, full, white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>not all black people are dumb niggers you racist fuck</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.542300</td>\n",
       "      <td>[(people, []), (you, [2])]</td>\n",
       "      <td>[are]</td>\n",
       "      <td>[[people, niggers, fuck]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[black, dumb, racist]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>lmaooo _cheatham: FOUR MORE YEARS HOE tayyoung_: FUCK OBAMA, dumb ass nigger</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.648600</td>\n",
       "      <td>[]</td>\n",
       "      <td>[lmaooo]</td>\n",
       "      <td>[[_, cheatham, :, tayyoung, OBAMA, nigger]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[(lmaooo _cheatham, ORG), (FOUR, CARDINAL)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[MORE, dumb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>She DM’s girls on Facebook she bullied in high school “hey girlie!”</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.624900</td>\n",
       "      <td>[(she, [3])]</td>\n",
       "      <td>[girls]</td>\n",
       "      <td>[[DM, on, bullied, girlie]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[(DM, ORG)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>fuck you chink. dumb yellow skin nigger can't even drive</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.595450</td>\n",
       "      <td>[(you, [2]), (nigger, [])]</td>\n",
       "      <td>[chink, drive]</td>\n",
       "      <td>[[fuck, you, .], [nigger, ca, n't, even]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dumb, yellow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>You're still an ignorant bitch with a black president, kill yourself or move to another country tayyoung_: FUCK OBAMA, dumb ass nigger</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.616900</td>\n",
       "      <td>[(You, [2])]</td>\n",
       "      <td>['re, kill, move, OBAMA]</td>\n",
       "      <td>[[You, still, bitch, ,, kill, :], [yourself, or, move], [to, tayyoung], [FUCK, ,, nigger]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[ignorant, black, dumb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>_ don't nobody give a FUCK !!! Don't fucking call us by nigger we got names you dumb fuck !!!</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.180767</td>\n",
       "      <td>[(_, []), (nobody, []), (we, [1]), (you, [2])]</td>\n",
       "      <td>[give, call, got]</td>\n",
       "      <td>[[_, do, n't, nobody, FUCK, !, !, !], [Do, n't, fucking, us, by, got, !, !], [we, names, !]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>These the type of girls that bullied the weird kids in high school</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.624900</td>\n",
       "      <td>[(that, [])]</td>\n",
       "      <td>[type]</td>\n",
       "      <td>[[These, the, of, bullied]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[weird, high]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Smh all you people say Mayweather is a \"dirty Nigger\" for doing that are dumb as fuck! if it was Ortiz yall woulda said \"that was a good hit</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.241700</td>\n",
       "      <td>[(people, []), (Mayweather, []), (it, [3]), (that, [])]</td>\n",
       "      <td>[say, dumb, woulda, said]</td>\n",
       "      <td>[[Smh, people, is, !], [Mayweather, Nigger, are], [was, \", was], [that, hit]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[(Mayweather, PERSON), (Ortiz, PERSON)]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dirty, dumb, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>As Real As It Gets: Bullying Victims Can Fight Back With Help From Brazilian Jiu-Jitsu Royalty</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.599400</td>\n",
       "      <td>[(It, [3])]</td>\n",
       "      <td>[Fight]</td>\n",
       "      <td>[[Real, Gets, :, Bullying, Can, Back, With]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[Brazilian]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Can you speak about this too?? Islamic Terrorism is funded by Arab Countries to forcefully convert people in Islam. Shame on biased approach by bigots like u.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.226933</td>\n",
       "      <td>[(you, [2])]</td>\n",
       "      <td>[speak, funded, Shame]</td>\n",
       "      <td>[[Can, you, about, too, ?, ?], [Terrorism, is, by, convert, .], [on]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[(Islamic, NORP), (Arab Countries, ORG), (Islam, GPE)]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[Arab, biased]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Fuck you for this dawg lmfaoooo \": Dumb worthless chinks if your eyes weren't so fucking tight you'd see the fucking orange nigger\"</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.595450</td>\n",
       "      <td>[(eyes, []), (you, [2])]</td>\n",
       "      <td>[chinks, tight]</td>\n",
       "      <td>[[Fuck, see], [were, you, 'd, nigger, \"]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dawg, worthless, fucking]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>LMFAO she mad as hell 2 Terms hoe ! A_little_Crzy Yea head nigger in charge bitch tayyoung_ FUCK OBAMA, dumb ass nigger</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.388900</td>\n",
       "      <td>[(she, [3])]</td>\n",
       "      <td>[LMFAO, Terms, A_little_Crzy, head, OBAMA]</td>\n",
       "      <td>[[mad], [she, as], [hoe, !], [], [Yea, nigger, _], [FUCK, ,, nigger]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[(2, CARDINAL)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[LMFAO, mad, Yea, dumb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>And fuck the dumb nigger that stole it.</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.648600</td>\n",
       "      <td>[(that, [])]</td>\n",
       "      <td>[fuck]</td>\n",
       "      <td>[[And, nigger, .]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dumb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>LowPayIsNotOK: 's killing in Ohio by a police officer has created a merger of police accountability activists and …</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.659700</td>\n",
       "      <td>[(killing, [])]</td>\n",
       "      <td>[created]</td>\n",
       "      <td>[[killing, has, merger, and, …]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[(Ohio, GPE)]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>Speaker at Tea party Rally:They made it about collective bargaining, it's about collective bullying. We are not going to be bullied anymore</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.208300</td>\n",
       "      <td>[(They, [3]), (it, [3]), (We, [1])]</td>\n",
       "      <td>['s, going]</td>\n",
       "      <td>[[Speaker, :, They, it, about], [made, ,, it, about, .], [We, are, not, bullied, anymore]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[collective, collective]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>: \"Yo nigger you tripping\", : \"nigger fuck you, you can't even spell \"tripping\" you dumb nigger\".</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-1.175050</td>\n",
       "      <td>[(you, [2]), (you, [2]), (you, [2])]</td>\n",
       "      <td>[spell]</td>\n",
       "      <td>[[:, \", nigger, you, \", ,, :], [tripping, fuck, ,, you, ca, n't, even, tripping, dumb, nigger, \", .]]</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[nigger]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                        Tweet  \\\n",
       "216                                                                                                                                It is PC gone mad! You cannot even punch anyone any more! Bloody leftard feminazi fascists at the BBC. Etc *froth* /joking   \n",
       "337                                                                                                                                                                                                       By victims you mean Daesh terrorists and dictators?   \n",
       "257                                                                                                                                                                                                                                    He's fucking dreadful.   \n",
       "40   Independent investigation into bullying policy at Broadoak Sports College (Hazel Wood High School) Bury, Lancashire and GMP full investigation - Sign the Petition! via This girl was bullied by her white peers &amp; they got to walk away, PLS SIGN!!   \n",
       "153                                                                                                                                                                                                     not all black people are dumb niggers you racist fuck   \n",
       "110                                                                                                                                                                              lmaooo _cheatham: FOUR MORE YEARS HOE tayyoung_: FUCK OBAMA, dumb ass nigger   \n",
       "33                                                                                                                                                                                        She DM’s girls on Facebook she bullied in high school “hey girlie!”   \n",
       "195                                                                                                                                                                                                  fuck you chink. dumb yellow skin nigger can't even drive   \n",
       "168                                                                                                                    You're still an ignorant bitch with a black president, kill yourself or move to another country tayyoung_: FUCK OBAMA, dumb ass nigger   \n",
       "180                                                                                                                                                             _ don't nobody give a FUCK !!! Don't fucking call us by nigger we got names you dumb fuck !!!   \n",
       "77                                                                                                                                                                                         These the type of girls that bullied the weird kids in high school   \n",
       "181                                                                                                              Smh all you people say Mayweather is a \"dirty Nigger\" for doing that are dumb as fuck! if it was Ortiz yall woulda said \"that was a good hit   \n",
       "321                                                                                                                                                            As Real As It Gets: Bullying Victims Can Fight Back With Help From Brazilian Jiu-Jitsu Royalty   \n",
       "556                                                                                            Can you speak about this too?? Islamic Terrorism is funded by Arab Countries to forcefully convert people in Islam. Shame on biased approach by bigots like u.   \n",
       "120                                                                                                                       Fuck you for this dawg lmfaoooo \": Dumb worthless chinks if your eyes weren't so fucking tight you'd see the fucking orange nigger\"   \n",
       "177                                                                                                                                   LMFAO she mad as hell 2 Terms hoe ! A_little_Crzy Yea head nigger in charge bitch tayyoung_ FUCK OBAMA, dumb ass nigger   \n",
       "134                                                                                                                                                                                                                   And fuck the dumb nigger that stole it.   \n",
       "317                                                                                                                                       LowPayIsNotOK: 's killing in Ohio by a police officer has created a merger of police accountability activists and …   \n",
       "427                                                                                                               Speaker at Tea party Rally:They made it about collective bargaining, it's about collective bullying. We are not going to be bullied anymore   \n",
       "108                                                                                                                                                         : \"Yo nigger you tripping\", : \"nigger fuck you, you can't even spell \"tripping\" you dumb nigger\".   \n",
       "\n",
       "    Classification  Adjusted SentiWordScore  \\\n",
       "216        Insults                -0.123475   \n",
       "337        Insults                -0.312450   \n",
       "257        Insults                -0.440400   \n",
       "40         Insults                -0.104150   \n",
       "153        Insults                -0.542300   \n",
       "110        Insults                -0.648600   \n",
       "33         Insults                -0.624900   \n",
       "195        Insults                -0.595450   \n",
       "168        Insults                -0.616900   \n",
       "180        Insults                -0.180767   \n",
       "77         Insults                -0.624900   \n",
       "181        Insults                -0.241700   \n",
       "321        Insults                -0.599400   \n",
       "556        Insults                -0.226933   \n",
       "120        Insults                -0.595450   \n",
       "177        Insults                -0.388900   \n",
       "134        Insults                -0.648600   \n",
       "317        Insults                -0.659700   \n",
       "427        Insults                -0.208300   \n",
       "108        Insults                -1.175050   \n",
       "\n",
       "                                                    Subjects  \\\n",
       "216                         [(It, [3]), (You, [2]), (*, [])]   \n",
       "337                                             [(you, [2])]   \n",
       "257                                              [(He, [3])]   \n",
       "40                        [(investigation, []), (they, [3])]   \n",
       "153                               [(people, []), (you, [2])]   \n",
       "110                                                       []   \n",
       "33                                              [(she, [3])]   \n",
       "195                               [(you, [2]), (nigger, [])]   \n",
       "168                                             [(You, [2])]   \n",
       "180           [(_, []), (nobody, []), (we, [1]), (you, [2])]   \n",
       "77                                              [(that, [])]   \n",
       "181  [(people, []), (Mayweather, []), (it, [3]), (that, [])]   \n",
       "321                                              [(It, [3])]   \n",
       "556                                             [(you, [2])]   \n",
       "120                                 [(eyes, []), (you, [2])]   \n",
       "177                                             [(she, [3])]   \n",
       "134                                             [(that, [])]   \n",
       "317                                          [(killing, [])]   \n",
       "427                      [(They, [3]), (it, [3]), (We, [1])]   \n",
       "108                     [(you, [2]), (you, [2]), (you, [2])]   \n",
       "\n",
       "                                    Complements  \\\n",
       "216         [gone, mad, punch, fascists, froth]   \n",
       "337                           [mean, dictators]   \n",
       "257                              ['s, dreadful]   \n",
       "40            [Bury, Lancashire, GMP, amp, got]   \n",
       "153                                       [are]   \n",
       "110                                    [lmaooo]   \n",
       "33                                      [girls]   \n",
       "195                              [chink, drive]   \n",
       "168                    ['re, kill, move, OBAMA]   \n",
       "180                           [give, call, got]   \n",
       "77                                       [type]   \n",
       "181                   [say, dumb, woulda, said]   \n",
       "321                                     [Fight]   \n",
       "556                      [speak, funded, Shame]   \n",
       "120                             [chinks, tight]   \n",
       "177  [LMFAO, Terms, A_little_Crzy, head, OBAMA]   \n",
       "134                                      [fuck]   \n",
       "317                                   [created]   \n",
       "427                                 ['s, going]   \n",
       "108                                     [spell]   \n",
       "\n",
       "                                                                                                                                Dependency Children  \\\n",
       "216                                                  [[is, PC, mad, !], [You, can, not, even, anyone, more, !], [feminazi, at, .], [*, *, /joking]]   \n",
       "337                                                                                                                  [[By, you, terrorists, ?], []]   \n",
       "257                                                                                                                             [[He, dreadful, .]]   \n",
       "40   [[investigation, ,, Lancashire, investigation, -, Sign, !], [and, GMP], [], [via, girl, was, by], [], [bullied, ;, they, walk, ,, SIGN, !, !]]   \n",
       "153                                                                                                                       [[people, niggers, fuck]]   \n",
       "110                                                                                                     [[_, cheatham, :, tayyoung, OBAMA, nigger]]   \n",
       "33                                                                                                                      [[DM, on, bullied, girlie]]   \n",
       "195                                                                                                       [[fuck, you, .], [nigger, ca, n't, even]]   \n",
       "168                                                      [[You, still, bitch, ,, kill, :], [yourself, or, move], [to, tayyoung], [FUCK, ,, nigger]]   \n",
       "180                                                    [[_, do, n't, nobody, FUCK, !, !, !], [Do, n't, fucking, us, by, got, !, !], [we, names, !]]   \n",
       "77                                                                                                                      [[These, the, of, bullied]]   \n",
       "181                                                                   [[Smh, people, is, !], [Mayweather, Nigger, are], [was, \", was], [that, hit]]   \n",
       "321                                                                                                    [[Real, Gets, :, Bullying, Can, Back, With]]   \n",
       "556                                                                           [[Can, you, about, too, ?, ?], [Terrorism, is, by, convert, .], [on]]   \n",
       "120                                                                                                       [[Fuck, see], [were, you, 'd, nigger, \"]]   \n",
       "177                                                                           [[mad], [she, as], [hoe, !], [], [Yea, nigger, _], [FUCK, ,, nigger]]   \n",
       "134                                                                                                                              [[And, nigger, .]]   \n",
       "317                                                                                                                [[killing, has, merger, and, …]]   \n",
       "427                                                      [[Speaker, :, They, it, about], [made, ,, it, about, .], [We, are, not, bullied, anymore]]   \n",
       "108                                           [[:, \", nigger, you, \", ,, :], [tripping, fuck, ,, you, ca, n't, even, tripping, dumb, nigger, \", .]]   \n",
       "\n",
       "    Aux/pronouns dependence  Has 1st Person  Has 2nd Person  Has 3rd Person  \\\n",
       "216                      []           False            True            True   \n",
       "337                      []           False            True           False   \n",
       "257                      []           False           False            True   \n",
       "40                       []           False           False            True   \n",
       "153                      []           False            True           False   \n",
       "110                      []           False           False           False   \n",
       "33                       []           False           False            True   \n",
       "195                      []           False            True           False   \n",
       "168                      []           False            True           False   \n",
       "180                      []            True            True           False   \n",
       "77                       []           False           False           False   \n",
       "181                      []           False           False            True   \n",
       "321                      []           False           False            True   \n",
       "556                      []           False            True           False   \n",
       "120                      []           False            True           False   \n",
       "177                      []           False           False            True   \n",
       "134                      []           False           False           False   \n",
       "317                      []           False           False           False   \n",
       "427                      []            True           False            True   \n",
       "108                      []           False            True           False   \n",
       "\n",
       "                                                                                                              Named Entities  \\\n",
       "216                                                                                                             [(BBC, ORG)]   \n",
       "337                                                                                                                       []   \n",
       "257                                                                                                                       []   \n",
       "40   [(Broadoak Sports College, ORG), (Hazel Wood High School, ORG), (Lancashire, GPE), (GMP, ORG), (white peers &amp, ORG)]   \n",
       "153                                                                                                                       []   \n",
       "110                                                                              [(lmaooo _cheatham, ORG), (FOUR, CARDINAL)]   \n",
       "33                                                                                                               [(DM, ORG)]   \n",
       "195                                                                                                                       []   \n",
       "168                                                                                                                       []   \n",
       "180                                                                                                                       []   \n",
       "77                                                                                                                        []   \n",
       "181                                                                                  [(Mayweather, PERSON), (Ortiz, PERSON)]   \n",
       "321                                                                                                                       []   \n",
       "556                                                                   [(Islamic, NORP), (Arab Countries, ORG), (Islam, GPE)]   \n",
       "120                                                                                                                       []   \n",
       "177                                                                                                          [(2, CARDINAL)]   \n",
       "134                                                                                                                       []   \n",
       "317                                                                                                            [(Ohio, GPE)]   \n",
       "427                                                                                                                       []   \n",
       "108                                                                                                                       []   \n",
       "\n",
       "     Has Discourse Marker  Has Question Mark  Has Third Person Verb  \\\n",
       "216                 False              False                  False   \n",
       "337                 False               True                  False   \n",
       "257                 False              False                  False   \n",
       "40                  False              False                  False   \n",
       "153                 False              False                  False   \n",
       "110                 False              False                  False   \n",
       "33                  False              False                  False   \n",
       "195                 False              False                  False   \n",
       "168                 False              False                  False   \n",
       "180                 False              False                  False   \n",
       "77                  False              False                  False   \n",
       "181                  True              False                  False   \n",
       "321                 False              False                  False   \n",
       "556                 False               True                  False   \n",
       "120                 False              False                  False   \n",
       "177                 False              False                  False   \n",
       "134                 False              False                  False   \n",
       "317                 False              False                  False   \n",
       "427                 False              False                  False   \n",
       "108                 False              False                  False   \n",
       "\n",
       "                     Adjectives  \n",
       "216               [mad, Bloody]  \n",
       "337                          []  \n",
       "257         [fucking, dreadful]  \n",
       "40   [Independent, full, white]  \n",
       "153       [black, dumb, racist]  \n",
       "110                [MORE, dumb]  \n",
       "33                       [high]  \n",
       "195              [dumb, yellow]  \n",
       "168     [ignorant, black, dumb]  \n",
       "180                          []  \n",
       "77                [weird, high]  \n",
       "181         [dirty, dumb, good]  \n",
       "321                 [Brazilian]  \n",
       "556              [Arab, biased]  \n",
       "120  [dawg, worthless, fucking]  \n",
       "177     [LMFAO, mad, Yea, dumb]  \n",
       "134                      [dumb]  \n",
       "317                          []  \n",
       "427    [collective, collective]  \n",
       "108                    [nigger]  "
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set max rows and columns\n",
    "# Set max column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df_insult = df1[df1['Classification'] == 'Insults']\n",
    "df_critique = df1[df1['Classification'] == 'Critique/Other']\n",
    "df_insult[['Tweet', 'Classification', 'Adjusted SentiWordScore', 'Subjects','Complements',\"Dependency Children\",\n",
    "           'Aux/pronouns dependence','Has 1st Person', 'Has 2nd Person', 'Has 3rd Person', 'Named Entities', 'Has Discourse Marker', \n",
    "           'Has Question Mark', 'Has Third Person Verb', 'Adjectives']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "\n",
    "# Create a VADER sentiment analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    ccomp_conj_children = []\n",
    "    ccomp_conj_sentiments = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"ccomp\", \"conj\", 'ROOT'):\n",
    "            children = [child.text for child in token.children]\n",
    "            ccomp_conj_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_ccomp_conj_sentiment = sum(ccomp_conj_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"CCOMP or CONJ Children\": ccomp_conj_children,\n",
    "        \"CCOMP or CONJ Children Sum Sentiment\": sum_ccomp_conj_sentiment,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She nsubj criticized VERB []\n",
      "criticized ROOT criticized VERB [She, work, believed, .]\n",
      "his poss work NOUN []\n",
      "work dobj criticized VERB [his]\n",
      "because mark believed VERB []\n",
      "she nsubj believed VERB []\n",
      "believed advcl criticized VERB [because, she, lacked]\n",
      "it nsubj lacked VERB []\n",
      "lacked ccomp believed VERB [it, attention, to]\n",
      "attention dobj lacked VERB []\n",
      "to prep lacked VERB [detail]\n",
      "detail pobj to ADP []\n",
      ". punct criticized VERB []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator at 0x7fd97544f9a0>"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"HOE you mad? damn right our PRESI§DENT IS BLACK ,tf you gone do bouh it? he smartn than romney ass : FUCK OBAMA, dumb ass nigger\"\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'The God of Abraham works this way: dirt done is visited upon perpetrators. Your face is in headlines news across Europe for lowlife abusive language. Nothing to do representing Christian decency of Floridians, just making America look like an immature school yard of bullies.'\n",
    "test =  \"She criticized his work because she believed it lacked attention to detail.\"\n",
    "instance1 = nlp(test)\n",
    "    # for token in doc_text:\n",
    "    #     print(token.text,\" : \",token.dep_, '|', token.head)\n",
    "for token in instance1:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        [child for child in token.children])\n",
    "\n",
    "token.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"a18fe30408c64730bb0bc3dcc033f3be-0\" class=\"displacy\" width=\"2500\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">believe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Russians</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">beautiful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Justin</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Trudeau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">angel,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">him</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 2150.0,2.0 2150.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-8\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-10\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-a18fe30408c64730bb0bc3dcc033f3be-0-12\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-a18fe30408c64730bb0bc3dcc033f3be-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,354.0 L2318.0,342.0 2302.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'The God of Abraham works this way: dirt done is visited upon perpetrators. Your face is in headlines news across Europe for lowlife abusive language. Nothing to do representing Christian decency of Floridians, just making America look like an immature school yard of bullies.'\n",
    "test = 'You idiot, Western slavery was started by the Muslims in Africa! They were sold to the Arabs by their “own” tribal chieftains!'\n",
    "test = \"HOE you mad? damn right our PRESI§DENT IS BLACK ,tf you gone do bouh it? he smartn than romney ass : FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test =  \"She criticized his work because she believed it lacked attention to detail.\"\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "instance = nlp(test)\n",
    "#This part of the library is imported for graphic visualization only.\n",
    "from spacy import displacy\n",
    "\n",
    "#For our doc object, we display the dependency graph, which allows us to better understand/analyze these relationships manually\n",
    "displacy.render(instance, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'are'], ['Russians', 'beautiful', 'and', 'is'], ['Trudeau', 'angel'], ['believe', ',', 'I', 'him']]\n",
      "[[], ['Russians', 'beautiful', 'and', 'is'], [], ['Justin'], ['Trudeau', 'angel']]\n",
      "beautiful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['beautiful']"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the CCOMP function \n",
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test = \"She does not look very beautiful\"\n",
    "test = 'You idiot, Western slavery was started by the Muslims in Africa! They were sold to the Arabs by their “own” tribal chieftains,\tInsults'\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "\n",
    "test \n",
    "doc = nlp(test)\n",
    "# Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "complements = [token.text for token in doc if token.dep_ in (\"ccomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "# Extract children of ccomp or conj\n",
    "ccomp_conj_children = []\n",
    "ccomp_conj_sentiments = []\n",
    "AUX_child = []\n",
    "text = []\n",
    "textToken = []\n",
    "for token in doc:\n",
    "    if token.dep_ in ('ROOT', 'ccomp', 'conj'): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "        children = [child.text for child in token.children]\n",
    "        #children = token.text\n",
    "        ccomp_conj_children.append(children)\n",
    "        # Compute sentiment scores for each child and append to list\n",
    "        #ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "    if token.pos_ in (\"AUX\", 'PROPN'): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "            child = [child.text for child in token.children]\n",
    "            AUX_child.append(child)\n",
    "    if token.pos_ in (\"ADJ\"): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "        text_1 = token.text\n",
    "        #children = token.text\n",
    "        text.append(text_1)\n",
    "    if token.dep_ in (\"acomp\", 'advmod', 'amod',\"appos\", \"coordination\"): \n",
    "        text = token.text\n",
    "        textToken.append(text)\n",
    "print(ccomp_conj_children)\n",
    "print(AUX_child)\n",
    "print(text)\n",
    "complements\n",
    "textToken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
