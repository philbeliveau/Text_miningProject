{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The degree of aggressivity & Negativity\n",
    "\n",
    "### 2. Personnalization\n",
    "    - Depency tree, establish the subject, object, complement, aux\n",
    "    - Also, tell the (person 1st, 2nd, 3rd) of the subject and the object. \n",
    "    - Identify what are they accuse from \n",
    "    - Identify who’s being targeted (Lexicon or wordnet)\n",
    "    - Established if the aux is directed to the object  (Establishing intention)\n",
    "### 3. Harms inflicted and to who\n",
    "    - Now, using the complement (output of dependency tree), we can map these to threats\n",
    "    - We need wordnet to identify threat. (Actually better lexicon than word net)\n",
    "    - To object refers to what category? race, nationality, religion, color, gender, \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your environment\n",
    "- source spacy/bin/activate (Bash)\n",
    "- get back to your base: deactivate (Bash)\n",
    "\n",
    "You have to be in anaconda3 interpreter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippebeliveau/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from spacy import displacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbook = ('/Users/philippebeliveau/Desktop/Notebook/Winter_2024/Text_mining/Git_MiningRepository/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "mac_mini = ('/Users/philippebeliveau/Desktop/Notebook_Jupyter_R/Winter_2024/Text_mining/Project/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "df = pd.read_csv(macbook)\n",
    "df.head()\n",
    "categories = df['cyberbullying_type'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the tweets by category\n",
    "tweets_by_category = {}\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['tweet_text'].tolist()\n",
    "    \n",
    "    # Add the list of tweets to the dictionary\n",
    "    tweets_by_category[category] = category_tweets\n",
    "\n",
    "category = tweets_by_category['ethnicity'][100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter 2000 tweets from each category\n",
    "df = df.groupby('cyberbullying_type').apply(lambda x: x.sample(min(len(x), 2000))).reset_index(drop=True)\n",
    "\n",
    "# Now you can work with 'sampled_df' which contains 2000 samples from each category\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    #removing hastags and links\n",
    "    pattern=re.compile(r\"(#[a-zA-Z0-9]+|@[a-zA-Z0-9]+|https?://\\S+|www\\.\\S+|\\S+\\.[a-z]+|RT @)\")\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning of data\n",
    "df['clean_data']=df['tweet_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to hold the tweets by category\n",
    "tweets_by_category = {}\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['clean_data'].tolist()\n",
    "    \n",
    "    # Add the list of tweets to the dictionary\n",
    "    tweets_by_category[category] = category_tweets\n",
    "\n",
    "category = tweets_by_category['ethnicity'][100:120]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Critique vs Insult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple approach\n",
    "- Extract the adjectives, nouns, the Subject Person, and the complement/conjunction. \n",
    "- Extract the name entities\n",
    "- Extract all the synonyms of the adjective, nouns and complement into a long list.\n",
    "- Create a polarity score using Vadersentiment on the tweet itself and on the list of synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gender = [\"rape is real..zvasiyana nema jokes about being drunk or being gay or being lesbian...rape is not ones choice or wish..thtz where the sensitivity is coming from\",\n",
    "         \"You never saw any celebrity say anything like this for Obama: B Maher Incest Rape 'Joke' S Colbert Gay 'joke' K Griffin beheading 'joke'\",\n",
    "         \"@ManhattaKnight I mean he's gay, but he uses gendered slurs and makes rape jokes\",\n",
    "         \"RT @Raul_Novoa16: @AliciaBernardez @Alex_Aim @_mecaesmal feminazi\",\n",
    "         \"Rape is rape. And the fact that I read one post about a guy getting raped and the comments are calling him gay and he should be happy...? stfu and I really hope no one takes this as a joke tf you own no ones body. You have no rights to do whatever you want to someone else.\",\n",
    "         \"@coiny Also, it's hard to take a company seriously re: harassment when the game has rape jokes & gay jokes and treats women the way it does.\",\n",
    "         \"Idgaf if you are gay, lesbian, bisexual, or whatever the fuck you are, you don't fuck make a rape joke and think it's fucken ok\",\n",
    "         \"#GermanProfessor gives meaning to term FemiNazi. Doesn't hire 'Indian Men' bcz of 'Indias's Rape Problem'\",\n",
    "         \"RT @mcclure111: #DontDateSJWs #ThatWouldBeAViolationOfTheRestrainingOrder\",\n",
    "         \"So I call you female I’m basically calling you a bitch ?\"]\n",
    "\n",
    "Ethnicity = [\n",
    "\"Hey dumb fuck celebs stop doing something for people for publicity on Facebook... Wtf happen to life u niggers are cowards.\",\n",
    "\"Fuck u bitch RT @tayyoung_: FUCK OBAMA, dumb ass nigger\",\n",
    "\"@JoeBiden No Joe, YOU are the RACIST. They have so many video and audio clips of the RACIST shit you have spewed in your past, How can anyone see you as anything but a RACIST. You hate Black people just admit it. And you loves the women to touch. Just look at your own stuff!\",\n",
    "\"when your truck looks dumb as fuck out trying to make it look worse. you ain't country. quit tryin nigger.\",\n",
    "\"That nigger food in the cafe today was disgusting\",\n",
    "\"Muthafuckas still EGO tripping!Fool fuck your damn ego and live life to the fullest!!! LOL at these dumb ass niggers with the bullshit!\",\n",
    "\"@iMADE_YouMOAN don't play dumb, I should fuck yo little white ass up you nigger. 3chainz.? Aw.\",\n",
    "\"I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR HAVE THE SUSPICION OF BEING CALLED A NIGGER! NIGGA! OR NEGRO AT ANY POINT IN YOUR LIFE YOU CANNOT USE THE N WORD. PERIOD. I am talking about non black people in specific!!!! White passing mixed kids are on THIN ICE.\",\n",
    "\"@CJ_Jacobs oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\",\n",
    "\"@Otoliver @Yo_PaTnA_dEm20 this bitch is dumb talk bout 'do not dew 'said not sayed' nigger not nigga lol bitch fuck off\"\n",
    "]\n",
    "\n",
    "noncyber = [\n",
    "\"In other words #katandandre, your food was crapilicious! #mkr\",\n",
    "\"Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\",\n",
    "\"@XochitlSuckkks a classy whore? Or more red velvet cupcakes?\",\n",
    "\"@Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.\",\n",
    "\"@RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.\",\n",
    "\"@Raja5aab @Quickieleaks Yes, the test of god is that good or bad or indifferent or weird or whatever, it all proves gods existence.\",\n",
    "\"Itu sekolah ya bukan tempat bully! Ga jauh kaya neraka\",\n",
    "\"Karma. I hope it bites Kat on the butt. She is just nasty. #mkr\",\n",
    "\"@stockputout everything but mostly my priest\",\n",
    "\"Rebecca Black Drops Out of School Due to Bullying:\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Create a polarity score\u001b[39;00m\n\u001b[1;32m     42\u001b[0m sentiment_score \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mpolarity_scores(tweet)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 43\u001b[0m synonyms_sentiment_score \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynonyms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompound\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Append data to list\u001b[39;00m\n\u001b[1;32m     46\u001b[0m data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweet\u001b[39m\u001b[38;5;124m\"\u001b[39m: tweet,\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjectives\u001b[39m\u001b[38;5;124m\"\u001b[39m: adjectives,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSynonyms Sentiment Score\u001b[39m\u001b[38;5;124m\"\u001b[39m: synonyms_sentiment_score\n\u001b[1;32m     58\u001b[0m })\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/vaderSentiment/vaderSentiment.py:269\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.polarity_scores\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    266\u001b[0m         sentiments\u001b[38;5;241m.\u001b[39mappend(valence)\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     sentiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment_valence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentitext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_but_check(words_and_emoticons, sentiments)\n\u001b[1;32m    273\u001b[0m valence_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_valence(sentiments, text)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/vaderSentiment/vaderSentiment.py:312\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.sentiment_valence\u001b[0;34m(self, valence, sentitext, item, i, sentiments)\u001b[0m\n\u001b[1;32m    310\u001b[0m     s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m    311\u001b[0m valence \u001b[38;5;241m=\u001b[39m valence \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 312\u001b[0m valence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_negation_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords_and_emoticons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    314\u001b[0m     valence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_special_idioms_check(valence, words_and_emoticons, i)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/vaderSentiment/vaderSentiment.py:402\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer._negation_check\u001b[0;34m(valence, words_and_emoticons, start_i, i)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_negation_check\u001b[39m(valence, words_and_emoticons, start_i, i):\n\u001b[0;32m--> 402\u001b[0m     words_and_emoticons_lower \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(w)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words_and_emoticons]\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m negated([words_and_emoticons_lower[i \u001b[38;5;241m-\u001b[39m (start_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]]):  \u001b[38;5;66;03m# 1 word preceding lexicon word (w/o stopwords)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/vaderSentiment/vaderSentiment.py:402\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_negation_check\u001b[39m(valence, words_and_emoticons, start_i, i):\n\u001b[0;32m--> 402\u001b[0m     words_and_emoticons_lower \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words_and_emoticons]\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m negated([words_and_emoticons_lower[i \u001b[38;5;241m-\u001b[39m (start_i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]]):  \u001b[38;5;66;03m# 1 word preceding lexicon word (w/o stopwords)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize lists to store data\n",
    "\n",
    "# Create a VADER sentiment analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    ccomp_conj_children = []\n",
    "    ccomp_conj_sentiments = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"ccomp\", \"conj\", 'ROOT'):\n",
    "            children = [child.text for child in token.children]\n",
    "            ccomp_conj_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_ccomp_conj_sentiment = sum(ccomp_conj_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"CCOMP or CONJ Children\": ccomp_conj_children,\n",
    "        \"CCOMP or CONJ Children Sum Sentiment\": sum_ccomp_conj_sentiment,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with new elements\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "# Initialize lists to store data\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data = []\n",
    "\n",
    "for tweet in df['clean_data']:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "    # Extract children of ccomp or conj\n",
    "    ccomp_conj_children = []\n",
    "    ccomp_conj_sentiments = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"ccomp\", \"conj\", 'ROOT'):\n",
    "            children = [child.text for child in token.children]\n",
    "            ccomp_conj_children.append(children)\n",
    "            # Compute sentiment scores for each child and append to list\n",
    "            ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "    # Compute sum of sentiment scores for ccomp or conj children\n",
    "    sum_ccomp_conj_sentiment = sum(ccomp_conj_sentiments)\n",
    "\n",
    "    # Extract named entities and their types\n",
    "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\", 'ccomp'):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Check if \"?\" is in the tweet\n",
    "    has_question_mark = \"?\" in tweet\n",
    "\n",
    "    # Mark negation in the tweet\n",
    "    negated_tweet = \" \".join(mark_negation(tweet.split()))\n",
    "\n",
    "    # Check for discourse markers\n",
    "    discourse_markers = ['say', 'claim']\n",
    "    has_discourse_marker = any(marker in tweet for marker in discourse_markers)\n",
    "\n",
    "    # Check if there is a 3rd person verb in the tweet\n",
    "    has_third_person_verb = any(token.morph.get('Person') == '3' and token.pos_ == 'VERB' for token in doc)\n",
    "\n",
    "    # Get SentiWordNet polarity scores\n",
    "    senti_scores = []\n",
    "    for token in doc:\n",
    "        synsets = list(swn.senti_synsets(token.text))\n",
    "        if synsets:\n",
    "            senti_scores.append(synsets[0].pos_score() - synsets[0].neg_score())\n",
    "\n",
    "    avg_senti_score = sum(senti_scores) / len(senti_scores) if senti_scores else 0\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Has Question Mark\": has_question_mark,\n",
    "        \"Negated Tweet\": negated_tweet,\n",
    "        \"Has Discourse Marker\": has_discourse_marker,\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"CCOMP or CONJ Children\": ccomp_conj_children,\n",
    "        \"CCOMP or CONJ Children Sum Sentiment\": sum_ccomp_conj_sentiment,\n",
    "        \"Named Entities\": named_entities,\n",
    "       # \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score,  \n",
    "        \"Has Third Person Verb\": has_third_person_verb,\n",
    "        \"Average SentiWordNet Score\": avg_senti_score,\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Has Question Mark</th>\n",
       "      <th>Negated Tweet</th>\n",
       "      <th>Has Discourse Marker</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>CCOMP or CONJ Children</th>\n",
       "      <th>CCOMP or CONJ Children Sum Sentiment</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Synonyms Sentiment Score</th>\n",
       "      <th>Has Third Person Verb</th>\n",
       "      <th>Average SentiWordNet Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m sending this comment to my daughter. Six months off school and the bullies are still slagging her off when they haven’t heard of or seen her in all that time!</td>\n",
       "      <td>False</td>\n",
       "      <td>I’m sending this comment to my daughter. Six months off school and the bullies are still slagging her off when they haven’t heard of or seen her in all that time!</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "      <td>[comment, daughter, months, school, bullies, time]</td>\n",
       "      <td>[(I, [1]), (bullies, []), (they, [3])]</td>\n",
       "      <td>[sending, slagging, seen]</td>\n",
       "      <td>[[I, ’m, comment, to, .], [off, bullies, are, still, her, off, heard, !], [her, in]]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[(Six months, DATE)]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.9451</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This gives me the same vibe as when the girl who constantly bullied the fuck out of me in high school tried to message me 5 years later like “long time no talk omg I miss you!! Have you ever heard of ITWORKS-“</td>\n",
       "      <td>False</td>\n",
       "      <td>This gives me the same vibe as when the girl who constantly bullied the fuck out of me in high school tried to message me 5 years later like “long time no talk_NEG omg_NEG I_NEG miss_NEG you!!_NEG Have_NEG you_NEG ever_NEG heard_NEG of_NEG ITWORKS-“_NEG</td>\n",
       "      <td>False</td>\n",
       "      <td>[same, high, long]</td>\n",
       "      <td>[vibe, girl, fuck, school, years, time, talk, omg]</td>\n",
       "      <td>[(This, []), (girl, []), (who, []), (I, [1]), (you, [2])]</td>\n",
       "      <td>[miss, heard]</td>\n",
       "      <td>[[This, me, vibe], [gives, omg, I, you, !, !], [Have, you, ever, of, “]]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[(5 years later, DATE)]</td>\n",
       "      <td>-0.8585</td>\n",
       "      <td>-0.6942</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.008065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hilarious how the girl who bullied me in high school and called me fat every single day of my life and told me I could go on a diet but I would never be pretty and that I should k*ll myself because I’m fat is on Instagram saying how she “totally understands eating disorders”</td>\n",
       "      <td>False</td>\n",
       "      <td>Hilarious how the girl who bullied me in high school and called me fat every single day of my life and told me I could go on a diet but I would never be_NEG pretty_NEG and_NEG that_NEG I_NEG should_NEG k*ll_NEG myself_NEG because_NEG I’m_NEG fat_NEG is_NEG on_NEG Instagram_NEG saying_NEG how_NEG she_NEG “totally_NEG understands_NEG eating_NEG disorders”_NEG</td>\n",
       "      <td>True</td>\n",
       "      <td>[high, fat, single, pretty, fat]</td>\n",
       "      <td>[girl, school, day, life, diet, disorders]</td>\n",
       "      <td>[(who, []), (I, [1]), (I, [1]), (I, [1]), (I, [1]), (she, [3])]</td>\n",
       "      <td>[Hilarious, called, told, be, pretty, k*ll, fat, is]</td>\n",
       "      <td>[[girl], [me, fat, day, and, told], [me, go], [I, could, on], [but, I, would, never, pretty, and, k*ll, is, ”], [that, I, should, myself, ’m], [on, saying], [how, she, “, totally, disorders]]</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.6300</td>\n",
       "      <td>0.7096</td>\n",
       "      <td>False</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hahaha this video reminds me of some shitty elementary school bully fuck these people</td>\n",
       "      <td>False</td>\n",
       "      <td>Hahaha this video reminds me of some shitty elementary school bully fuck these people</td>\n",
       "      <td>False</td>\n",
       "      <td>[shitty, elementary]</td>\n",
       "      <td>[video, bully, people]</td>\n",
       "      <td>[(Hahaha, []), (video, [])]</td>\n",
       "      <td>[reminds]</td>\n",
       "      <td>[[Hahaha, video, me, of, fuck]]</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>[(Hahaha, PERSON)]</td>\n",
       "      <td>-0.7717</td>\n",
       "      <td>-0.9790</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I don't care if the chance of children dying is virtually 0. We must protect our children from virus', bullies, and gender classifications. Special schools need to be built to protect our fragile babies.</td>\n",
       "      <td>False</td>\n",
       "      <td>I don't care_NEG if_NEG the_NEG chance_NEG of_NEG children_NEG dying_NEG is_NEG virtually_NEG 0._NEG We_NEG must_NEG protect_NEG our_NEG children_NEG from_NEG virus',_NEG bullies,_NEG and_NEG gender_NEG classifications._NEG Special_NEG schools_NEG need_NEG to_NEG be_NEG built_NEG to_NEG protect_NEG our_NEG fragile_NEG babies._NEG</td>\n",
       "      <td>False</td>\n",
       "      <td>[Special, fragile]</td>\n",
       "      <td>[chance, children, children, virus, bullies, gender, classifications, schools, babies]</td>\n",
       "      <td>[(I, [1]), (chance, []), (We, [1]), (schools, [])]</td>\n",
       "      <td>[care, protect, bullies, classifications, need]</td>\n",
       "      <td>[[I, do, n't, is, .], [We, must, children, from, ', ,, bullies, .], [,, and, classifications], [gender], [schools, built, .]]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.7409</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.114583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                 Tweet  \\\n",
       "0                                                                                                                   I’m sending this comment to my daughter. Six months off school and the bullies are still slagging her off when they haven’t heard of or seen her in all that time!   \n",
       "1                                                                    This gives me the same vibe as when the girl who constantly bullied the fuck out of me in high school tried to message me 5 years later like “long time no talk omg I miss you!! Have you ever heard of ITWORKS-“   \n",
       "2  Hilarious how the girl who bullied me in high school and called me fat every single day of my life and told me I could go on a diet but I would never be pretty and that I should k*ll myself because I’m fat is on Instagram saying how she “totally understands eating disorders”   \n",
       "3                                                                                                                                                                                                Hahaha this video reminds me of some shitty elementary school bully fuck these people   \n",
       "4                                                                          I don't care if the chance of children dying is virtually 0. We must protect our children from virus', bullies, and gender classifications. Special schools need to be built to protect our fragile babies.   \n",
       "\n",
       "   Has Question Mark  \\\n",
       "0              False   \n",
       "1              False   \n",
       "2              False   \n",
       "3              False   \n",
       "4              False   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                             Negated Tweet  \\\n",
       "0                                                                                                                                                                                                       I’m sending this comment to my daughter. Six months off school and the bullies are still slagging her off when they haven’t heard of or seen her in all that time!   \n",
       "1                                                                                                            This gives me the same vibe as when the girl who constantly bullied the fuck out of me in high school tried to message me 5 years later like “long time no talk_NEG omg_NEG I_NEG miss_NEG you!!_NEG Have_NEG you_NEG ever_NEG heard_NEG of_NEG ITWORKS-“_NEG   \n",
       "2  Hilarious how the girl who bullied me in high school and called me fat every single day of my life and told me I could go on a diet but I would never be_NEG pretty_NEG and_NEG that_NEG I_NEG should_NEG k*ll_NEG myself_NEG because_NEG I’m_NEG fat_NEG is_NEG on_NEG Instagram_NEG saying_NEG how_NEG she_NEG “totally_NEG understands_NEG eating_NEG disorders”_NEG   \n",
       "3                                                                                                                                                                                                                                                                                    Hahaha this video reminds me of some shitty elementary school bully fuck these people   \n",
       "4                              I don't care_NEG if_NEG the_NEG chance_NEG of_NEG children_NEG dying_NEG is_NEG virtually_NEG 0._NEG We_NEG must_NEG protect_NEG our_NEG children_NEG from_NEG virus',_NEG bullies,_NEG and_NEG gender_NEG classifications._NEG Special_NEG schools_NEG need_NEG to_NEG be_NEG built_NEG to_NEG protect_NEG our_NEG fragile_NEG babies._NEG   \n",
       "\n",
       "   Has Discourse Marker                        Adjectives  \\\n",
       "0                 False                                []   \n",
       "1                 False                [same, high, long]   \n",
       "2                  True  [high, fat, single, pretty, fat]   \n",
       "3                 False              [shitty, elementary]   \n",
       "4                 False                [Special, fragile]   \n",
       "\n",
       "                                                                                    Nouns  \\\n",
       "0                                      [comment, daughter, months, school, bullies, time]   \n",
       "1                                      [vibe, girl, fuck, school, years, time, talk, omg]   \n",
       "2                                              [girl, school, day, life, diet, disorders]   \n",
       "3                                                                  [video, bully, people]   \n",
       "4  [chance, children, children, virus, bullies, gender, classifications, schools, babies]   \n",
       "\n",
       "                                                          Subjects  \\\n",
       "0                           [(I, [1]), (bullies, []), (they, [3])]   \n",
       "1        [(This, []), (girl, []), (who, []), (I, [1]), (you, [2])]   \n",
       "2  [(who, []), (I, [1]), (I, [1]), (I, [1]), (I, [1]), (she, [3])]   \n",
       "3                                      [(Hahaha, []), (video, [])]   \n",
       "4               [(I, [1]), (chance, []), (We, [1]), (schools, [])]   \n",
       "\n",
       "                                            Complements  \\\n",
       "0                             [sending, slagging, seen]   \n",
       "1                                         [miss, heard]   \n",
       "2  [Hilarious, called, told, be, pretty, k*ll, fat, is]   \n",
       "3                                             [reminds]   \n",
       "4       [care, protect, bullies, classifications, need]   \n",
       "\n",
       "                                                                                                                                                                            CCOMP or CONJ Children  \\\n",
       "0                                                                                                             [[I, ’m, comment, to, .], [off, bullies, are, still, her, off, heard, !], [her, in]]   \n",
       "1                                                                                                                         [[This, me, vibe], [gives, omg, I, you, !, !], [Have, you, ever, of, “]]   \n",
       "2  [[girl], [me, fat, day, and, told], [me, go], [I, could, on], [but, I, would, never, pretty, and, k*ll, is, ”], [that, I, should, myself, ’m], [on, saying], [how, she, “, totally, disorders]]   \n",
       "3                                                                                                                                                                  [[Hahaha, video, me, of, fuck]]   \n",
       "4                                                                    [[I, do, n't, is, .], [We, must, children, from, ', ,, bullies, .], [,, and, classifications], [gender], [schools, built, .]]   \n",
       "\n",
       "   CCOMP or CONJ Children Sum Sentiment           Named Entities  \\\n",
       "0                                0.0000     [(Six months, DATE)]   \n",
       "1                                0.0000  [(5 years later, DATE)]   \n",
       "2                                0.4939                       []   \n",
       "3                                0.0151       [(Hahaha, PERSON)]   \n",
       "4                                0.0000                       []   \n",
       "\n",
       "   Sentiment Score  Synonyms Sentiment Score  Has Third Person Verb  \\\n",
       "0           0.0000                   -0.9451                  False   \n",
       "1          -0.8585                   -0.6942                  False   \n",
       "2          -0.6300                    0.7096                  False   \n",
       "3          -0.7717                   -0.9790                  False   \n",
       "4           0.7409                    0.9844                  False   \n",
       "\n",
       "   Average SentiWordNet Score  \n",
       "0                   -0.112500  \n",
       "1                   -0.008065  \n",
       "2                    0.027027  \n",
       "3                   -0.187500  \n",
       "4                   -0.114583  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_person(subjects, person):\n",
    "    person_list = [p for _, p_list in subjects for p in p_list]\n",
    "    return str(person) in person_list\n",
    "\n",
    "df['Has 1st Person'] = df['Subjects'].apply(lambda x: contains_person(x, 1))\n",
    "df['Has 2nd Person'] = df['Subjects'].apply(lambda x: contains_person(x, 2))\n",
    "df['Has 3rd Person'] = df['Subjects'].apply(lambda x: contains_person(x, 3))\n",
    "\n",
    "df['Word Count'] = df['Tweet'].apply(lambda x: len(x.split()))\n",
    "df['Adjusted Sentiment Score'] = df.apply(lambda row: row['CCOMP or CONJ Children Sum Sentiment'] / len(row['CCOMP or CONJ Children']) if len(row['CCOMP or CONJ Children']) != 0 else 0, axis=1)\n",
    "df['Has Capital Word'] = df['Tweet'].apply(lambda x: any(word.isupper() for word in x.split()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv\n",
    "df.to_csv('/Users/philippebeliveau/Library/Mobile Documents/com~apple~CloudDocs/_Bureau_/Master/Winter_2024/Text_mining/Project/Dataframe/smaller_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv \n",
    "df1 = pd.read_csv('/Users/philippebeliveau/Library/Mobile Documents/com~apple~CloudDocs/_Bureau_/Master/Winter_2024/Text_mining/Project/Dataframe/smaller_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>CCOMP or CONJ Children</th>\n",
       "      <th>CCOMP or CONJ Children Sum Sentiment</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Synonyms Sentiment Score</th>\n",
       "      <th>Has 1st Person</th>\n",
       "      <th>Has 2nd Person</th>\n",
       "      <th>Has 3rd Person</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Adjusted Sentiment Score</th>\n",
       "      <th>Has Capital Word</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I just saw a girl who bullied me in high school twerking on facebook live, good for her lmao</td>\n",
       "      <td>['high', 'good']</td>\n",
       "      <td>['girl', 'school', 'twerking', 'facebook', 'lmao']</td>\n",
       "      <td>[('I', ['1']), ('who', [])]</td>\n",
       "      <td>['saw']</td>\n",
       "      <td>[['I', 'just', 'girl']]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the girls who wore this shit bullied me in high school so if that gives any context to its audience</td>\n",
       "      <td>['high']</td>\n",
       "      <td>['girls', 'shit', 'school', 'context', 'audience']</td>\n",
       "      <td>[('girls', []), ('who', []), ('that', [])]</td>\n",
       "      <td>['bullied']</td>\n",
       "      <td>[['girls', 'me', 'in', 'so', 'gives']]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.8537</td>\n",
       "      <td>-0.9974</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>False</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i actually feel sick i don’t understand how it’s men &gt; women rn omg i don’t wanna go back to that school they’re gonna bully me and make fun of me for being bi</td>\n",
       "      <td>['sick']</td>\n",
       "      <td>['men', 'women', 'omg', 'school', 'fun', 'bi']</td>\n",
       "      <td>[('i', ['1']), ('i', ['1']), ('it', ['3']), ('i', ['1']), ('they', ['3'])]</td>\n",
       "      <td>['feel', 'sick', 'make']</td>\n",
       "      <td>[['i', 'actually', 'sick', 'understand', 'wanna', 'gon', 'bi'], ['i', 'do', 'n’t', '’s'], ['how', 'it', 'men', '&gt;', 'rn'], ['they', '’re', 'bully', 'and', 'make'], ['fun', 'of', 'for']]</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>-0.9814</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.09878</td>\n",
       "      <td>False</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>you’d have to get girls who bullied in high school to ADMIT they were bullies first, though..... imagine lmao</td>\n",
       "      <td>['high']</td>\n",
       "      <td>['girls', 'school', 'bullies']</td>\n",
       "      <td>[('you', ['2']), ('who', []), ('they', ['3'])]</td>\n",
       "      <td>['have']</td>\n",
       "      <td>[['you', '’d', 'get', 'imagine'], ['they', 'bullies', 'first', ',']]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[('first', 'ORDINAL')]</td>\n",
       "      <td>0.3254</td>\n",
       "      <td>-0.9260</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>True</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The God of Abraham works this way: dirt done is visited upon perpetrators. Your face is in headlines news across Europe for lowlife abusive language. Nothing to do representing Christian decency of Floridians, just making America look like an immature school yard of bullies.</td>\n",
       "      <td>['abusive', 'Christian', 'immature']</td>\n",
       "      <td>['way', 'dirt', 'perpetrators', 'face', 'headlines', 'news', 'lowlife', 'language', 'decency', 'school', 'yard', 'bullies']</td>\n",
       "      <td>[('God', []), ('face', []), ('America', [])]</td>\n",
       "      <td>['visited', 'is', 'Nothing']</td>\n",
       "      <td>[['God', 'way'], ['works', ':', 'dirt', 'is', 'upon', '.'], ['face', 'in', '.'], ['do', ',', 'making', '.'], ['America', 'like']]</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>[('Abraham', 'PERSON'), ('Europe', 'LOC'), ('Christian', 'NORP'), ('Floridians', 'NORP'), ('America', 'GPE')]</td>\n",
       "      <td>-0.7579</td>\n",
       "      <td>-0.9952</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>44</td>\n",
       "      <td>0.05888</td>\n",
       "      <td>False</td>\n",
       "      <td>Critique/Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 Tweet  \\\n",
       "0                                                                                                                                                                                         I just saw a girl who bullied me in high school twerking on facebook live, good for her lmao   \n",
       "1                                                                                                                                                                                  the girls who wore this shit bullied me in high school so if that gives any context to its audience   \n",
       "2                                                                                                                      i actually feel sick i don’t understand how it’s men > women rn omg i don’t wanna go back to that school they’re gonna bully me and make fun of me for being bi   \n",
       "3                                                                                                                                                                        you’d have to get girls who bullied in high school to ADMIT they were bullies first, though..... imagine lmao   \n",
       "4  The God of Abraham works this way: dirt done is visited upon perpetrators. Your face is in headlines news across Europe for lowlife abusive language. Nothing to do representing Christian decency of Floridians, just making America look like an immature school yard of bullies.   \n",
       "\n",
       "                             Adjectives  \\\n",
       "0                      ['high', 'good']   \n",
       "1                              ['high']   \n",
       "2                              ['sick']   \n",
       "3                              ['high']   \n",
       "4  ['abusive', 'Christian', 'immature']   \n",
       "\n",
       "                                                                                                                         Nouns  \\\n",
       "0                                                                           ['girl', 'school', 'twerking', 'facebook', 'lmao']   \n",
       "1                                                                           ['girls', 'shit', 'school', 'context', 'audience']   \n",
       "2                                                                               ['men', 'women', 'omg', 'school', 'fun', 'bi']   \n",
       "3                                                                                               ['girls', 'school', 'bullies']   \n",
       "4  ['way', 'dirt', 'perpetrators', 'face', 'headlines', 'news', 'lowlife', 'language', 'decency', 'school', 'yard', 'bullies']   \n",
       "\n",
       "                                                                     Subjects  \\\n",
       "0                                                 [('I', ['1']), ('who', [])]   \n",
       "1                                  [('girls', []), ('who', []), ('that', [])]   \n",
       "2  [('i', ['1']), ('i', ['1']), ('it', ['3']), ('i', ['1']), ('they', ['3'])]   \n",
       "3                              [('you', ['2']), ('who', []), ('they', ['3'])]   \n",
       "4                                [('God', []), ('face', []), ('America', [])]   \n",
       "\n",
       "                    Complements  \\\n",
       "0                       ['saw']   \n",
       "1                   ['bullied']   \n",
       "2      ['feel', 'sick', 'make']   \n",
       "3                      ['have']   \n",
       "4  ['visited', 'is', 'Nothing']   \n",
       "\n",
       "                                                                                                                                                                      CCOMP or CONJ Children  \\\n",
       "0                                                                                                                                                                    [['I', 'just', 'girl']]   \n",
       "1                                                                                                                                                     [['girls', 'me', 'in', 'so', 'gives']]   \n",
       "2  [['i', 'actually', 'sick', 'understand', 'wanna', 'gon', 'bi'], ['i', 'do', 'n’t', '’s'], ['how', 'it', 'men', '>', 'rn'], ['they', '’re', 'bully', 'and', 'make'], ['fun', 'of', 'for']]   \n",
       "3                                                                                                                       [['you', '’d', 'get', 'imagine'], ['they', 'bullies', 'first', ',']]   \n",
       "4                                                          [['God', 'way'], ['works', ':', 'dirt', 'is', 'upon', '.'], ['face', 'in', '.'], ['do', ',', 'making', '.'], ['America', 'like']]   \n",
       "\n",
       "   CCOMP or CONJ Children Sum Sentiment  \\\n",
       "0                                0.0000   \n",
       "1                                0.0000   \n",
       "2                               -0.4939   \n",
       "3                                0.0000   \n",
       "4                                0.2944   \n",
       "\n",
       "                                                                                                  Named Entities  \\\n",
       "0                                                                                                             []   \n",
       "1                                                                                                             []   \n",
       "2                                                                                                             []   \n",
       "3                                                                                         [('first', 'ORDINAL')]   \n",
       "4  [('Abraham', 'PERSON'), ('Europe', 'LOC'), ('Christian', 'NORP'), ('Floridians', 'NORP'), ('America', 'GPE')]   \n",
       "\n",
       "   Sentiment Score  Synonyms Sentiment Score  Has 1st Person  Has 2nd Person  \\\n",
       "0           0.4019                    0.9986            True           False   \n",
       "1          -0.8537                   -0.9974           False           False   \n",
       "2          -0.4939                   -0.9814            True           False   \n",
       "3           0.3254                   -0.9260           False            True   \n",
       "4          -0.7579                   -0.9952           False           False   \n",
       "\n",
       "   Has 3rd Person  Word Count  Adjusted Sentiment Score  Has Capital Word  \\\n",
       "0           False          19                   0.00000              True   \n",
       "1           False          20                   0.00000             False   \n",
       "2            True          34                  -0.09878             False   \n",
       "3            True          19                   0.00000              True   \n",
       "4           False          44                   0.05888             False   \n",
       "\n",
       "   Classification  \n",
       "0  Critique/Other  \n",
       "1  Critique/Other  \n",
       "2  Critique/Other  \n",
       "3  Critique/Other  \n",
       "4  Critique/Other  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(row):\n",
    "    education_terms = ['school', 'schools', 'college', 'graduation']\n",
    "    if any(term in str(word) for term in education_terms for word in row[['Nouns', 'Tweet']]):\n",
    "        return 'Critique/Other'\n",
    "    elif row['Adjusted Sentiment Score'] > -0.20:\n",
    "        return 'Critique/Other'\n",
    "    elif row['Adjusted Sentiment Score'] < -0.15 and row['Sentiment Score'] > 0.3 and row['Synonyms Sentiment Score'] > 0.2:\n",
    "        return 'Critique/Other'\n",
    "    elif row['Has Capital Word'] == True and row['Adjusted Sentiment Score'] < -0.10 and 'you' in row['Subjects'] or 'your' in row['Subjects']:\n",
    "        return 'Insults'\n",
    "    else:\n",
    "        return 'Insults'\n",
    "\n",
    "df1['Classification'] = df1.apply(classify, axis=1)\n",
    "df1.shape[0]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Classification</th>\n",
       "      <th>Adjusted Sentiment Score</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>CCOMP or CONJ Children</th>\n",
       "      <th>Has 1st Person</th>\n",
       "      <th>Has 2nd Person</th>\n",
       "      <th>Has 3rd Person</th>\n",
       "      <th>Named Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>So you admit you were wrong about Muslim extremists?</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.238350</td>\n",
       "      <td>[('you', ['2']), ('you', ['2'])]</td>\n",
       "      <td>['admit', 'wrong']</td>\n",
       "      <td>[['So', 'you', 'were', '?'], ['you', 'wrong']]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[('Muslim', 'NORP')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9064</th>\n",
       "      <td>Stop trolling you idiot !!!</td>\n",
       "      <td>Insults</td>\n",
       "      <td>-0.296000</td>\n",
       "      <td>[('you', ['2'])]</td>\n",
       "      <td>['idiot']</td>\n",
       "      <td>[['Stop', 'you', '!', '!', '!']]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>folding under corporate pressure after allowing me to be bullied by Blue Check Journalist who are high off jealousy and running a smear campaign</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[('who', [])]</td>\n",
       "      <td>['folding', 'running']</td>\n",
       "      <td>[['under', 'after'], ['me', 'to', 'be', 'by'], ['who', 'off', 'and', 'running'], ['campaign']]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[('Blue Check', 'ORG')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4677</th>\n",
       "      <td>What substance is being expressed when they attack women with vile sexist slurs, gay men with vile homophobic slurs, black people with vile racist slurs. When they threaten peoples' children. When they threaten women with rape.</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[('they', ['3']), ('they', ['3']), ('they', ['3'])]</td>\n",
       "      <td>['expressed', 'people', 'threaten', 'threaten']</td>\n",
       "      <td>[['substance', 'is', 'being', 'men', '.'], ['black', 'with'], ['When', 'they', 'children', '.'], ['When', 'they', 'women', 'with', '.']]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>_ dis s more lyk a gay joke, not rape! lol</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[('dis', [])]</td>\n",
       "      <td>['_', 'rape', 'lol']</td>\n",
       "      <td>[[], ['dis', '!'], []]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[('dis s', 'PERSON')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8621</th>\n",
       "      <td>first, i noticed the misogyny. but then i realized - plz brush your teeth.</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>0.025733</td>\n",
       "      <td>[('i', ['1']), ('i', ['1'])]</td>\n",
       "      <td>['noticed', 'realized', 'plz']</td>\n",
       "      <td>[['first', ',', 'i', 'misogyny', '.'], ['but', 'then', 'i', 'plz', 'brush', '.'], ['teeth']]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[('first', 'ORDINAL')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11090</th>\n",
       "      <td>This piece reads like a bizarro PSA from the 50s: \"'far right ideology bled into his daily life. He began referring to himself as a conservative. He dated a christian woman.' 'it was kind of sad. How did you get this way?' Said, a friend of Mr. Cain\"</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>0.063067</td>\n",
       "      <td>[('piece', []), ('ideology', []), ('He', ['3']), ('He', ['3']), ('it', ['3']), ('you', ['2'])]</td>\n",
       "      <td>['reads', 'began', 'dated', 'was', 'sad', 'get', 'Said']</td>\n",
       "      <td>[['piece', 'like', ':', 'bled', '.'], ['He', 'referring', '.'], ['He', 'woman', '.', \"'\"], [\"'\", 'it', 'sad', '.'], ['How', 'did', 'you', 'way', '?', \"'\"], [',', 'friend', '\"']]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[('PSA', 'ORG'), ('50s', 'DATE'), ('christian', 'NORP'), ('Said', 'PERSON'), ('Cain', 'PERSON')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5456</th>\n",
       "      <td>I wanna feel bad for you but then i remember you made rape jokes/anti gay jokes sooo</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>-0.180767</td>\n",
       "      <td>[('I', ['1']), ('i', ['1']), ('you', ['2'])]</td>\n",
       "      <td>['feel', 'bad', 'remember']</td>\n",
       "      <td>[['I', 'wanna', 'bad', 'for', 'but', 'remember', 'sooo'], ['then', 'i', 'made'], ['you']]</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3242</th>\n",
       "      <td>Did u even watch the full video, the indian man steped right into the crowd the boys didnt do anything , except claims harrasment after a black kid said jesus was black, dont say white bois im white and friends with tons of colored , whatchu got up rn is racism</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>-0.018525</td>\n",
       "      <td>[('u', []), ('man', []), ('boys', []), ('harrasment', []), ('kid', []), ('jesus', []), ('i', ['1']), ('whatchu', [])]</td>\n",
       "      <td>['do', 'said', 'black', 'white', 'friends']</td>\n",
       "      <td>[['Did', 'u', 'even', 'video'], ['watch', ',', 'man', 'into', 'do'], ['boys', 'did', 'nt', 'anything', ','], ['steped', 'except', 'harrasment', 'after', 'kid', 'was', 'is'], ['jesus', 'black'], ['i', 'white', 'and', 'friends'], ['with'], ['say', 'got', 'racism']]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[('indian', 'NORP'), ('jesus', 'PERSON')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7226</th>\n",
       "      <td>Bacon to keep the happy, smart</td>\n",
       "      <td>Critique/Other</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>[('Bacon', [])]</td>\n",
       "      <td>['keep']</td>\n",
       "      <td>[['Bacon', 'to', 'smart']]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[('Bacon', 'ORG')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                       Tweet  \\\n",
       "7971                                                                                                                                                                                                                    So you admit you were wrong about Muslim extremists?   \n",
       "9064                                                                                                                                                                                                                                             Stop trolling you idiot !!!   \n",
       "222                                                                                                                         folding under corporate pressure after allowing me to be bullied by Blue Check Journalist who are high off jealousy and running a smear campaign   \n",
       "4677                                     What substance is being expressed when they attack women with vile sexist slurs, gay men with vile homophobic slurs, black people with vile racist slurs. When they threaten peoples' children. When they threaten women with rape.   \n",
       "4362                                                                                                                                                                                                                              _ dis s more lyk a gay joke, not rape! lol   \n",
       "8621                                                                                                                                                                                              first, i noticed the misogyny. but then i realized - plz brush your teeth.   \n",
       "11090             This piece reads like a bizarro PSA from the 50s: \"'far right ideology bled into his daily life. He began referring to himself as a conservative. He dated a christian woman.' 'it was kind of sad. How did you get this way?' Said, a friend of Mr. Cain\"   \n",
       "5456                                                                                                                                                                                    I wanna feel bad for you but then i remember you made rape jokes/anti gay jokes sooo   \n",
       "3242   Did u even watch the full video, the indian man steped right into the crowd the boys didnt do anything , except claims harrasment after a black kid said jesus was black, dont say white bois im white and friends with tons of colored , whatchu got up rn is racism   \n",
       "7226                                                                                                                                                                                                                                          Bacon to keep the happy, smart   \n",
       "\n",
       "       Classification  Adjusted Sentiment Score  \\\n",
       "7971          Insults                 -0.238350   \n",
       "9064          Insults                 -0.296000   \n",
       "222    Critique/Other                  0.000000   \n",
       "4677   Critique/Other                  0.000000   \n",
       "4362   Critique/Other                  0.000000   \n",
       "8621   Critique/Other                  0.025733   \n",
       "11090  Critique/Other                  0.063067   \n",
       "5456   Critique/Other                 -0.180767   \n",
       "3242   Critique/Other                 -0.018525   \n",
       "7226   Critique/Other                  0.401900   \n",
       "\n",
       "                                                                                                                    Subjects  \\\n",
       "7971                                                                                        [('you', ['2']), ('you', ['2'])]   \n",
       "9064                                                                                                        [('you', ['2'])]   \n",
       "222                                                                                                            [('who', [])]   \n",
       "4677                                                                     [('they', ['3']), ('they', ['3']), ('they', ['3'])]   \n",
       "4362                                                                                                           [('dis', [])]   \n",
       "8621                                                                                            [('i', ['1']), ('i', ['1'])]   \n",
       "11090                         [('piece', []), ('ideology', []), ('He', ['3']), ('He', ['3']), ('it', ['3']), ('you', ['2'])]   \n",
       "5456                                                                            [('I', ['1']), ('i', ['1']), ('you', ['2'])]   \n",
       "3242   [('u', []), ('man', []), ('boys', []), ('harrasment', []), ('kid', []), ('jesus', []), ('i', ['1']), ('whatchu', [])]   \n",
       "7226                                                                                                         [('Bacon', [])]   \n",
       "\n",
       "                                                    Complements  \\\n",
       "7971                                         ['admit', 'wrong']   \n",
       "9064                                                  ['idiot']   \n",
       "222                                      ['folding', 'running']   \n",
       "4677            ['expressed', 'people', 'threaten', 'threaten']   \n",
       "4362                                       ['_', 'rape', 'lol']   \n",
       "8621                             ['noticed', 'realized', 'plz']   \n",
       "11090  ['reads', 'began', 'dated', 'was', 'sad', 'get', 'Said']   \n",
       "5456                                ['feel', 'bad', 'remember']   \n",
       "3242                ['do', 'said', 'black', 'white', 'friends']   \n",
       "7226                                                   ['keep']   \n",
       "\n",
       "                                                                                                                                                                                                                                                        CCOMP or CONJ Children  \\\n",
       "7971                                                                                                                                                                                                                            [['So', 'you', 'were', '?'], ['you', 'wrong']]   \n",
       "9064                                                                                                                                                                                                                                          [['Stop', 'you', '!', '!', '!']]   \n",
       "222                                                                                                                                                                             [['under', 'after'], ['me', 'to', 'be', 'by'], ['who', 'off', 'and', 'running'], ['campaign']]   \n",
       "4677                                                                                                                                  [['substance', 'is', 'being', 'men', '.'], ['black', 'with'], ['When', 'they', 'children', '.'], ['When', 'they', 'women', 'with', '.']]   \n",
       "4362                                                                                                                                                                                                                                                    [[], ['dis', '!'], []]   \n",
       "8621                                                                                                                                                                              [['first', ',', 'i', 'misogyny', '.'], ['but', 'then', 'i', 'plz', 'brush', '.'], ['teeth']]   \n",
       "11090                                                                                        [['piece', 'like', ':', 'bled', '.'], ['He', 'referring', '.'], ['He', 'woman', '.', \"'\"], [\"'\", 'it', 'sad', '.'], ['How', 'did', 'you', 'way', '?', \"'\"], [',', 'friend', '\"']]   \n",
       "5456                                                                                                                                                                                 [['I', 'wanna', 'bad', 'for', 'but', 'remember', 'sooo'], ['then', 'i', 'made'], ['you']]   \n",
       "3242   [['Did', 'u', 'even', 'video'], ['watch', ',', 'man', 'into', 'do'], ['boys', 'did', 'nt', 'anything', ','], ['steped', 'except', 'harrasment', 'after', 'kid', 'was', 'is'], ['jesus', 'black'], ['i', 'white', 'and', 'friends'], ['with'], ['say', 'got', 'racism']]   \n",
       "7226                                                                                                                                                                                                                                                [['Bacon', 'to', 'smart']]   \n",
       "\n",
       "       Has 1st Person  Has 2nd Person  Has 3rd Person  \\\n",
       "7971            False            True           False   \n",
       "9064            False            True           False   \n",
       "222             False           False           False   \n",
       "4677            False           False            True   \n",
       "4362            False           False           False   \n",
       "8621             True           False           False   \n",
       "11090           False            True            True   \n",
       "5456             True            True           False   \n",
       "3242             True           False           False   \n",
       "7226            False           False           False   \n",
       "\n",
       "                                                                                         Named Entities  \n",
       "7971                                                                               [('Muslim', 'NORP')]  \n",
       "9064                                                                                                 []  \n",
       "222                                                                             [('Blue Check', 'ORG')]  \n",
       "4677                                                                                                 []  \n",
       "4362                                                                              [('dis s', 'PERSON')]  \n",
       "8621                                                                             [('first', 'ORDINAL')]  \n",
       "11090  [('PSA', 'ORG'), ('50s', 'DATE'), ('christian', 'NORP'), ('Said', 'PERSON'), ('Cain', 'PERSON')]  \n",
       "5456                                                                                                 []  \n",
       "3242                                                          [('indian', 'NORP'), ('jesus', 'PERSON')]  \n",
       "7226                                                                                 [('Bacon', 'ORG')]  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set max rows and columns\n",
    "# Set max column width\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Now print your dataframe\n",
    "df1[['Tweet', 'Classification', 'Adjusted Sentiment Score', 'Subjects','Complements', \n",
    "     \"CCOMP or CONJ Children\",'Has 1st Person', 'Has 2nd Person', 'Has 3rd Person', 'Named Entities']].sample(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature explanation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That nsubj 's AUX []\n",
      "'s ROOT 's AUX [That, OBAMA, to, tayyoung, _, :]\n",
      "PRESIDENT compound OBAMA PROPN []\n",
      "OBAMA attr 's AUX [PRESIDENT]\n",
      "to prep 's AUX [you]\n",
      "you pobj to ADP []\n",
      "tayyoung attr 's AUX []\n",
      "_ attr 's AUX []\n",
      ": punct 's AUX []\n",
      "FUCK compound OBAMA PROPN []\n",
      "OBAMA ROOT OBAMA PROPN [FUCK, ,, nigger]\n",
      ", punct OBAMA PROPN []\n",
      "dumb amod nigger NOUN []\n",
      "ass compound nigger NOUN []\n",
      "nigger appos OBAMA PROPN [dumb, ass]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator at 0x288971da0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"HOE you mad? damn right our PRESI§DENT IS BLACK ,tf you gone do bouh it? he smartn than romney ass : FUCK OBAMA, dumb ass nigger\"\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "instance1 = nlp(test)\n",
    "    # for token in doc_text:\n",
    "    #     print(token.text,\" : \",token.dep_, '|', token.head)\n",
    "for token in instance1:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        [child for child in token.children])\n",
    "\n",
    "token.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7ccdcc998c4b4ad5850994dcc8a4eac0-0\" class=\"displacy\" width=\"2500\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">believe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Russians</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">beautiful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Justin</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Trudeau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">angel,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">him</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 2150.0,2.0 2150.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-8\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-10\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-12\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7ccdcc998c4b4ad5850994dcc8a4eac0-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,354.0 L2318.0,342.0 2302.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = \"HOE you mad? damn right our PRESI§DENT IS BLACK ,tf you gone do bouh it? he smartn than romney ass : FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "instance = nlp(test)\n",
    "#This part of the library is imported for graphic visualization only.\n",
    "from spacy import displacy\n",
    "\n",
    "#For our doc object, we display the dependency graph, which allows us to better understand/analyze these relationships manually\n",
    "displacy.render(instance, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['I', 'are'], ['Russians', 'beautiful', 'and', 'is'], [], ['believe', ',', 'I', 'him']]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Test the CCOMP function \n",
    "test = \"That's PRESIDENT OBAMA to you tayyoung_: FUCK OBAMA, dumb ass nigger\"\n",
    "test = 'Bell, based in Los Angeles, makes and distributes electronic, computer and building products.'\n",
    "test = \"She does not look very beautiful\"\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "doc = nlp(test)\n",
    "# Extract adjectives, nouns, subjects and complements/conjunctions/root\n",
    "adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "complements = [token.text for token in doc if token.dep_ in (\"ccomp\", \"conj\", \"ROOT\")]\n",
    "\n",
    "# Extract children of ccomp or conj\n",
    "ccomp_conj_children = []\n",
    "ccomp_conj_sentiments = []\n",
    "for token in doc:\n",
    "    if token.dep_ in (\"acomp\", 'ROOT', 'ccomp'): # \"ccomp\", \"obj\", 'ROOT''AUX'\n",
    "        children = [child.text for child in token.children]\n",
    "        #children = token.text\n",
    "        ccomp_conj_children.append(children)\n",
    "        # Compute sentiment scores for each child and append to list\n",
    "        #ccomp_conj_sentiments.extend([analyzer.polarity_scores(child)[\"compound\"] for child in children])\n",
    "\n",
    "print(ccomp_conj_children)\n",
    "print(ccomp_conj_sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
