{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyberbullying Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The degree of aggressivity & Negativity\n",
    "\n",
    "### 2. Personnalization\n",
    "    - Depency tree, establish the subject, object, complement, aux\n",
    "    - Also, tell the (person 1st, 2nd, 3rd) of the subject and the object. \n",
    "    - Identify what are they accuse from \n",
    "    - Identify who’s being targeted (Lexicon or wordnet)\n",
    "    - Established if the aux is directed to the object  (Establishing intention)\n",
    "### 3. Harms inflicted and to who\n",
    "    - Now, using the complement (output of dependency tree), we can map these to threats\n",
    "    - We need wordnet to identify threat. (Actually better lexicon than word net)\n",
    "    - To object refers to what category? race, nationality, religion, color, gender, \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your environment\n",
    "- source spacy/bin/activate (Bash)\n",
    "- get back to your base: deactivate (Bash)\n",
    "\n",
    "You have to be in anaconda3 interpreter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/philippebeliveau/Desktop/Notebook/Winter_2024/Text_mining/Git_MiningRepository/Text_miningProject/Notebook/cyberbullying_tweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ck/hjxynrwd7f74cn88t3sb1_j80000gn/T/ipykernel_62493/2512720622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmac_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/philippebeliveau/Desktop/Notebook_Jupyter_R/Winter_2024/Text_mining/Project/Text_miningProject/Notebook/cyberbullying_tweets.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmacbook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/philippebeliveau/Desktop/Notebook/Winter_2024/Text_mining/Git_MiningRepository/Text_miningProject/Notebook/cyberbullying_tweets.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "macbook = ('/Users/philippebeliveau/Desktop/Notebook/Winter_2024/Text_mining/Git_MiningRepository/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "mac_mini = ('/Users/philippebeliveau/Desktop/Notebook_Jupyter_R/Winter_2024/Text_mining/Project/Text_miningProject/Notebook/cyberbullying_tweets.csv')\n",
    "\n",
    "df = pd.read_csv(macbook)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not_cyberbullying Tweets:\n",
      "1. In other words #katandandre, your food was crapilicious! #mkr\n",
      "2. Why is #aussietv so white? #MKR #theblock #ImACelebrityAU #today #sunrise #studio10 #Neighbours #WonderlandTen #etc\n",
      "3. @XochitlSuckkks a classy whore? Or more red velvet cupcakes?\n",
      "4. @Jason_Gio meh. :P  thanks for the heads up, but not too concerned about another angry dude on twitter.\n",
      "5. @RudhoeEnglish This is an ISIS account pretending to be a Kurdish account.  Like Islam, it is all lies.\n",
      "6. @Raja5aab @Quickieleaks Yes, the test of god is that good or bad or indifferent or weird or whatever, it all proves gods existence.\n",
      "7. Itu sekolah ya bukan tempat bully! Ga jauh kaya neraka\n",
      "8. Karma. I hope it bites Kat on the butt. She is just nasty. #mkr\n",
      "9. @stockputout everything but mostly my priest\n",
      "10. Rebecca Black Drops Out of School Due to Bullying:\n",
      "\n",
      "Gender Tweets:\n",
      "1. rape is real..zvasiyana nema jokes about being drunk or being gay or being lesbian...rape is not ones choice or wish..thtz where the sensitivity is coming from\n",
      "2. You never saw any celebrity say anything like this for Obama: B Maher Incest Rape 'Joke' S Colbert Gay 'joke' K Griffin beheading 'joke'\n",
      "3. @ManhattaKnight I mean he's gay, but he uses gendered slurs and makes rape jokes\n",
      "4. RT @Raul_Novoa16: @AliciaBernardez @Alex_Aim @_mecaesmal  feminazi\n",
      "5. Rape is rape. And the fact that I read one post about a guy getting raped and the comments are calling him gay and he should be happy...? stfu and I really hope no one takes this as a joke tf you own no ones body. You have no rights to do whatever you want to someone else.\n",
      "6. @coiny Also, it's hard to take a company seriously re: harassment when the game has rape jokes &amp; gay jokes and treats women the way it does.\n",
      "7. Idgaf if you are gay, lesbian, bisexual, or whatever the fuck you are, you don't fuck make a rape joke and think it's fucken ok\n",
      "8. #GermanProfessor gives meaning to term FemiNazi.Doesn't hire \"Indian Men\" bcz of \"Indias's Rape Problem\"\n",
      "9. RT @mcclure111: #DontDateSJWs #ThatWouldBeAViolationOfTheRestrainingOrder\n",
      "10. So I call you female I’m basically calling you a bitch ?\n",
      "\n",
      "Religion Tweets:\n",
      "1. Sudeep, did she invite him though? No right? Why are you getting worded up? You're okay with Parvesh Verma cause he speaks against Muslims but against an idiot like Imam because he called for chakka jam?\n",
      "2. @discerningmumin Islam has never been a resistance to oppression. It has always been source of oppression to both believers and non believer\n",
      "3. Boy, your comment about Journalists wanting to keep churches closed is beneath you. As a Christian woman and human being your bosses filth is brushing off on you. Not at all unbiased and a down right lie. SHAME ON YOU.\n",
      "4. @ShashiTharoor @INCIndia Hindus were and are getting killed by Muslims terriorists in Kashmir. Congress mukt bharat will certainly happen if congressmen like you don’t change their idealogies and keep sounding like idiots. You actually don’t need enemies.\n",
      "5. White supremicists? How many do you know? There a few idiots in all races. Where is anti-semitism coming from? Dems, BLM, Antifa, Muslims. You won’t appease them by throwing white supremacy. They hate you &amp; want to destroy Israel &amp; all Jews &amp; you know it.\n",
      "6. Urban naxal and a jihadi TT gang put together..wen more than 95% ot the population are happy to be united as one nation inspite of their diversity...who the F are u to preach otherwise..ur just a petty muslim activist...or should i say ground operative for Islamic Terrorism\n",
      "7. @OneLegSandpiper @DblBlackDs So what is to blame for the more than 100 Islamist terrorists organizations? The Book of Mormon?\n",
      "8. It's not bjp, it's about you guys always support bad Muslims which makes you bad too, stop supporting those idiots..Hindus never do that..!!\n",
      "9. @PetaIndia @peta IDIOTS, Rakshabandhan is a Hindu festival, and Hindus by their nature protect COWS! If you have guts put same poster and Tweet, for any Muslim festival! Now shut the f..k up and get lost\n",
      "10. But I think junk that u r born through Halala pl check Ur DNA bcz u people marry Ur chacheri mamaeri phuphurei sister how can u mentain this is ur actual face &amp; lastly the result is HALALAA . Abdul am I right HV u guts to say anything about Islamic Terrorism no bcz they r Ur com\n",
      "\n",
      "Other_cyberbullying Tweets:\n",
      "1. @ikralla fyi, it looks like I was caught by it. I'm not a botter, so...\n",
      "2. I need to just switch to an organization-based github, but I don't want to pay $25/month because I'm cheap. :\\\n",
      "3. RMAed my monoprice. Shoddy power bricks on those. Getting a refund and picking up another ASUS VG278HE. It's cheaper, anyways.\n",
      "4. @murphy_slaw https://t.co/M8w8xnUnDL\n",
      "5. @1Life0Continues i've got the code to interpret &amp; analyze, but the weights themselves are going to take a lot of fiddling.\n",
      "6. @krainboltgreene didn't mention your name. i call out behavior, not people. &lt;3\n",
      "7. RT @exokmv: i swear people are so fucking ignorant and annoying\n",
      "8. Yeah, I've got a bit of a rageboner. Mentally speaking.\n",
      "9. RT @Laralyn: Uh... wow. https://t.co/CdoNLaApVr\n",
      "10. The morning after, hung over, struggling to remember how that pile of o'reilly books ended up in your bed. \"WHAT DID I CODE LAST NIGHT\"\n",
      "\n",
      "Age Tweets:\n",
      "1. Here at home. Neighbors pick on my family and I. Mind you my son is autistic. It feels like high school. They call us names attack us for no reason and bully us all the time. Can't step on my front porch without them doing something to us\n",
      "2. Being bullied at school: High-achieving boys use further strategies to maintain masculinity, for example becoming bullies themselves, disrupting the lessons, or devaluing girls’ achievements https://www.bipartisanalliance.com/2019/12/being-bullied-at-school-high-achieving.html?spref=tw\n",
      "3. There was a girl in my class in 6th grade who was a little autistic, and her parents thought it’d be better to send her to a public school. She didn’t fit in with the class and was lowkey bullied (it was actually high key but the teacher downplayed it and did nothing)\n",
      "4. He’s probably a white gay kid from some suburb. We don’t want him to shoot up his school...so no one bully him.\n",
      "5. You are pushed ti resorting. Treating thr bullies is you percent then from being pressured. Most school sitters evidence severe bullying. Treat thre bullies and it isn't a problem.\n",
      "6. Today was an awesome day. Vivaldi called me a VSCO girl and Kali bullied my high school bully. I still can not believe I created my two best friends. I will never be lonely again. Life rules and being a mom is the best.\n",
      "7. Trump is like that jealous little school bully who can’t stand it when everbody loves the other guy.\n",
      "8. oh and also, saying that we’re on the same level of the columbine jocks just because we didn’t add you to a fucking gc is so SO fucking disrespectful to dylan and eric. if you were a columbiner, you’d understand that. we don’t bully people into sh**ting up their fucking schools.\n",
      "9. My high school life got better when I finally snapped and punched my abusive bully: hidd shocked expression and tears made the detention I got more than worth it.\n",
      "10. I have collaborated with Ms. Luenavos and find her a strong force for good; not at all what Mr. White describes here. Feels to me like Mr. White looking to bully and attack. He's the wrong person to sit on OUR school board.\n",
      "\n",
      "Ethnicity Tweets:\n",
      "1. Hey dumb fuck celebs stop doing something for people for publicity on Facebook... Wtf happen to life u niggers are cowards.\n",
      "2. Fuck u bitch RT @tayyoung_: FUCK OBAMA, dumb ass nigger\n",
      "3. @JoeBiden No Joe, YOU are the RACIST. They have so many video and audio clips of the RACIST shit you have spewed in your past, How can anyone see you as anything but a RACIST. You hate Black people just admit it. And you loves the women to touch. Just look at your own stuff!\n",
      "4. when your truck looks dumb as fuck out trying to make it look worse. you ain't country. quit tryin nigger.\n",
      "5. That nigger food in the cafe today was disgusting\n",
      "6. Muthafuckas still EGO tripping!Fool fuck your damn ego and live life to the fullest!!! LOL at these dumb ass niggers with the bullshit!\n",
      "7. @iMADE_YouMOAN don't play dumb, I should fuck yo little white ass up you nigger. 3chainz.? Aw.\n",
      "8. I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR HAVE THE SUSPICION OF BEING CALLED A NIGGER! NIGGA! OR NEGRO AT ANY POINT IN YOUR LIFE YOU CANNOT USE THE N WORD. PERIOD. I am talking about non black people in specific!!!! White passing mixed kids are on THIN ICE.\n",
      "9. @CJ_Jacobs oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\n",
      "10. @Otoliver @Yo_PaTnA_dEm20 this bitch is dumb talk bout \"do not dew \"said not sayed\" nigger not nigga lol bitch fuck off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get unique categories of cyberbullying type\n",
    "categories = df['cyberbullying_type'].unique()\n",
    "\n",
    "# Iterate over each category\n",
    "for category in categories:\n",
    "    # Filter the dataset for the current category\n",
    "    category_tweets = df[df['cyberbullying_type'] == category]['tweet_text'].head(10)\n",
    "    \n",
    "    # Print the category name\n",
    "    print(f\"{category.capitalize()} Tweets:\")\n",
    "    \n",
    "    # Print the tweets for the current category\n",
    "    for i, tweet in enumerate(category_tweets):\n",
    "        print(f\"{i+1}. {tweet}\")\n",
    "    \n",
    "    # Add a newline for separation between categories\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- What about the links & Hashtag?\n",
    "- Do some processing for name entities\n",
    "\n",
    "N-gram features, which consider sequences of words, have proven to be highly predictive in identifying hate speech. Furthermore, character n-gram features are particularly effective as they address spelling variations by capturing the similarity between canonical spellings. In fact, character n-grams frequently outperform token n-grams in hate speech detection.\n",
    "Also, other surface features such as the frequency of URLs, punctuation usage, comments, token lengths, capitalization, and non-alphanumeric tokens offer valuable insights.\n",
    "\n",
    "## Tokenization \n",
    "https://spacy.io/usage/linguistic-features#tokenization\n",
    "\n",
    "https://spacy.io/api/tokenizer\n",
    "\n",
    "### Dependency parser\n",
    "https://spacy.io/usage/linguistic-features#sbd-parser\n",
    "\n",
    "### Rule base matching\n",
    "https://spacy.io/usage/rule-based-matching#fuzzy\n",
    "\n",
    "Hashtag: https://spacy.io/usage/rule-based-matching#example3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction 1\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Construction 2\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't believe Iranians are gay, but they uses gendered slurs and makes rape jokes\n"
     ]
    }
   ],
   "source": [
    "#doc = nlp(\"I don't believe Iranians are gay, but they uses gendered slurs and makes rape jokes\")\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text)\n",
    "\n",
    "# Statistical sentence segmenter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(\"I don't believe Iranians are gay, but they uses gendered slurs and makes rape jokes\")\n",
    "nlp.enable_pipe(\"senter\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "\n",
    "# for chunk in doc.noun_chunks:\n",
    "#     print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "#             chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Critique vs Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/philippebeliveau/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "from spacy import displacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word generalization \n",
    "Given the data sparsity problem inherent in hate speech detection, word generalization techniques are crucial. Word clustering, such as Brown clustering, enables the representation of sets of words as additional features. Latent Dirichlet Allocation (LDA) is another powerful technique that groups words into topics, providing a deeper understanding of the underlying content. Word embeddings and paragraph embeddings further enhance hate speech detection by capturing semantic relationships and contextual information.\n",
    "\n",
    "\n",
    "### Word net\n",
    "\n",
    "What other tool could be use instead of Wordnet?\n",
    "\n",
    "Disambiguation\n",
    "- Do we have any case of word that we should disambiguate?\n",
    "\n",
    "\n",
    "### Lexicons \n",
    "Identify good verb and adjective that identify to this lexicons\n",
    "1. Hate speech lexicon https://github.com/bgmartins/hate-speech-lexicons/tree/master\n",
    "\n",
    "### Name entities\n",
    "Entity Linking: https://spacy.io/usage/linguistic-features#entity-linking\n",
    "\n",
    "Named Entity Recognition NER using spaCy | NLP | Part 4\n",
    "\n",
    "- https://towardsdatascience.com/named-entity-recognition-ner-using-spacy-nlp-part-4-28da2ece57c6#:~:text=Text%20Processing%20using%20spaCy%20%7C%20NLP%20Library&text=Named%20Entity%20Recognition%20NER%20works,values%2C%20percentage%2C%20codes%20etc.\n",
    "- Could be use to identify the groups being targeted? \n",
    "\n",
    "The named entities are categories that answer these contextual questions: \n",
    "- Proper nouns such as people names, organizations, companies, etc. \n",
    "- Locations \n",
    "- Quantities, distances or values \n",
    "- Dates\n",
    "\n",
    "https://support.prodi.gy/t/entity-recognition-vs-text-classification/532/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "en_EntNom = \"Julie, the arabs are the ones that are going to push the real change. they are the ones in power, generally. we need their help.\"\n",
    "ent1 = \"oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philippebeliveau/miniforge3/lib/python3.10/site-packages/spacy/displacy/__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#On charge le texte dans le format  de Spacy et on utilise la fonction de visualisation displacy pour surligné les ENs\n",
    "doc = nlp(ent1)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you put Julie! instead of Julie, the model won't recognize it as an entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a function that tokenize the entities and allow us to replace the entities with their function \n",
    "#We execute the function on our example sentence, replacing the entities with their function. \n",
    "# We can then generalize all entities and reduce the noise in our data, for example!\n",
    "\n",
    "def replace_ent(doc_text):\n",
    "    #Search for the list of entities named in the text with their type\n",
    "    entity_dict = {}\n",
    "    for ent in doc_text.ents:\n",
    "        entity_dict[ent.text] = ent.label_\n",
    "    \n",
    "    token_list = []\n",
    "    #We tokenize the text with the entity condition\n",
    "    for token in doc_text:\n",
    "        if token.text in entity_dict:\n",
    "            token_list.append(entity_dict[token.text])\n",
    "        else:\n",
    "            token_list.append(token.text)\n",
    "            \n",
    "    return token_list\n",
    "\n",
    "token = replace_ent(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'look',\n",
       " \"'s\",\n",
       " 'dumb',\n",
       " 'niggers',\n",
       " 'ghost',\n",
       " 'acct',\n",
       " '!',\n",
       " 'Fuck',\n",
       " 'dumb',\n",
       " 'motherfucker',\n",
       " '!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token without stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def replace_ent(doc_text):\n",
    "    #Search for the list of entities named in the text with their type\n",
    "    entity_dict = {}\n",
    "    for ent in doc_text.ents:\n",
    "        entity_dict[ent.text] = ent.label_\n",
    "    \n",
    "    token_list = []\n",
    "    #We tokenize the text with the entity condition\n",
    "    for token in doc_text:\n",
    "        if token.text in entity_dict:\n",
    "            token_list.append(entity_dict[token.text])\n",
    "        else:\n",
    "            if token.text not in stop_words:\n",
    "                token_list.append(token.text)\n",
    "            \n",
    "    return token_list\n",
    "\n",
    "token = replace_ent(doc)\n",
    "token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag for oh: NN\n",
      "Tag for look: NN\n",
      "Tag for 's: POS\n",
      "Tag for dumb: NN\n",
      "Tag for niggers: NNS\n",
      "Tag for ghost: NN\n",
      "Tag for acct: NN\n",
      "Tag for !: .\n",
      "Tag for Fuck: NN\n",
      "Tag for dumb: NN\n",
      "Tag for motherfucker: NN\n",
      "Tag for !: .\n",
      ".\n",
      "{'dim', 'nooky', 'prick', 'sleep_together', 'mute', 'roll_in_the_hay', 'spectre', 'SOB', 'aspect', 'expect', 'have_it_off', 'nookie', 'obtuse', 'await', 'nigger', 'specter', 'fuck', 'ghostwriter', 'slow', 'piece_of_ass', 'lie_with', 'looking', 'front', 'asshole', 'love', 'cocksucker', 'wait', 'take_care', 'attend', 'face', 'bed', 'shag', 'dickhead', 'smell', 'Ohio', 'ghostwrite', 'looking_at', 'flavor', 'spirit', 'sleep_with', 'nigra', 'OH', 'screwing', 'jazz', 'motherfucker', 'have_it_away', 'appear', 'facial_expression', 'whoreson', 'shtup', 'spade', 'count', 'silent', 'get_it_on', 'bang', 'dense', 'be_intimate', 'Buckeye_State', 'dumb', 'touch', 'shade', 'dull', 'seem', 'jigaboo', 'look', 'know', 'hump', 'expression', 'screw', 'piece_of_tail', 'wraith', 'shit', 'flavour', 'feel', 'ghost', 'son_of_a_bitch', 'have_a_go_at_it', 'tone', 'calculate', 'make_love', 'bastard', 'fucking', 'make_out', 'trace', 'depend', 'obsess', 'get_laid', 'spook', 'feeling', 'have_intercourse', 'reckon', 'search', 'haunt', 'nigga', 'have_sex', 'mother_fucker', 'see', 'ass', 'coon', 'do_it', 'eff', 'speechless', 'bonk', 'bet'}\n",
      "{'back'}\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def find_synonyms(token_list):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "\n",
    "    for word in token_list:\n",
    "        # Get the part of speech tag of the word\n",
    "        pos_tag = nltk.pos_tag([word])[0][1]\n",
    "        print(f'Tag for {word}: {pos_tag}')\n",
    "        # Only find synonyms if the word is a noun or an adjective\n",
    "        if pos_tag.startswith('N') or pos_tag.startswith('J'):\n",
    "            for syn in wordnet.synsets(word):\n",
    "                for l in syn.lemmas():\n",
    "                    synonyms.append(l.name())\n",
    "                    if l.antonyms():\n",
    "                        antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "    return set(synonyms), set(antonyms), print(pos_tag)\n",
    "synonyms, antonyms , tag = find_synonyms(token)\n",
    "print(synonyms)\n",
    "print(antonyms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Question:\n",
    "- Use of Leveinstein distance? We would have to build a reference dictionnary which would be too much\n",
    "\n",
    "#### Normalization\n",
    "To create the normalization function, we need to add the merge_entities tool to our Spacy nlp pipeline nlp.add_pipe('merge_entities') \n",
    "\n",
    "- Texte d'origine :  Last night I went to the National Bank to look at a mortgage.\n",
    "- Texte normalisé :  Last night I went to National Bank of Canada to look at a mortgage .\n",
    "\n",
    "#### More advance methods for NER \n",
    "The simplest NER  systems are based on a  set of regular expressions,  which are used to obtain patterns of characters that are probably named entities. State-of-the-art systems are often hybrids*, made up of three different components: \n",
    "- Regex markers, which are always used to identify our recurring patterns \n",
    "- Statistical learning based on lexicographic (capitalization), syntactic (POS, position of the\n",
    "element in the sentence, etc.) and semantic (importance of the element) variables. \n",
    "- Elements from external systems, such as reference dictionaries (Wikipedia) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "A multi-step approach involves examining the presence of positive, negative, and neutral words, while a single-step approach focuses on the polar intensity of utterances, using tools like SentiStrength. \n",
    "- http://sentistrength.wlv.ac.uk/\n",
    "- https://pypi.org/project/PySentiStrength/\n",
    "- https://github.com/bobvdvelde/SentiStrength_for_python\n",
    "\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "- https://vadersentiment.readthedocs.io/en/latest/\n",
    "specifically attuned to sentiments expressed in social media. I\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic features\n",
    "Linguistic features, such as part-of-speech tags and syntactical dependency relationships, can offer insights into the offensive level of an utterance. Combining n-gram analysis with Part-of-Speech (POS) tagging enriches the hate speech detection process. Syntactical dependency relationships and offensive level scoring enhance the accuracy of classification models. These linguistic features provide deeper insights into the structure and intent of hate speech expressions.\n",
    "### Dependency tree\n",
    "https://spacy.io/usage/linguistic-features\n",
    "Dependency grammar analyze the functional role of words in addition to their syntactic role\n",
    "\n",
    "On the other hand, discourse theory looks at sequence of sentences.\n",
    " \n",
    "https://spacy.io/usage/rule-based-matching#models-rules-pos-dep\n",
    "### POS tagging\n",
    "With POS tagging, we recover the syntactic categories of words. As a reminder, syntactic categories include nouns, verbs, numbers (NUM) and punctuation (PUNCT).\n",
    "- helps in identifying distinction by identifying one bear as a noun and the other as a verb\n",
    "- Word-sense disambiguation\n",
    "    - \"The bear is a majestic animal\"\n",
    "    - \"Please bear with me\"\n",
    "- Sentiment analysis\n",
    "- Question answering\n",
    "- Fake news and opinion spam detection\n",
    "\n",
    "How to acces the categories: https://notebook.community/VictorQuintana91/Thesis/notebooks/002_pos_tagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['3']\n",
      "['3']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['2']\n",
      "['2']\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#We use the spacy loader with our previously defined example (cell 3)\n",
    " \n",
    "#en_EntNom = \"Hindus were and are getting killed by Muslims terriorists in Kashmir\"\n",
    "#en_EntNom = \"I believe Iranians are gay, but they uses gendered slurs and makes rape jokes\"\n",
    "en_EntNom = \"oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\"\n",
    "\n",
    "doc = nlp(en_EntNom)\n",
    "\n",
    "#We define a function that scans the text object and prints the text (token), its dependency relationship and the word with which it is linked.\n",
    "def dep_tree(doc_text): \n",
    "    for token in doc_text:\n",
    "        print( token.morph.get('Person'))\n",
    "data = dep_tree(doc)\n",
    "\n",
    "# token.text,\" : \" ,token.lemma_,\" : \", token.pos_,\" : \", token.tag_,\" : \", token.dep_,\" : \",\n",
    "#                 token.shape_,\" : \", token.is_alpha, \" : \",token.is_stop, \":\", token.morph, \":\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh  :  intj | 's\n",
      "look  :  intj | 's\n",
      "it  :  nsubj | 's\n",
      "'s  :  ROOT | 's\n",
      "that  :  mark | acct\n",
      "dumb  :  amod | niggers\n",
      "niggers  :  compound | acct\n",
      "ghost  :  compound | acct\n",
      "acct  :  attr | 's\n",
      "!  :  punct | 's\n",
      "Fuck  :  ROOT | Fuck\n",
      "you  :  dobj | Fuck\n",
      "you  :  dobj | Fuck\n",
      "dumb  :  amod | motherfucker\n",
      "motherfucker  :  dobj | Fuck\n",
      "!  :  punct | Fuck\n"
     ]
    }
   ],
   "source": [
    "#We define a function that scans the text object and prints the text (token), its dependency relationship and the word with which it is linked.\n",
    "test =  \"Christians were and are getting killed by Muslims terrorists in Kashmir\"\n",
    "test = \"oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\"\n",
    "test = nlp(test)\n",
    "def dep_tree(doc_text): \n",
    "    for token in doc_text:\n",
    "        print(token.text,\" : \",token.dep_, '|', token.head)\n",
    "        \n",
    "dep_tree(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it it nsubj 's\n",
      "that dumb niggers ghost acct acct attr 's\n",
      "you you dobj Fuck\n",
      "you you dobj Fuck\n",
      "dumb motherfucker motherfucker dobj Fuck\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh intj 's AUX []\n",
      "look intj 's AUX []\n",
      "it nsubj 's AUX []\n",
      "'s ROOT 's AUX [oh, look, it, acct, !]\n",
      "that mark acct NOUN []\n",
      "dumb amod niggers NOUN []\n",
      "niggers compound acct NOUN [dumb]\n",
      "ghost compound acct NOUN []\n",
      "acct attr 's AUX [that, niggers, ghost]\n",
      "! punct 's AUX []\n",
      "Fuck ROOT Fuck VERB [you, you, motherfucker, !]\n",
      "you dobj Fuck VERB []\n",
      "you dobj Fuck VERB []\n",
      "dumb amod motherfucker NOUN []\n",
      "motherfucker dobj Fuck VERB [dumb]\n",
      "! punct Fuck VERB []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# Finding a verb with a subject from below — good\n",
    "from spacy.symbols import nsubj, VERB, acomp\n",
    "verbs = set()\n",
    "for possible_subject in doc:\n",
    "    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB:\n",
    "        verbs.add(possible_subject.head)\n",
    "print(verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding a verb with a subject from above — less good\n",
    "verbs = []\n",
    "for possible_verb in doc:\n",
    "    if possible_verb.pos == VERB:\n",
    "        for possible_subject in possible_verb.children:\n",
    "            if possible_subject.dep == nsubj:\n",
    "                verbs.append(possible_verb)\n",
    "                break\n",
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Object</th>\n",
       "      <th>Aux</th>\n",
       "      <th>Subject Named Entity Text</th>\n",
       "      <th>Object Named Entity Text</th>\n",
       "      <th>Complement</th>\n",
       "      <th>Sentiment_SUBJ</th>\n",
       "      <th>Pronous</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6. Muthafuckas still EGO tripping!Fool fuck yo...</td>\n",
       "      <td>Muthafuckas</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>ORG</td>\n",
       "      <td>None</td>\n",
       "      <td>life</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[do]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>@iMADE_YouMOAN</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[do]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[do]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>@iMADE_YouMOAN</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[do]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>.</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[do]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>@iMADE_YouMOAN</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[do]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>dumb</td>\n",
       "      <td>-0.5106</td>\n",
       "      <td>[[], []]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>I</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[should]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>yo</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>I</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[should]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ass</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>[[1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>I</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[should]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>yo</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7. @iMADE_YouMOAN don't play dumb, I should fu...</td>\n",
       "      <td>I</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[should]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ass</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8. I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR...</td>\n",
       "      <td>I</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[WILL]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>IT</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[1]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8. I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR...</td>\n",
       "      <td>I</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[WILL]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>IT</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9. @CJ_Jacobs oh look it's that dumb niggers g...</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>acct</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[3]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9. @CJ_Jacobs oh look it's that dumb niggers g...</td>\n",
       "      <td>it</td>\n",
       "      <td>&lt;class 'object'&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>acct</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text         Subject  \\\n",
       "0   6. Muthafuckas still EGO tripping!Fool fuck yo...     Muthafuckas   \n",
       "1   7. @iMADE_YouMOAN don't play dumb, I should fu...               .   \n",
       "2   7. @iMADE_YouMOAN don't play dumb, I should fu...  @iMADE_YouMOAN   \n",
       "3   7. @iMADE_YouMOAN don't play dumb, I should fu...               .   \n",
       "4   7. @iMADE_YouMOAN don't play dumb, I should fu...  @iMADE_YouMOAN   \n",
       "5   7. @iMADE_YouMOAN don't play dumb, I should fu...               .   \n",
       "6   7. @iMADE_YouMOAN don't play dumb, I should fu...  @iMADE_YouMOAN   \n",
       "7   7. @iMADE_YouMOAN don't play dumb, I should fu...               I   \n",
       "8   7. @iMADE_YouMOAN don't play dumb, I should fu...               I   \n",
       "9   7. @iMADE_YouMOAN don't play dumb, I should fu...               I   \n",
       "10  7. @iMADE_YouMOAN don't play dumb, I should fu...               I   \n",
       "11  8. I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR...               I   \n",
       "12  8. I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR...               I   \n",
       "13  9. @CJ_Jacobs oh look it's that dumb niggers g...              it   \n",
       "14  9. @CJ_Jacobs oh look it's that dumb niggers g...              it   \n",
       "\n",
       "              Object       Aux Subject Named Entity Text  \\\n",
       "0   <class 'object'>        []                       ORG   \n",
       "1   <class 'object'>      [do]                      None   \n",
       "2   <class 'object'>      [do]                      None   \n",
       "3   <class 'object'>      [do]                      None   \n",
       "4   <class 'object'>      [do]                      None   \n",
       "5   <class 'object'>      [do]                      None   \n",
       "6   <class 'object'>      [do]                      None   \n",
       "7   <class 'object'>  [should]                      None   \n",
       "8   <class 'object'>  [should]                      None   \n",
       "9   <class 'object'>  [should]                      None   \n",
       "10  <class 'object'>  [should]                      None   \n",
       "11  <class 'object'>    [WILL]                      None   \n",
       "12  <class 'object'>    [WILL]                      None   \n",
       "13  <class 'object'>        []                      None   \n",
       "14  <class 'object'>        []                      None   \n",
       "\n",
       "   Object Named Entity Text Complement  Sentiment_SUBJ   Pronous  \n",
       "0                      None       life          0.0000      [[]]  \n",
       "1                      None       dumb         -0.5106  [[], []]  \n",
       "2                      None       dumb         -0.5106  [[], []]  \n",
       "3                      None       dumb         -0.5106  [[], []]  \n",
       "4                      None       dumb         -0.5106  [[], []]  \n",
       "5                      None       dumb         -0.5106  [[], []]  \n",
       "6                      None       dumb         -0.5106  [[], []]  \n",
       "7                      None         yo          0.0000     [[1]]  \n",
       "8                      None        ass         -0.5423     [[1]]  \n",
       "9                      None         yo          0.0000      [[]]  \n",
       "10                     None        ass         -0.5423      [[]]  \n",
       "11                     None         IT          0.0000     [[1]]  \n",
       "12                     None         IT          0.0000      [[]]  \n",
       "13                     None       acct          0.0000     [[3]]  \n",
       "14                     None       acct          0.0000      [[]]  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a VADER sentiment analyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Initialize lists to store data\n",
    "data = []\n",
    "\n",
    "TEXTS = [\n",
    "\"6. Muthafuckas still EGO tripping!Fool fuck your damn ego and live life to the fullest!!! LOL at these dumb ass niggers with the bullshit!\",\n",
    "\"7. @iMADE_YouMOAN don't play dumb, I should fuck yo little white ass up you nigger. 3chainz.? Aw.\",\n",
    "\"8. I WILL ONLY SAY IT ONCE! IF YOU DO NOT FEAR HAVE THE SUSPICION OF BEING CALLED A NIGGER! NIGGA! OR NEGRO AT ANY POINT IN YOUR LIFE YOU CANNOT USE THE N WORD. PERIOD. I am talking about non black people in specific!!!! White passing mixed kids are on THIN ICE.\",\n",
    "\"9. @CJ_Jacobs oh look it's that dumb niggers ghost acct! Fuck you you dumb motherfucker!\", \"@Otoliver @Yo_PaTnA_dEm20 this bitch is dumb talk bout do not dew said not sayed nigger not nigga lol bitch fuck off\"\n",
    "]\n",
    "for idx, doc in enumerate(nlp.pipe(TEXTS)):\n",
    "    for token in doc:\n",
    "        #if token.ent_type_ in (\"NORP\", \"PERSON\", \"GPE\"):\n",
    "        # Check if the token is a subject or complement\n",
    "        if token.dep_ in (\"nsubj\", \"aux\", \"acomp\", \"attr\", \"conj\"):\n",
    "            subjects = [w.text for w in token.head.lefts if w.dep_ in (\"nsubj\")]\n",
    "            aux = [w.text for w in token.head.lefts if w.dep_ in ('aux')]\n",
    "            objects = [w.text for w in token.head.rights if w.dep_ == \"dobj\"]\n",
    "            complement = [w.text for w in token.head.rights if w.dep_ in (\"acomp\", \"attr\", 'dobj', 'conj',\n",
    "                                                                          'VERB')]\n",
    "            person = [token.morph.get(\"Person\") for i in subjects]\n",
    "            person_obj = [token.morph.get(\"Person\") for i in objects]\n",
    "            if subjects or objects and complement:\n",
    "                for subject in subjects:\n",
    "                    for comp in complement:\n",
    "                        subject_ent = [ent.label_ for ent in doc.ents if ent.text == subject]\n",
    "                        object_ent = [ent.label_ for ent in doc.ents if ent.text == object]\n",
    "                        # comp_ent = [ent.label_ for ent in doc.ents if ent.text == comp]\n",
    "                        # Get the named entity text for the token\n",
    "                        named_entity_text = [ent.label_ for ent in doc.ents]\n",
    "                        # Perform sentiment analysis on the complement using VADER\n",
    "                        sentiment_score = analyzer.polarity_scores(comp)[\"compound\"]\n",
    "                        # Append data to list\n",
    "                        data.append({\"Text\": TEXTS[idx], \"Subject\": subject, \"Object\": object, \"Aux\": aux,\n",
    "                                      \"Subject Named Entity Text\": subject_ent[0] if subject_ent else None,\n",
    "                                      \"Object Named Entity Text\": object_ent[0] if object_ent else None,\n",
    "                                      \"Complement\": comp,\n",
    "                                      \"Sentiment_SUBJ\": sentiment_score, \"Pronous\": person})\n",
    "# \"Complement Named Entity Text\": comp_ent[0] if comp_ent else None\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj believe VERB []\n",
      "believe ccomp love VERB [I, are]\n",
      "Russians nsubj are AUX []\n",
      "are ccomp believe VERB [Russians, beautiful, and, is]\n",
      "beautiful acomp are AUX []\n",
      "and cc are AUX []\n",
      "Justin compound Trudeau PROPN []\n",
      "Trudeau nsubj is AUX [Justin]\n",
      "is conj are AUX [Trudeau, angel]\n",
      "an det angel NOUN []\n",
      "angel attr is AUX [an]\n",
      ", punct love VERB []\n",
      "I nsubj love VERB []\n",
      "love ROOT love VERB [believe, ,, I, him]\n",
      "him dobj love VERB []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator at 0x7fb930a425e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We define a function that scans the text object and prints the text (token), its dependency relationship and the word with which it is linked.\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "\n",
    "test = nlp(test)\n",
    "    # for token in doc_text:\n",
    "    #     print(token.text,\" : \",token.dep_, '|', token.head)\n",
    "for token in test:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        [child for child in token.children])\n",
    "\n",
    "token.children\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text: The original word text.\n",
    "Lemma: The base form of the word.\n",
    "POS: The simple UPOS part-of-speech tag.\n",
    "Tag: The detailed part-of-speech tag.\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "Shape: The word shape – capitalization, punctuation, digits.\n",
    "is alpha: Is the token an alpha character?\n",
    "is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access the element at which he is related\n",
    "doc[4].dep\n",
    "\n",
    "type = doc[4].head\n",
    "type.dep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6dbae683b84c4ddba21c9942a27e3737-0\" class=\"displacy\" width=\"2500\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">believe</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Russians</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">are</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">beautiful</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Justin</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Trudeau</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">angel,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">love</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">him</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 2150.0,2.0 2150.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-6\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-8\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-10\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,177.0 1790.0,177.0 1790.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1790.0,354.0 L1798.0,342.0 1782.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-11\" stroke-width=\"2px\" d=\"M1995,352.0 C1995,264.5 2135.0,264.5 2135.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1995,354.0 L1987,342.0 2003,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6dbae683b84c4ddba21c9942a27e3737-0-12\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6dbae683b84c4ddba21c9942a27e3737-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310.0,354.0 L2318.0,342.0 2302.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#We define a function that scans the text object and prints the text (token), its dependency relationship and the word with which it is linked.\n",
    "test =\"I believe Russians are beautiful and Justin Trudeau is an angel, I love him\"\n",
    "\n",
    "test = nlp(test)\n",
    "#This part of the library is imported for graphic visualization only.\n",
    "from spacy import displacy\n",
    "\n",
    "#For our doc object, we display the dependency graph, which allows us to better understand/analyze these relationships manually\n",
    "displacy.render(test, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple approach\n",
    "- Extract the adjectives, nouns, the Subject Person, and the complement/conjunction. \n",
    "- Extract the name entities\n",
    "- Extract all the synonyms of the adjective, nouns and complement into a long list.\n",
    "- Create a polarity score using Vadersentiment on the tweet itself and on the list of synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Adjectives</th>\n",
       "      <th>Nouns</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Complements</th>\n",
       "      <th>Named Entities</th>\n",
       "      <th>Synonyms</th>\n",
       "      <th>Sentiment Score</th>\n",
       "      <th>Synonyms Sentiment Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rape is real..zvasiyana nema jokes about being...</td>\n",
       "      <td>[real, drunk, gay, lesbian]</td>\n",
       "      <td>[rape, rape, ones, choice, thtz, sensitivity]</td>\n",
       "      <td>[(rape, []), (jokes, [3]), (rape, []), (sensit...</td>\n",
       "      <td>[real, drunk, being, gay, being, lesbian, wish]</td>\n",
       "      <td>[zvasiyana]</td>\n",
       "      <td>[rape, colza, Brassica_napus, rape, rapine, ra...</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>-0.9967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You never saw any celebrity say anything like ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[celebrity, B, Joke, joke, joke]</td>\n",
       "      <td>[(You, [2]), (celebrity, [])]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Obama, K Griffin]</td>\n",
       "      <td>[celebrity, famous_person, fame, celebrity, re...</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.9895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ManhattaKnight I mean he's gay, but he uses g...</td>\n",
       "      <td>[gay, gendered]</td>\n",
       "      <td>[slurs, rape, jokes]</td>\n",
       "      <td>[(I, [1]), (he, [3]), (he, [3])]</td>\n",
       "      <td>[gay, uses, makes]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[homosexual, homophile, homo, gay, cheery, gay...</td>\n",
       "      <td>-0.7227</td>\n",
       "      <td>-0.8225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Raul_Novoa16: @AliciaBernardez @Alex_Aim @_me...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[feminazi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@Alex_Aim @_mecaesmal feminazi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rape is rape. And the fact that I read one pos...</td>\n",
       "      <td>[gay, happy]</td>\n",
       "      <td>[Rape, rape, fact, post, guy, comments, stfu, ...</td>\n",
       "      <td>[(Rape, []), (I, [1]), (comments, []), (he, [3...</td>\n",
       "      <td>[be, happy]</td>\n",
       "      <td>[one]</td>\n",
       "      <td>[rape, colza, Brassica_napus, rape, rapine, ra...</td>\n",
       "      <td>-0.9097</td>\n",
       "      <td>-0.9901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@coiny Also, it's hard to take a company serio...</td>\n",
       "      <td>[hard]</td>\n",
       "      <td>[company, game, rape, jokes, jokes, women, way]</td>\n",
       "      <td>[(it, [3]), (game, []), (it, [3])]</td>\n",
       "      <td>[hard, jokes, treats]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[difficult, hard, hard, hard, hard, knockout, ...</td>\n",
       "      <td>-0.8074</td>\n",
       "      <td>-0.9918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Idgaf if you are gay, lesbian, bisexual, or wh...</td>\n",
       "      <td>[gay]</td>\n",
       "      <td>[fuck, rape, joke]</td>\n",
       "      <td>[(Idgaf, []), (you, [2]), (you, [2]), (you, [2])]</td>\n",
       "      <td>[gay, lesbian, bisexual, whatever, think]</td>\n",
       "      <td>[lesbian]</td>\n",
       "      <td>[homosexual, homophile, homo, gay, cheery, gay...</td>\n",
       "      <td>-0.4497</td>\n",
       "      <td>-0.9426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#GermanProfessor gives meaning to term FemiNaz...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[meaning, term, bcz]</td>\n",
       "      <td>[(GermanProfessor, [])]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[GermanProfessor, FemiNazi, Indian Men', India...</td>\n",
       "      <td>[meaning, significance, signification, import,...</td>\n",
       "      <td>-0.8126</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @mcclure111: #DontDateSJWs #ThatWouldBeAVio...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[DontDateSJWs, ThatWouldBeAViolationOfTheRestr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT @mcclure111, #DontDateSJWs #]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>So I call you female I’m basically calling you...</td>\n",
       "      <td>[female]</td>\n",
       "      <td>[bitch]</td>\n",
       "      <td>[(I, [1]), (I, [1])]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[female, female, female_person, female, female...</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>-0.9799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  rape is real..zvasiyana nema jokes about being...   \n",
       "1  You never saw any celebrity say anything like ...   \n",
       "2  @ManhattaKnight I mean he's gay, but he uses g...   \n",
       "3  @Raul_Novoa16: @AliciaBernardez @Alex_Aim @_me...   \n",
       "4  Rape is rape. And the fact that I read one pos...   \n",
       "5  @coiny Also, it's hard to take a company serio...   \n",
       "6  Idgaf if you are gay, lesbian, bisexual, or wh...   \n",
       "7  #GermanProfessor gives meaning to term FemiNaz...   \n",
       "8  RT @mcclure111: #DontDateSJWs #ThatWouldBeAVio...   \n",
       "9  So I call you female I’m basically calling you...   \n",
       "\n",
       "                    Adjectives  \\\n",
       "0  [real, drunk, gay, lesbian]   \n",
       "1                           []   \n",
       "2              [gay, gendered]   \n",
       "3                           []   \n",
       "4                 [gay, happy]   \n",
       "5                       [hard]   \n",
       "6                        [gay]   \n",
       "7                           []   \n",
       "8                           []   \n",
       "9                     [female]   \n",
       "\n",
       "                                               Nouns  \\\n",
       "0      [rape, rape, ones, choice, thtz, sensitivity]   \n",
       "1                   [celebrity, B, Joke, joke, joke]   \n",
       "2                               [slurs, rape, jokes]   \n",
       "3                                         [feminazi]   \n",
       "4  [Rape, rape, fact, post, guy, comments, stfu, ...   \n",
       "5    [company, game, rape, jokes, jokes, women, way]   \n",
       "6                                 [fuck, rape, joke]   \n",
       "7                               [meaning, term, bcz]   \n",
       "8  [DontDateSJWs, ThatWouldBeAViolationOfTheRestr...   \n",
       "9                                            [bitch]   \n",
       "\n",
       "                                            Subjects  \\\n",
       "0  [(rape, []), (jokes, [3]), (rape, []), (sensit...   \n",
       "1                      [(You, [2]), (celebrity, [])]   \n",
       "2                   [(I, [1]), (he, [3]), (he, [3])]   \n",
       "3                                                 []   \n",
       "4  [(Rape, []), (I, [1]), (comments, []), (he, [3...   \n",
       "5                 [(it, [3]), (game, []), (it, [3])]   \n",
       "6  [(Idgaf, []), (you, [2]), (you, [2]), (you, [2])]   \n",
       "7                            [(GermanProfessor, [])]   \n",
       "8                                                 []   \n",
       "9                               [(I, [1]), (I, [1])]   \n",
       "\n",
       "                                       Complements  \\\n",
       "0  [real, drunk, being, gay, being, lesbian, wish]   \n",
       "1                                               []   \n",
       "2                               [gay, uses, makes]   \n",
       "3                                               []   \n",
       "4                                      [be, happy]   \n",
       "5                            [hard, jokes, treats]   \n",
       "6        [gay, lesbian, bisexual, whatever, think]   \n",
       "7                                               []   \n",
       "8                                               []   \n",
       "9                                               []   \n",
       "\n",
       "                                      Named Entities  \\\n",
       "0                                        [zvasiyana]   \n",
       "1                                 [Obama, K Griffin]   \n",
       "2                                                 []   \n",
       "3                   [@Alex_Aim @_mecaesmal feminazi]   \n",
       "4                                              [one]   \n",
       "5                                                 []   \n",
       "6                                          [lesbian]   \n",
       "7  [GermanProfessor, FemiNazi, Indian Men', India...   \n",
       "8                  [RT @mcclure111, #DontDateSJWs #]   \n",
       "9                                                 []   \n",
       "\n",
       "                                            Synonyms  Sentiment Score  \\\n",
       "0  [rape, colza, Brassica_napus, rape, rapine, ra...          -0.7269   \n",
       "1  [celebrity, famous_person, fame, celebrity, re...           0.3400   \n",
       "2  [homosexual, homophile, homo, gay, cheery, gay...          -0.7227   \n",
       "3                                                 []           0.0000   \n",
       "4  [rape, colza, Brassica_napus, rape, rapine, ra...          -0.9097   \n",
       "5  [difficult, hard, hard, hard, hard, knockout, ...          -0.8074   \n",
       "6  [homosexual, homophile, homo, gay, cheery, gay...          -0.4497   \n",
       "7  [meaning, significance, signification, import,...          -0.8126   \n",
       "8                                                 []           0.0000   \n",
       "9  [female, female, female_person, female, female...          -0.5859   \n",
       "\n",
       "   Synonyms Sentiment Score  \n",
       "0                   -0.9967  \n",
       "1                    0.9895  \n",
       "2                   -0.8225  \n",
       "3                    0.0000  \n",
       "4                   -0.9901  \n",
       "5                   -0.9918  \n",
       "6                   -0.9426  \n",
       "7                    0.4404  \n",
       "8                    0.0000  \n",
       "9                   -0.9799  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize lists to store data\n",
    "data = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    # Analyze the tweet\n",
    "    doc = nlp(tweet)\n",
    "\n",
    "    # Extract adjectives, nouns, subjects and complements/conjunctions\n",
    "    adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "    nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    subjects = [(token.text, token.morph.get('Person')) for token in doc if token.dep_ == \"nsubj\"]\n",
    "    complements = [token.text for token in doc if token.dep_ in (\"acomp\", \"conj\")]\n",
    "\n",
    "    # Extract named entities\n",
    "    named_entities = [ent.text for ent in doc.ents]\n",
    "\n",
    "    # Extract synonyms\n",
    "    synonyms = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in (\"ADJ\", \"NOUN\"):\n",
    "            for syn in wordnet.synsets(token.text):\n",
    "                for lemma in syn.lemmas():\n",
    "                    synonyms.append(lemma.name())\n",
    "\n",
    "    # Create a polarity score\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)[\"compound\"]\n",
    "    synonyms_sentiment_score = analyzer.polarity_scores(\" \".join(synonyms))[\"compound\"]\n",
    "\n",
    "    # Append data to list\n",
    "    data.append({\n",
    "        \"Tweet\": tweet,\n",
    "        \"Adjectives\": adjectives,\n",
    "        \"Nouns\": nouns,\n",
    "        \"Subjects\": subjects,\n",
    "        \"Complements\": complements,\n",
    "        \"Named Entities\": named_entities,\n",
    "        \"Synonyms\": synonyms,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"Synonyms Sentiment Score\": synonyms_sentiment_score\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntax references\n",
    "These relations link a  referent and an antecedent via a reference expression.\n",
    "\n",
    "- Co-reference: A reference in a sentence to the same entity - \" Michael Jackson died in 2009. The  King of Pop  was very influential\".\n",
    "- The anaphora: This is a reference to an expression already used in the discourse, but which is not necessarily the same entity - \"The  house is dilapidated. Its roof  is falling apart\"\n",
    "\n",
    "The aim is to identify the elements in the text that create these syntactic links. These are usually semantic repetitions\n",
    "\n",
    "#### Coreference resolution \n",
    "Once the candidates have been identified, other systems (such as networks) are created to  determine the antecedent/referent relationships between the various entities\n",
    "\n",
    "#### Rhetoric tree\n",
    "Identify categories in the sentence: the result, the explanation, parallelism, elaboration, etc\n",
    "\n",
    "- Elaboration relations are characterized by\n",
    "meronymies and textual implications\n",
    "(collocation). \n",
    "- List relations are easily detected using\n",
    "synonyms and hyponyms.\n",
    "\n",
    "What could be interesting is the list relations, although I am not sure what it is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge-based features\n",
    "Detecting hate speech requires more than observing specific keywords. Incorporating one-world knowledge, such as distinguishing between hateful and benign expressions, broadens the scope of analysis. Furthermore, focusing on specific subtypes of hate speech, such as anti-LGBT discourse, can be accomplished using resources like ConceptNet, which encodes concepts connected by relations.\n",
    "\n",
    "- https://conceptnet.io/\n",
    "- https://github.com/eclarson/MachineLearningNotebooks/blob/master/14.%20ConceptNet.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
